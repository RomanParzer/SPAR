<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Roman Parzer">
<meta name="author" content="Peter Filzmoser">
<meta name="author" content="Laura Vana Gür">
<meta name="keywords" content="random projection, variable screening, ensemble learning, R">

<title>SPAR: Sparse Projected Averaged Regression in R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="SPAR_files/libs/clipboard/clipboard.min.js"></script>
<script src="SPAR_files/libs/quarto-html/quarto.js"></script>
<script src="SPAR_files/libs/quarto-html/popper.min.js"></script>
<script src="SPAR_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="SPAR_files/libs/quarto-html/anchor.min.js"></script>
<link href="SPAR_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="SPAR_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="SPAR_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="SPAR_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="SPAR_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-intro" id="toc-sec-intro" class="nav-link active" data-scroll-target="#sec-intro"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#sec-models" id="toc-sec-models" class="nav-link" data-scroll-target="#sec-models"><span class="header-section-number">2</span> Methods</a>
  <ul class="collapse">
  <li><a href="#variable-screening" id="toc-variable-screening" class="nav-link" data-scroll-target="#variable-screening"><span class="header-section-number">2.1</span> Variable screening</a></li>
  <li><a href="#random-projection" id="toc-random-projection" class="nav-link" data-scroll-target="#random-projection"><span class="header-section-number">2.2</span> Random projection</a></li>
  <li><a href="#algorithm" id="toc-algorithm" class="nav-link" data-scroll-target="#algorithm"><span class="header-section-number">2.3</span> Algorithm</a></li>
  </ul></li>
  <li><a href="#sec-software" id="toc-sec-software" class="nav-link" data-scroll-target="#sec-software"><span class="header-section-number">3</span> Software</a></li>
  <li><a href="#sec-illustrations" id="toc-sec-illustrations" class="nav-link" data-scroll-target="#sec-illustrations"><span class="header-section-number">4</span> Illustrations</a>
  <ul class="collapse">
  <li><a href="#face-image-data" id="toc-face-image-data" class="nav-link" data-scroll-target="#face-image-data"><span class="header-section-number">4.1</span> Face image data</a></li>
  <li><a href="#darwin-data-set" id="toc-darwin-data-set" class="nav-link" data-scroll-target="#darwin-data-set"><span class="header-section-number">4.2</span> Darwin data set</a></li>
  </ul></li>
  <li><a href="#sec-conclusion" id="toc-sec-conclusion" class="nav-link" data-scroll-target="#sec-conclusion"><span class="header-section-number">5</span> Conclusion</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><code>SPAR</code>: Sparse Projected Averaged Regression in <code>R</code></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Roman Parzer <a href="mailto:Roman.Parzers@tuwien.ac.at" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            TU Wien
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Peter Filzmoser </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            TU Wien
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Laura Vana Gür </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            TU Wien
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p><code>SPAR</code> is a package for building predictive generalized linear models (GLMs) with high-dimensional (HD) predictors in <code>R</code>. In package <code>SPAR</code>, probabilistic variable screening and random projection of the predictors are performed to obtain an ensemble of GLMs, which are then averaged to obtain predictions in an high-dimensional regression setting.</p>
  </div>
</div>

<div>
  <div class="keywords">
    <div class="block-title">Keywords</div>
    <p>random projection, variable screening, ensemble learning, R</p>
  </div>
</div>

</header>


<section id="sec-intro" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="sec-intro"><span class="header-section-number">1</span> Introduction</h2>
<p><code>SPAR</code> is a package for building predictive generalized linear models (GLMs) with high-dimensional (HD) predictors in <code>R</code>. In package <code>SPAR</code>, probabilistic variable screening and random projection of the predictors are performed to obtain an ensemble of GLMs, which are then averaged to obtain predictions in an high-dimensional regression setting.</p>
<p>Random projection is a computationally-efficient method which linearly maps a set of points in high dimensions into a much lower-dimensional space while approximately preserving pairwise distances. For very large <span class="math inline">\(p\)</span>, random projection can suffer from overfitting, as too many irrelevant predictors are being considered for prediction purposes <span class="citation" data-cites="Dunson2020TargRandProj">(<a href="#ref-Dunson2020TargRandProj" role="doc-biblioref">Mukhopadhyay and Dunson 2020</a>)</span>. Therefore, screening out irrelevant variables before performing the random projection is advisable in order to tackle this issue. The screening can be performed in a probabilistic fashion, by randomly sampling covariates for inclusion in the model based on probabilities proportional to an importance measure (as opposed to random subspace sampling employed in e.g., random forests). Finally, in practice the information from multiple such screening and projections can be combined by averaging, in order to reduce the variance introduced by the random sampling (of both projections and screening indicators) <span class="citation" data-cites="Thanei2017RPforHDR">(<a href="#ref-Thanei2017RPforHDR" role="doc-biblioref">Thanei, Heinze, and Meinshausen 2017</a>)</span>.</p>
<p>Several packages which provide functionality for random projections are available for <code>R</code>. Package <code>RandPro</code> <span class="citation" data-cites="RandProR SIDDHARTH2020100629">(<a href="#ref-RandProR" role="doc-biblioref">Aghila and Siddharth 2020</a>; <a href="#ref-SIDDHARTH2020100629" role="doc-biblioref">Siddharth and Aghila 2020</a>)</span> allows for four different random projection matrices to be applied to the predictor matrix before employing one of <span class="math inline">\(k\)</span>~nearest neighbor, support vector machine or naive Bayes classifier. Package <code>SPCAvRP</code> <span class="citation" data-cites="SPCAvRPR">(<a href="#ref-SPCAvRPR" role="doc-biblioref">Gataric, Wang, and Samworth 2019</a>)</span> implements sparse principal component analysis, based on the aggregation of eigenvector information from “carefully-selected” axis-aligned random projections of the sample covariance matrix. Package <code>RPEnsembleR</code> <span class="citation" data-cites="RPEnsembleR">(<a href="#ref-RPEnsembleR" role="doc-biblioref">Cannings and Samworth 2021</a>)</span> implements the same idea of “carefully-selected” random projections when building an ensemble of classifiers. For <code>Python</code> <span class="citation" data-cites="Python">Rossum et al. (<a href="#ref-Python" role="doc-biblioref">2011</a>)</span> the <code>sklearn.random_projection</code> module implements two types of unstructured random matrix, namely Gaussian random matrix and sparse random matrix.</p>
<p>On the other hand, there are a multitude of packages dealing with variable screening on the Comprehensive <code>R</code> Archive Network (CRAN). The (iterative) sure independence screening procedure and extensions in <span class="citation" data-cites="Fan2007SISforUHD">Fan and Lv (<a href="#ref-Fan2007SISforUHD" role="doc-biblioref">2007</a>)</span>, <span class="citation" data-cites="Fan2010sisglms">Fan and Song (<a href="#ref-Fan2010sisglms" role="doc-biblioref">2010</a>)</span>, <span class="citation" data-cites="fan2010high">Fan, Feng, and Wu (<a href="#ref-fan2010high" role="doc-biblioref">2010</a>)</span> are implemented in package <code>SIS</code> <span class="citation" data-cites="SISR">(<a href="#ref-SISR" role="doc-biblioref">Saldana and Feng 2018</a>)</span>, which also provides functionality for estimating a penalized generalized linear model or a cox regression model for the variables picked by the screening procedure.</p>
<p>Package <code>VariableScreening</code> <span class="citation" data-cites="pkg:VariableScreening">(<a href="#ref-pkg:VariableScreening" role="doc-biblioref">R. Li, Huang, and Dziak 2022</a>)</span> implements screening for iid data, varying-coefficient models, and longitudinal data using different screening methods: Sure Independent Ranking and Screening – which ranks the predictors by their correlation with the rank-ordered response (SIRS), Distance Correlation Sure Independence Screening – a non-parametric extension of the correlation coefficient (DC-SIS), MV Sure Independence Screening – using the mean conditional variance measure (MV-SIS).</p>
<p>A collection of model-free screening techniques such as SIRS, DC-SIS, MV-SIS, the fused Kolmogorov filter <span class="citation" data-cites="mai2015fusedkolmogorov">(<a href="#ref-mai2015fusedkolmogorov" role="doc-biblioref">Mai and Zou 2015</a>)</span>, the projection correlation method using knock-off features <span class="citation" data-cites="liu2020knockoff">(<a href="#ref-liu2020knockoff" role="doc-biblioref">Wanjun Liu and Li 2022</a>)</span>, are provided in package <code>MFSIS</code> <span class="citation" data-cites="pkg:MFSIS">(<a href="#ref-pkg:MFSIS" role="doc-biblioref">Cheng et al. 2024</a>)</span>. Package <code>tilting</code> <span class="citation" data-cites="pkg:tilting">(<a href="#ref-pkg:tilting" role="doc-biblioref">Cho and Fryzlewicz 2016</a>)</span> implements an algorithm for variable selection in high-dimensional linear regression using tilted correlation, which takes into account high correlations among the variables in a data-driven way. Feature screening based on conditional distance correlation <span class="citation" data-cites="wang2015conditional">(<a href="#ref-wang2015conditional" role="doc-biblioref">Xueqin Wang et al. 2015</a>)</span> can be performed with the <code>cdcsis</code> package <span class="citation" data-cites="pkg:cdcsis">(<a href="#ref-pkg:cdcsis" role="doc-biblioref">Hu et al. 2024</a>)</span> while package <code>QCSIS</code> <span class="citation" data-cites="pkg:QCSIS">(<a href="#ref-pkg:QCSIS" role="doc-biblioref">Ma, Zhang, and Zhou 2015</a>)</span> implements screening based on (composite) quantile correlation.</p>
<p>Package <code>LqG</code> <span class="citation" data-cites="pkg:LqG">(<a href="#ref-pkg:LqG" role="doc-biblioref">Wu, Li, and Li 2022</a>)</span> provides a group screening procedure that is based on maximum Lq-likelihood estimation, to simultaneously account for the group structure and data contamination in variable screening.</p>
<p>Feature screening using an <span class="math inline">\(L1\)</span> fusion penalty can be performed with package <code>fusionclust</code> <span class="citation" data-cites="pkg:fusionclust">(<a href="#ref-pkg:fusionclust" role="doc-biblioref">Banerjee, Mukherjee, and Radchenko 2017</a>)</span>. Package <code>SMLE</code> <span class="citation" data-cites="pkg:SMLE">(<a href="#ref-pkg:SMLE" role="doc-biblioref">Zang, Xu, and Burkett 2020</a>)</span> implements joint feature screening via sparse MLE <span class="citation" data-cites="SMLE2014">(<a href="#ref-SMLE2014" role="doc-biblioref">Xu and Chen 2014</a>)</span> in high-dimensional linear, logistic, and Poisson models. Package <code>TSGSIS</code> <span class="citation" data-cites="pkg:TSGSIS">(<a href="#ref-pkg:TSGSIS" role="doc-biblioref">Fang, Wang, and Hsiung 2017b</a>)</span> provides a high-dimensional grouped variable selection approach for detecting interactions that may not have marginal effects in high dimensional linear and logistic regression <span class="citation" data-cites="10.1093/bioinformatics/btx409">(<a href="#ref-10.1093/bioinformatics/btx409" role="doc-biblioref">Fang, Wang, and Hsiung 2017a</a>)</span>.</p>
<p>Package <code>RaSEn</code> <span class="citation" data-cites="pkg:RaSEn">(<a href="#ref-pkg:RaSEn" role="doc-biblioref">Tian and Feng 2021</a>)</span> implements the RaSE algorithm for ensemble classification and classification problems, where random subspaces are generated and the optimal one is chosen to train a weak learner on the basis of some criterion. Various choices of base classifiers are implemented, for instance, linear discriminant analysis, quadratic discriminant analysis, k-nearest neighbor, logistic or linear regression, decision trees, random forest, support vector machines. The selected percentages of variables can be employed for variable screening.</p>
<p>Package <code>Ball</code> <span class="citation" data-cites="pkg:ball">(<a href="#ref-pkg:ball" role="doc-biblioref">J. Zhu et al. 2021</a>)</span> provides functionality for variable screening using ball statistics, which is appropriate for shape, directional, compositional and symmetric positive definite matrix data.</p>
<p>Package <code>BayesS5</code> <span class="citation" data-cites="pkg:BayesS5">(<a href="#ref-pkg:BayesS5" role="doc-biblioref">Shin and Tian 2020</a>)</span> implements Bayesian variable selection using simplified shotgun stochastic search algorithm with screening <span class="citation" data-cites="shin2017scalablebayesianvariableselection">(<a href="#ref-shin2017scalablebayesianvariableselection" role="doc-biblioref">Shin, Bhattacharya, and Johnson 2017</a>)</span> while package <code>bravo</code> <span class="citation" data-cites="pkg:bravo">(<a href="#ref-pkg:bravo" role="doc-biblioref">D. Li et al. 2024</a>)</span> implements the Bayesian iterative screening method proposed in <span class="citation" data-cites="wang2021bayesianiterativescreeningultrahigh">(<a href="#ref-wang2021bayesianiterativescreeningultrahigh" role="doc-biblioref">R. Wang, Dutta, and Roy 2021</a>)</span>.</p>
<p>The rest of the paper is organized as follows: <a href="#sec-models" class="quarto-xref">Section&nbsp;2</a> provides the methodological details of the implemented algorithm. The package is described in <a href="#sec-software" class="quarto-xref">Section&nbsp;3</a>. <a href="#sec-illustrations" class="quarto-xref">Section&nbsp;4</a> contains two examples of employing the package on real data sets. Finally, <a href="#sec-conclusion" class="quarto-xref">Section&nbsp;5</a> concludes.</p>
</section>
<section id="sec-models" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-models"><span class="header-section-number">2</span> Methods</h2>
<section id="variable-screening" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="variable-screening"><span class="header-section-number">2.1</span> Variable screening</h3>
<p>The general idea of variable screening is to select a (small) subset of variables, based on some marginal utility measure for the predictors, and disregard the rest for further analysis. In their seminal work on sure independence screening (SIS), <span class="citation" data-cites="Fan2007SISforUHD">Fan and Lv (<a href="#ref-Fan2007SISforUHD" role="doc-biblioref">2007</a>)</span> propose to use the vector of marginal empirical correlations <span class="math inline">\(\hat\alpha=(\alpha_1,\ldots ,\alpha_p)'\in\mathbb{R}^p,\alpha_j=\text{Cor}(X_{.j},y)\)</span> for variable screening in a linear regression setting by selecting the variable set <span class="math inline">\(\mathcal{A}_\gamma = \{j\in [p]:|w_j|&gt;\gamma\}\)</span> depending on a threshold <span class="math inline">\(\gamma&gt;0\)</span>, where <span class="math inline">\([p]=\{1,\dots,p\}\)</span>. Under certain technical conditions, where <span class="math inline">\(p\)</span> grows exponentially with <span class="math inline">\(n\)</span>, they show that this procedure has the <em>sure screening property</em> <span class="math display">\[
\mathbb{P}(\mathcal{A} \subset \mathcal{A}_{\gamma_n})\to 1 \text{ for } n\to \infty
\]</span> with an explicit exponential rate of convergence, where <span class="math inline">\(\mathcal{A}=\{j\in[p]:\beta_j\neq 0\}\)</span> is the set of truly active variables. These conditions imply that <span class="math inline">\(\mathcal{A}\)</span> and <span class="math inline">\(\mathcal{A}_{\gamma_n}\)</span> contain less than <span class="math inline">\(n\)</span> variables. One of the critical conditions is that on the population level for some fixed <span class="math inline">\(i\in[n]\)</span>, <span class="math inline">\(\min_{j\in\mathcal{A}}|\text{Cov}(y_i/\beta_j,x_{ij})| \geq c\)</span> for some constant <span class="math inline">\(c&gt;0\)</span>, which rules out practically possible scenarios where an important variable is marginally uncorrelated to the response. <span class="citation" data-cites="Fan2010sisglms">Fan and Song (<a href="#ref-Fan2010sisglms" role="doc-biblioref">2010</a>)</span> extend the approach to GLMs, where the screening is performed based on the log-likelihood of the GLM containing only <span class="math inline">\(X_j\)</span> as a predictor: <span class="math inline">\(\hat\alpha_j=: \text{min}_{{\beta_j}\in\mathbb{R}}\sum_{i=1}^n -\ell(\beta;y_i,x_{ij})\)</span>.</p>
<p>A rich stream of literature focuses on developing semi- or non-parametric alternatives to SIS which should be more robust to model misspecification. For linear regression, approaches include using the ranked correlation <span class="citation" data-cites="zhu2011model">(<a href="#ref-zhu2011model" role="doc-biblioref">L.-P. Zhu et al. 2011</a>)</span>, (conditional) distance correlation <span class="citation" data-cites="li2012feature">Xueqin Wang et al. (<a href="#ref-wang2015conditional" role="doc-biblioref">2015</a>)</span>. or quantile correlation <span class="citation" data-cites="ma2016robust">(<a href="#ref-ma2016robust" role="doc-biblioref">Ma and Zhang 2016</a>)</span>. For GLMs, <span class="citation" data-cites="fan2011nonparametric">Fan, Feng, and Song (<a href="#ref-fan2011nonparametric" role="doc-biblioref">2011</a>)</span> extend <span class="citation" data-cites="Fan2010sisglms">Fan and Song (<a href="#ref-Fan2010sisglms" role="doc-biblioref">2010</a>)</span> by fitting a generalized additive model with B-splines. Further extensions for discrete (or categorical) outcomes include the fused Kolmogorov filter <span class="citation" data-cites="mai2013kolmogorov">(<a href="#ref-mai2013kolmogorov" role="doc-biblioref">Mai and Zou 2013</a>)</span>, using the mean conditional variance, i.e., the expectation in <span class="math inline">\(X_j\)</span> of the variance in the response of the conditional cumulative distribution function <span class="math inline">\(\Prob(X\leq x|Y)\)</span> <span class="citation" data-cites="cui2015model">(<a href="#ref-cui2015model" role="doc-biblioref">Cui, Li, and Zhong 2015</a>)</span>. <span class="citation" data-cites="ke2023sufficient">Ke (<a href="#ref-ke2023sufficient" role="doc-biblioref">2023</a>)</span> propose a model free method where the contribution of each individual predictor is quantified marginally and conditionally in the presence of the control variables as well as the other candidates by reproducing-kernel-based <span class="math inline">\(R^2\)</span> and partial <span class="math inline">\(R^2\)</span> statistics.</p>
<p>To account for multicollinearity among the predictors, which can cause relevant predictors to be marginally uncorrelated with the response, various extensions have been proposed. In a linear regression setting, <span class="citation" data-cites="Wang2015HOLP">Xiangyu Wang and Leng (<a href="#ref-Wang2015HOLP" role="doc-biblioref">2016</a>)</span> propose employing the ridge estimator when the penalty term converges to zero while <span class="citation" data-cites="cho2012high">Cho and Fryzlewicz (<a href="#ref-cho2012high" role="doc-biblioref">2012</a>)</span> propose using the tilted correlation, i.e., the correlation of a tilted version of <span class="math inline">\(X_j\)</span> with <span class="math inline">\(y\)</span>. For discrete outcomes, joint feature screening <span class="citation" data-cites="SMLE2014">Xu and Chen (<a href="#ref-SMLE2014" role="doc-biblioref">2014</a>)</span> has been proposed.</p>
</section>
<section id="random-projection" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="random-projection"><span class="header-section-number">2.2</span> Random projection</h3>
<p>The random projection method relies on the Johnson-Lindenstrauss (JL) lemma <span class="citation" data-cites="JohnsonLindenstrauss1984">(<a href="#ref-JohnsonLindenstrauss1984" role="doc-biblioref">Johnson and Lindenstrauss 1984</a>)</span>, which asserts that there exists a random map <span class="math inline">\(\Phi\in \mathbb{R}^{m \times p}\)</span> that embeds any set of points in <span class="math inline">\(p\)</span>-dimensional Euclidean space collected in the rows of <span class="math inline">\(X\in \mathbb{R}^{n\times p}\)</span> into a <span class="math inline">\(m\)</span>-dimensional Euclidean space with <span class="math inline">\(m&lt; \mathcal{O}(\log n/\varepsilon^2)\)</span> so that all pairwise distances are maintained within a factor of <span class="math inline">\(1 \pm \varepsilon\)</span>, for any <span class="math inline">\(0 &lt;\varepsilon&lt; 1\)</span>.</p>
<p>The random map <span class="math inline">\(\Phi\)</span> should be chosen such that it fulfills certain conditions <span class="citation" data-cites="JohnsonLindenstrauss1984">(see <a href="#ref-JohnsonLindenstrauss1984" role="doc-biblioref">Johnson and Lindenstrauss 1984</a>)</span>. The literature focuses on constructing such matrices either by sampling them from some “appropriate” distribution, by inducing sparsity in the matrix and/or by employing specific fast constructs which lead to efficient matrix-vector multiplications.</p>
<p>It turns out that the conditions are generally satisfied by nearly all sub-Gaussian distributions <span class="citation" data-cites="matouvsek2008variants">(<a href="#ref-matouvsek2008variants" role="doc-biblioref">Matoušek 2008</a>)</span>. Common choices are:</p>
<ul>
<li><p>Normal distribution.: <span class="math inline">\(\Phi_{ij} \overset{iid}{\sim} N(0,1)\)</span> <span class="citation" data-cites="FRANKL1988JLSphere">(<a href="#ref-FRANKL1988JLSphere" role="doc-biblioref">Frankl and Maehara 1988</a>)</span> or <span class="math inline">\(\Phi_{ij} = \begin{cases} {\sim} N(0,1/\sqrt{\psi}) &amp; \text{with prob. } \psi \\ 0 &amp; \text{with prob. } 1 - \psi \end{cases}\)</span> <span class="citation" data-cites="matouvsek2008variants">(<a href="#ref-matouvsek2008variants" role="doc-biblioref">Matoušek 2008</a>)</span>,</p></li>
<li><p>Rademacher distribution <span class="citation" data-cites="ACHLIOPTAS2003JL LiHastie2006VerySparseRP">(<a href="#ref-ACHLIOPTAS2003JL" role="doc-biblioref">Achlioptas 2003</a>; <a href="#ref-LiHastie2006VerySparseRP" role="doc-biblioref">P. Li, Hastie, and Church 2006</a>)</span> <span class="math display">\[
\Phi_{ij} = \begin{cases}
    \pm 1/\sqrt{\psi} &amp; \text{with prob. } \psi/2 \\
    0 &amp; \text{with prob. } 1 - \psi, \quad 0&lt;\psi\leq 1‚
  \end{cases},
\]</span> where <span class="citation" data-cites="ACHLIOPTAS2003JL">Achlioptas (<a href="#ref-ACHLIOPTAS2003JL" role="doc-biblioref">2003</a>)</span> shows results for <span class="math inline">\(\psi=1\)</span> and <span class="math inline">\(\psi=1/3\)</span> while <span class="citation" data-cites="LiHastie2006VerySparseRP">P. Li, Hastie, and Church (<a href="#ref-LiHastie2006VerySparseRP" role="doc-biblioref">2006</a>)</span> recommend using <span class="math inline">\(\psi=1/\sqrt{p}\)</span> to obtain very sparse matrices.</p></li>
</ul>
<p>Distributions which are not sub-Gaussian, such as standard Cauchy, have also been proposed in the literature to tackle scenarios where the data is high-dimensional, non-sparse, and heavy-tailed by preserving approximate <span class="math inline">\(\ell_1\)</span> distances <span class="citation" data-cites="li2007nonlinear">(see e.g., <a href="#ref-li2007nonlinear" role="doc-biblioref">P. Li, Hastie, and Church 2007</a>)</span>.</p>
<p>An orthonormalization is usually applied <span class="math inline">\((\Phi\Phi^\top)^{-1/2}\Phi\)</span>. Orthonormalization can constitute a computational bottleneck for the random projection method, however, in high-dimensions it can be omitted. <!-- TODO: citation --> <!-- For the RH examples, taking $\psi$ too small gives high distortion of sparse vectors \citep{matouvsek2008variants}. Ailon-Chazelle 2006 get around this by using a randomized orthogonal (normalized Hadamard) matrix to ensure w.h.p all data vectors are dense. --></p>
<p>To speed computations, <span class="citation" data-cites="ailon2009fast">Ailon and Chazelle (<a href="#ref-ailon2009fast" role="doc-biblioref">2009</a>)</span> propose the fast Johnson- Lindenstrauss transform (FJLT), where the random projection matrix is given by <span class="math inline">\(\Phi=PHD\)</span> with <span class="math inline">\(P\)</span> random and sparse, <span class="math inline">\(P_{ij} \sim N (0, 1/q)\)</span> with probability <span class="math inline">\(1/q\)</span> and <span class="math inline">\(0\)</span> otherwise, <span class="math inline">\(H\)</span> the normalized Hadamard (orthogonal) matrix <span class="math inline">\(H_{ij} = p^{-1/2}(-1)^{\langle i-1,j-1\rangle}\)</span>, where <span class="math inline">\(\langle i, j\rangle\)</span> is the dot-product of the <span class="math inline">\(m\)</span>-bit vectors <span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span> expressed in binary, and <span class="math inline">\(D = \text{diag}(\pm 1)\)</span> is a diagonal matrix with random elements <span class="math inline">\(D_{ii}\)</span>.</p>
<p><span class="citation" data-cites="Clarkson2013LowRankApproxShort">Clarkson and Woodruff (<a href="#ref-Clarkson2013LowRankApproxShort" role="doc-biblioref">2013</a>)</span> propose a sparse embedding matrix <span class="math inline">\({\Phi=BD}\)</span>, where <span class="math inline">\(B\in\{0,1\}^{m \times p}\)</span> is random binary matrix and <span class="math inline">\(D\)</span> is a <span class="math inline">\(p\times p\)</span> diagonal matrix with <span class="math inline">\((D_{ii}+1)/2\sim \text{Unif}\{0,1\}\)</span>, and prove that the dimension <span class="math inline">\(m\)</span> is bounded by a polynomial in <span class="math inline">\(r\varepsilon^{-1}\)</span> for <span class="math inline">\(0 &lt;\varepsilon&lt; 1\)</span> and <span class="math inline">\(r\)</span> being the rank of <span class="math inline">\(X\)</span>. While this is generally larger than that of FJLT, the sparse embedding matrix requires less time to compute <span class="math inline">\(\Phi X\)</span> compared to other subspace embeddings.</p>
<p><span class="citation" data-cites="parzer2023sparse">Parzer, Vana-Gür, and Filzmoser (<a href="#ref-parzer2023sparse" role="doc-biblioref">2023</a>)</span> propose employing <span class="math inline">\({D_{ii}=\hat \alpha}\)</span> in the sparse embedding matrix of <span class="citation" data-cites="Clarkson2013LowRankApproxShort">Clarkson and Woodruff (<a href="#ref-Clarkson2013LowRankApproxShort" role="doc-biblioref">2013</a>)</span>, <span class="math inline">\(\hat \alpha\)</span> is a screening coefficient in the regression such as the ridge or the HOLP coefficients, and show that the proposed projection increases the predictive performance in a linear regression setting.</p>
<!-- see more info at https://www.cs.waikato.ac.nz/~bobd/ECML_Tutorial/ECML_handouts.pdf. -->
<!-- See also https://web.math.princeton.edu/~amits/publications/LeanWalsh_published.pdf. -->
<!-- https://arxiv.org/pdf/1710.03163.pdf -->
<!-- https://cseweb.ucsd.edu/~dasgupta/papers/randomf.pdf -->
<!-- https://people.math.ethz.ch/~nicolai/mv/notes6.pdf -->
<!-- https://web.math.princeton.edu/~amits/publications/LeanWalsh_published.pdf -->
<!-- https://pastel.hal.science/tel-01481912/document -->
</section>
<section id="algorithm" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="algorithm"><span class="header-section-number">2.3</span> Algorithm</h3>
<ul>
<li><p>choose family with corresponding log-likelihood <span class="math inline">\(\ell(.)\)</span> and link</p></li>
<li><p>standardize predictors <span class="math inline">\(X:n\times p\)</span></p></li>
<li><p>calculate screening coefficients <span class="math inline">\(\hat\alpha\)</span> e.g.,</p>
<ul>
<li>ridge: <span class="math inline">\(\hat\alpha=: \text{argmin}_{{\beta}\in\mathbb{R}^p}\sum_{i=1}^n -\ell(\beta;y_i,x_i) + \frac{\varepsilon}{2}\sum_{j=1}^p{\beta}_j^2, \, \varepsilon &gt; 0\)</span></li>
<li>marginal likelihood: <span class="math inline">\(\hat\alpha_j=: \text{min}_{{\beta_j}\in\mathbb{R}}\sum_{i=1}^n -\ell(\beta;y_i,x_{ij})\)</span></li>
</ul></li>
<li><p>For <span class="math inline">\(k=1,\dots,M\)</span> models:</p>
<ul>
<li><p>draw <span class="math inline">\(2n\)</span> predictors with probabilities <span class="math inline">\(p_j\propto |\hat\alpha_j|\)</span> yielding screening index set <span class="math inline">\(I_k=\{j_1^k,\dots,j_{2n}^k\}\subset[p]\)</span></p></li>
<li><p>project remaining variables to dim. <span class="math inline">\(m_k\sim \text{Unif}\{\log(p),\dots,n/2\}\)</span> using <span class="math inline">\(\Phi_k\)</span> to obtain <span class="math inline">\(Z_k=X_{\cdot I_k}\Phi_k^\top \in \mathbb{R}^{n\times m_k}\)</span>:</p></li>
<li><p>fit a of <span class="math inline">\(y\)</span> against <span class="math inline">\(Z_k\)</span> (with small <span class="math inline">\(L_2\)</span>-penalty ) to obtain estimated coefficients <span class="math inline">\(\gamma^k\in\mathbb{R}^{m_k}\)</span> and <span class="math inline">\(\hat \beta_{I_k}^k=\Phi_k'\gamma^k\)</span>, <span class="math inline">\(\hat \beta_{\bar I_k}^k=0\)</span>.</p></li>
</ul></li>
<li><p>for a given threshold <span class="math inline">\(\lambda&gt;0\)</span>, set all <span class="math inline">\(\hat\beta_j^k\)</span> with <span class="math inline">\(|\hat\beta_j^k|&lt;\lambda\)</span> to <span class="math inline">\(0\)</span> for all <span class="math inline">\(j,k\)</span></p></li>
<li><p> choose <span class="math inline">\(M\)</span> and <span class="math inline">\(\lambda\)</span> via cross-validation by repeating steps 1 to 4 for each fold and evaluating a prediction performance measure on the withheld fold; and choose <span class="math display">\[\begin{align}
     (M_{\text{best}},\lambda_{\text{best}}) = \text{argmin}_{M,\lambda}\text{Dev}(M,\lambda)
   \end{align}\]</span></p></li>
<li><p>combine via <strong>simple average</strong> <span class="math inline">\(\hat \beta = \sum_{k=1}^M\hat \beta^k / M\)</span></p></li>
<li><p>output the estimated coefficients and predictions for the chosen <span class="math inline">\(M\)</span> and <span class="math inline">\(\lambda\)</span></p></li>
</ul>
</section>
</section>
<section id="sec-software" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-software"><span class="header-section-number">3</span> Software</h2>
<p>The two main functions are:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">spar</span>(x, y, <span class="at">family =</span> <span class="fu">gaussian</span>(), ...)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">spar.cv</span>(x, y, <span class="at">family =</span> <span class="fu">gaussian</span>(), <span class="at">nfolds =</span> <span class="dv">10</span>L, ...)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Most important arguments:</p>
<ul>
<li><p><code>x</code> <span class="math inline">\(n \times p\)</span> numeric matrix of predictor variables.</p></li>
<li><p><code>y</code> numeric response vector of length <span class="math inline">\(n\)</span>.</p></li>
<li><p><code>family</code> object from <code>stats::family</code>.</p></li>
<li><p><code>type.rpm</code> type of random projection matrix to be employed; one of <code>"cwdatadriven"</code>, <code>"cw"</code> <span class="citation" data-cites="Clarkson2013LowRankApproxShort">Clarkson and Woodruff (<a href="#ref-Clarkson2013LowRankApproxShort" role="doc-biblioref">2013</a>)</span>, <code>"gaussian"</code>, <code>"sparse"</code>.</p></li>
<li><p><code>nscreen</code> max number of variables kept after screening in each marginal model; defaults to <span class="math inline">\(2n\)</span>.</p></li>
<li><p><code>type.screening</code> measure by which the coefficients are screened; <code>"ridge"</code> performs screening based on ridge regression, <code>"marglik"</code> marginal likelihood of fitting a GLM for each predictor, <code>"corr"</code> correlation of the response with each predictor.</p></li>
<li><p><code>type.measure</code> loss to use for choosing <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(M\)</span>; defaults to <code>"deviance"</code> (available for all families). Other options are <code>"mse"</code> or <code>"mae"</code> (for all families), <code>"class"</code> and <code>"1-auc"</code> for <code>"binomial"</code>.</p></li>
<li><p><code>nlambda</code> number of <span class="math inline">\(\lambda\)</span> values to be considered for thresholding and optionally <code>lambdas</code>, a vector of values.</p></li>
<li><p><code>nummods</code> vector containing the size of the different ensembles <span class="math inline">\(M\)</span> to consider for the prediction.</p></li>
</ul>
<p>Methods <code>print</code>, <code>plot</code>, <code>coef</code>, <code>predict</code> are available.</p>
<div id="tbl-overviewrp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-overviewrp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Overview of implemented random projection matrices.
</figcaption>
<div aria-describedby="tbl-overviewrp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Name</th>
<th>Random projection method</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>gaussian</code></td>
<td>Standard Gaussian</td>
</tr>
<tr class="even">
<td><code>sparse</code></td>
<td>Rademacher</td>
</tr>
<tr class="odd">
<td><code>cw</code></td>
<td>sparse embedding matrix</td>
</tr>
<tr class="even">
<td><code>cwdatadriven</code></td>
<td>data driven sparse embedding matrix</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="sec-illustrations" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-illustrations"><span class="header-section-number">4</span> Illustrations</h2>
<section id="face-image-data" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="face-image-data"><span class="header-section-number">4.1</span> Face image data</h3>
<p>We illustrate the package on a data set containing <span class="math inline">\(n = 698\)</span> black and white face images of size <span class="math inline">\(p = 64 \times 64 = 4096\)</span> and the faces’ horizontal looking direction angle as the response variable. The Isomap face data can be found online on https://web.archive.org/web/20160913051505/http://isomap. stanford.edu/datasets.html</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"R.matlab"</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>temp <span class="ot">&lt;-</span> <span class="fu">tempdir</span>()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">download.file</span>(<span class="st">"https://web.archive.org/web/20150922051706/http://isomap.stanford.edu/face_data.mat.Z"</span>, <span class="fu">file.path</span>(temp, <span class="st">"face_data.mat.Z"</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="fu">system</span>(<span class="fu">sprintf</span>(<span class="st">'uncompress %s'</span>, <span class="fu">paste0</span>(temp, <span class="st">"/face_data.mat.Z"</span>)))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>facedata <span class="ot">&lt;-</span> <span class="fu">readMat</span>(<span class="fu">file.path</span>(temp, <span class="st">"face_data.mat"</span>))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">t</span>(facedata<span class="sc">$</span>images)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> facedata<span class="sc">$</span>poses[<span class="dv">1</span>,]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can visualize e.g., the first observation in this data set by:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">X =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">64</span>,<span class="at">each=</span><span class="dv">64</span>),<span class="at">Y =</span> <span class="fu">rep</span>(<span class="dv">64</span><span class="sc">:</span><span class="dv">1</span>,<span class="dv">64</span>),</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">Z =</span> facedata<span class="sc">$</span>images[,i]),</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(X, Y, <span class="at">fill =</span> Z)) <span class="sc">+</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_tile</span>() <span class="sc">+</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_void</span>() <span class="sc">+</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="fu">paste0</span>(<span class="st">"y = "</span>,<span class="fu">round</span>(facedata<span class="sc">$</span>poses[<span class="dv">1</span>,i],<span class="dv">1</span>))) <span class="sc">+</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>,</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can split the data into training vs test sample:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>ntot <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>ntest <span class="ot">&lt;-</span> ntot <span class="sc">*</span> <span class="fl">0.25</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>testind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>ntot, ntest, <span class="at">replace=</span><span class="cn">FALSE</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>xtrain <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(x[<span class="sc">-</span>testind, ])</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>ytrain <span class="ot">&lt;-</span> y[<span class="sc">-</span>testind]</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>xtest <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(x[testind, ])</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>ytest <span class="ot">&lt;-</span> y[testind]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now estimate the model on the training data:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(SPAR)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>spar_faces <span class="ot">&lt;-</span> <span class="fu">spar.cv</span>(xtrain, ytrain,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">family =</span> <span class="fu">gaussian</span>(),</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">nummods =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">50</span>),</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>                      <span class="at">type.measure =</span> <span class="st">"mse"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>spar_faces</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>plot</code> method for ‘<code>spar.cv</code>’ objects displays by default the measure employed in the cross validation (in this case MSE) for a grid of <span class="math inline">\(\lambda\)</span> values, where the number of models is fixed to the value found to perform best in cross-validation exercise:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(spar_faces)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The coefficients of the different variables (in this example pixels) obtained by averaging over the coefficients the marginal models (for optimal <span class="math inline">\(\lambda\)</span> and number of models) are given by:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>face_coef <span class="ot">&lt;-</span> <span class="fu">coef</span>(spar_faces, <span class="at">opt_par =</span> <span class="st">"best"</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(face_coef)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The coefficients from each of the marginal models (before averaging) can be plotted using the <code>plot(..., plot_type = "coefs")</code></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(spar_faces, <span class="st">"coefs"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>predict()</code> function can be applied to the ‘<code>spar.cv</code>’ object:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>ynew <span class="ot">&lt;-</span> <span class="fu">predict</span>(spar_faces, <span class="at">xnew =</span> xtest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the high-dimensional setting it is interesting to look at the relative mean square prediction error which compares the MSE to the MSE of a model containing only an intercept:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>rMSPEconst <span class="ot">&lt;-</span> <span class="fu">mean</span>((ytest <span class="sc">-</span> <span class="fu">mean</span>(y))<span class="sc">^</span><span class="dv">2</span>) </span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((ynew<span class="sc">-</span>ytest)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>rMSPEconst</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Additionally, for this data set, one can visualize the effect of each pixel <span class="math inline">\(\hat\beta_j x^\text{new}_{i,j}\)</span> in predicting the face orientation in a given image e.g., 9th in the test set:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="dv">9</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plot4 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">X =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">64</span>, <span class="at">each =</span> <span class="dv">64</span>),</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>                           <span class="at">Y =</span> <span class="fu">rep</span>(<span class="dv">64</span><span class="sc">:</span><span class="dv">1</span>, <span class="dv">64</span>),</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>                           <span class="at">effect =</span> xtest[i,] <span class="sc">*</span> face_coef<span class="sc">$</span>beta), </span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>                <span class="fu">aes</span>(X, Y, <span class="at">fill=</span> effect)) <span class="sc">+</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_tile</span>() <span class="sc">+</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_void</span>() <span class="sc">+</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_gradient2</span>() <span class="sc">+</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="fu">bquote</span>(<span class="fu">hat</span>(y) <span class="sc">==</span> .(<span class="fu">round</span>(ynew[i])))) <span class="sc">+</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>)) </span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>plot4</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="darwin-data-set" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="darwin-data-set"><span class="header-section-number">4.2</span> Darwin data set</h3>
<p>The Darwin dataset <span class="citation" data-cites="CILIA2022darwin">(<a href="#ref-CILIA2022darwin" role="doc-biblioref">Cilia et al. 2022</a>)</span> contains a binary response for Alzheimer’s disease (AD) together with extracted features from 25 handwriting tests (18 features per task) for 89 AD patients and 85 healthy people (<span class="math inline">\(n=174\)</span>).</p>
<p>The data set can be downloaded from https://archive.ics.uci.edu/dataset/732/darwin:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">download.file</span>(<span class="st">"https://archive.ics.uci.edu/static/public/732/darwin.zip"</span>, temp)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>darwin_tmp <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="fu">unzip</span>(temp,  <span class="st">"data.csv"</span>), <span class="at">stringsAsFactors =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Before proceeding with the analysis, the data is screened for multivariate outliers using the DDC algorithm in package <code>cellWise</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>darwin_orig <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">as.matrix</span>(darwin_tmp[, <span class="sc">!</span>(<span class="fu">colnames</span>(darwin_tmp) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"ID"</span>, <span class="st">"class"</span>))]),</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">as.numeric</span>(darwin_tmp<span class="sc">$</span>class) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>tmp <span class="ot">&lt;-</span> cellWise<span class="sc">::</span><span class="fu">DDC</span>(darwin_orig<span class="sc">$</span>x,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>                     <span class="fu">list</span>(<span class="at">returnBigXimp =</span> <span class="cn">TRUE</span>, </span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>                          <span class="at">tolProb =</span> <span class="fl">0.999</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>                          <span class="at">silent =</span> <span class="cn">TRUE</span>))</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>darwin <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">x =</span> tmp<span class="sc">$</span>Ximp,</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>               <span class="at">y =</span> darwin_orig<span class="sc">$</span>y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We estimate the spar model with binomial family and logit link and use <span class="math inline">\(1-\)</span>area under the ROC curve as the cross-validation measure:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>spar_darwin <span class="ot">&lt;-</span> <span class="fu">spar.cv</span>(darwin<span class="sc">$</span>x, darwin<span class="sc">$</span>y,</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>                       <span class="at">family =</span> <span class="fu">binomial</span>(logit),</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">nummods =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">50</span>),</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">type.measure =</span> <span class="st">"1-auc"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>plot</code> method for ‘<code>spar.cv</code>’ objects displays by default the measure employed in the cross validation (in this case MSE) for a grid of <span class="math inline">\(\lambda\)</span> values, where the number of models is fixed to the value found to perform best in cross-validation exercise:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(spar_darwin)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The plot of the coefficients can be interpreted nicely in this example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>ntasks <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>nfeat <span class="ot">&lt;-</span> <span class="dv">18</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>reorder_ind <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">outer</span>((<span class="fu">seq_len</span>(ntasks) <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> nfeat, <span class="fu">seq_len</span>(nfeat), <span class="st">"+"</span>))</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>feat_names <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="fu">colnames</span>(darwin<span class="sc">$</span>x)[<span class="fu">seq_len</span>(nfeat)],</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>                     <span class="cf">function</span>(name) <span class="fu">substr</span>(name, <span class="dv">1</span>, <span class="fu">nchar</span>(name) <span class="sc">-</span> <span class="dv">1</span>))</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(spar_darwin,<span class="st">"coefs"</span>,<span class="at">coef_order =</span> reorder_ind) <span class="sc">+</span> </span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fl">0.5</span> <span class="sc">+</span> <span class="fu">seq_len</span>(ntasks <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> ntasks, </span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>             <span class="at">alpha =</span> <span class="fl">0.2</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>,<span class="at">x =</span> (<span class="fu">seq_len</span>(nfeat) <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> ntasks <span class="sc">+</span> <span class="dv">12</span>,</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>           <span class="at">y =</span> <span class="dv">45</span>,<span class="at">label=</span>feat_names, <span class="at">angle =</span> <span class="dv">90</span>,</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>           <span class="at">size =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In general we observe that the different features measures across different tasks have the same impact on the probability of AD (observable by the blocks of blue or red lines).</p>
</section>
</section>
<section id="sec-conclusion" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="sec-conclusion"><span class="header-section-number">5</span> Conclusion</h2>
<p>Package <code>SPAR</code> provides an implementation for estimating an ensemble of GLMs after performing probabilistic screening and random projection in a high-dimensional setting.</p>
</section>
<section id="computational-details" class="level2 unnumbered unlisted">
<h2 class="unnumbered unlisted anchored" data-anchor-id="computational-details">Computational details</h2>
<p>The results in this paper were obtained using <code>R</code> 4.4.0. <!-- with the \pkg{MASS}~\Sexpr{packageVersion("MASS")} package. --></p>
<p><code>R</code> itself and all packages used are available from the Comprehensive <code>R</code> Archive Network (CRAN) at .</p>
</section>
<section id="acknowledgments" class="level2 unnumbered unlisted">
<h2 class="unnumbered unlisted anchored" data-anchor-id="acknowledgments">Acknowledgments</h2>
<p>Roman Parzer and Laura Vana-Gür acknowledge funding from the Austrian Science Fund (FWF) for the project “High-dimensional statistical learning: New methods to advance economic and sustainability policies” (ZK 35), jointly carried out by WU Vienna University of Economics and Business, Paris Lodron University Salzburg, TU Wien, and the Austrian Institute of Economic Research (WIFO).</p>
</section>
<section id="references" class="level2 unnumbered unlisted">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ACHLIOPTAS2003JL" class="csl-entry" role="listitem">
Achlioptas, Dimitris. 2003. <span>“Database-Friendly Random Projections: Johnson-Lindenstrauss with Binary Coins.”</span> <em>Journal of Computer and System Sciences</em> 66 (4): 671–87. <a href="https://doi.org/10.1016/S0022-0000(03)00025-4">https://doi.org/10.1016/S0022-0000(03)00025-4</a>.
</div>
<div id="ref-RandProR" class="csl-entry" role="listitem">
Aghila, G, and R Siddharth. 2020. <em>: Random Projection with Classification</em>. <a href="https://CRAN.R-project.org/package=RandPro">https://CRAN.R-project.org/package=RandPro</a>.
</div>
<div id="ref-ailon2009fast" class="csl-entry" role="listitem">
Ailon, Nir, and Bernard Chazelle. 2009. <span>“The Fast Johnson–Lindenstrauss Transform and Approximate Nearest Neighbors.”</span> <em>SIAM Journal on Computing</em> 39 (1): 302–22.
</div>
<div id="ref-pkg:fusionclust" class="csl-entry" role="listitem">
Banerjee, Trambak, Gourab Mukherjee, and Peter Radchenko. 2017. <em>Fusionclust: Clustering and Feature Screening Using L1 Fusion Penalty</em>. <a href="https://CRAN.R-project.org/package=fusionclust">https://CRAN.R-project.org/package=fusionclust</a>.
</div>
<div id="ref-RPEnsembleR" class="csl-entry" role="listitem">
Cannings, Timothy I., and Richard J. Samworth. 2021. <em>: Random Projection Ensemble Classification</em>. <a href="https://CRAN.R-project.org/package=RPEnsemble">https://CRAN.R-project.org/package=RPEnsemble</a>.
</div>
<div id="ref-pkg:MFSIS" class="csl-entry" role="listitem">
Cheng, Xuewei, Hong Wang, Liping Zhu, Wei Zhong, and Hanpu Zhou. 2024. <em>MFSIS: Model-Free Sure Independent Screening Procedures</em>. <a href="https://CRAN.R-project.org/package=MFSIS">https://CRAN.R-project.org/package=MFSIS</a>.
</div>
<div id="ref-cho2012high" class="csl-entry" role="listitem">
Cho, Haeran, and Piotr Fryzlewicz. 2012. <span>“High Dimensional Variable Selection via Tilting.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 74 (3): 593–622.
</div>
<div id="ref-pkg:tilting" class="csl-entry" role="listitem">
———. 2016. <em>Tilting: Variable Selection via Tilted Correlation Screening Algorithm</em>. <a href="https://CRAN.R-project.org/package=tilting">https://CRAN.R-project.org/package=tilting</a>.
</div>
<div id="ref-CILIA2022darwin" class="csl-entry" role="listitem">
Cilia, Nicole D., Giuseppe De Gregorio, Claudio De Stefano, Francesco Fontanella, Angelo Marcelli, and Antonio Parziale. 2022. <span>“Diagnosing Alzheimer’s Disease from on-Line Handwriting: A Novel Dataset and Performance Benchmarking.”</span> <em>Engineering Applications of Artificial Intelligence</em> 111: 104822. https://doi.org/<a href="https://doi.org/10.1016/j.engappai.2022.104822">https://doi.org/10.1016/j.engappai.2022.104822</a>.
</div>
<div id="ref-Clarkson2013LowRankApproxShort" class="csl-entry" role="listitem">
Clarkson, Kenneth L., and David P. Woodruff. 2013. <span>“Low Rank Approximation and Regression in Input Sparsity Time.”</span> In <em>Proceedings of the Forty-Fifth Annual ACM Symposium on Theory of Computing</em>, 81–90.
</div>
<div id="ref-cui2015model" class="csl-entry" role="listitem">
Cui, Hengjian, Runze Li, and Wei Zhong. 2015. <span>“Model-Free Feature Screening for Ultrahigh Dimensional Discriminant Analysis.”</span> <em>Journal of the American Statistical Association</em> 110 (510): 630–41.
</div>
<div id="ref-fan2011nonparametric" class="csl-entry" role="listitem">
Fan, Jianqing, Yang Feng, and Rui Song. 2011. <span>“Nonparametric Independence Screening in Sparse Ultra-High-Dimensional Additive Models.”</span> <em>Journal of the American Statistical Association</em> 106 (494): 544–57.
</div>
<div id="ref-fan2010high" class="csl-entry" role="listitem">
Fan, Jianqing, Yang Feng, and Yichao Wu. 2010. <span>“High-Dimensional Variable Selection for Cox’s Proportional Hazards Model.”</span> In <em>Borrowing Strength: Theory Powering Applications–a Festschrift for Lawrence d. Brown</em>, 6:70–87. Institute of Mathematical Statistics.
</div>
<div id="ref-Fan2007SISforUHD" class="csl-entry" role="listitem">
Fan, Jianqing, and Jinchi Lv. 2007. <span>“Sure Independence Screening for Ultra-High Dimensional Feature Space.”</span> <em>J Roy Stat Soc</em> B 70 (January).
</div>
<div id="ref-Fan2010sisglms" class="csl-entry" role="listitem">
Fan, Jianqing, and Rui Song. 2010. <span>“Sure Independence Screening in Generalized Linear Models with <span>NP</span>-Dimensionality.”</span> <em>The Annals of Statistics</em> 38 (6): 3567–3604. <a href="https://doi.org/10.1214/10-AOS798">https://doi.org/10.1214/10-AOS798</a>.
</div>
<div id="ref-10.1093/bioinformatics/btx409" class="csl-entry" role="listitem">
Fang, Yao-Hwei, Jie-Huei Wang, and Chao A Hsiung. 2017a. <span>“TSGSIS: A High-Dimensional Grouped Variable Selection Approach for Detection of Whole-Genome SNP–SNP Interactions.”</span> <em>Bioinformatics</em> 33 (22): 3595–3602. <a href="https://doi.org/10.1093/bioinformatics/btx409">https://doi.org/10.1093/bioinformatics/btx409</a>.
</div>
<div id="ref-pkg:TSGSIS" class="csl-entry" role="listitem">
Fang, Yao-Hwei, Jie-Huei Wang, and Chao A. Hsiung. 2017b. <em>TSGSIS: Two Stage-Grouped Sure Independence Screening</em>. <a href="https://CRAN.R-project.org/package=TSGSIS">https://CRAN.R-project.org/package=TSGSIS</a>.
</div>
<div id="ref-FRANKL1988JLSphere" class="csl-entry" role="listitem">
Frankl, P, and H Maehara. 1988. <span>“The Johnson-Lindenstrauss Lemma and the Sphericity of Some Graphs.”</span> <em>Journal of Combinatorial Theory, Series B</em> 44 (3): 355–62. https://doi.org/<a href="https://doi.org/10.1016/0095-8956(88)90043-3">https://doi.org/10.1016/0095-8956(88)90043-3</a>.
</div>
<div id="ref-SPCAvRPR" class="csl-entry" role="listitem">
Gataric, Milana, Tengyao Wang, and Richard J. Samworth. 2019. <em>: Sparse Principal Component Analysis via Random Projections (SPCAvRP)</em>. <a href="https://CRAN.R-project.org/package=SPCAvRP">https://CRAN.R-project.org/package=SPCAvRP</a>.
</div>
<div id="ref-pkg:cdcsis" class="csl-entry" role="listitem">
Hu, Wenhao, Mian Huang, Wenliang Pan, Xueqin Wang, Canhong Wen, Yuan Tian, Heping Zhang, and Jin Zhu. 2024. <em>Cdcsis: Conditional Distance Correlation Based Feature Screening and Conditional Independence Inference</em>. <a href="https://CRAN.R-project.org/package=cdcsis">https://CRAN.R-project.org/package=cdcsis</a>.
</div>
<div id="ref-JohnsonLindenstrauss1984" class="csl-entry" role="listitem">
Johnson, William, and Joram Lindenstrauss. 1984. <span>“Extensions of Lipschitz Maps into a Hilbert Space.”</span> <em>Contemporary Mathematics</em> 26 (January): 189–206. https://doi.org/<a href="https://doi.org/10.1090/conm/026/737400">https://doi.org/10.1090/conm/026/737400</a>.
</div>
<div id="ref-ke2023sufficient" class="csl-entry" role="listitem">
Ke, Chenlu. 2023. <span>“Sufficient Variable Screening with High-Dimensional Controls.”</span> <em>Electronic Journal of Statistics</em> 17 (2): 2139–79. <a href="https://doi.org/10.1214/23-EJS2150">https://doi.org/10.1214/23-EJS2150</a>.
</div>
<div id="ref-pkg:bravo" class="csl-entry" role="listitem">
Li, Dongjin, Debarshi Chakraborty, Somak Dutta, and Vivekananda Roy. 2024. <em>Bravo: Bayesian Screening and Variable Selection</em>. <a href="https://CRAN.R-project.org/package=bravo">https://CRAN.R-project.org/package=bravo</a>.
</div>
<div id="ref-LiHastie2006VerySparseRP" class="csl-entry" role="listitem">
Li, Ping, Trevor J. Hastie, and Kenneth W. Church. 2006. <span>“Very Sparse Random Projections.”</span> In <em>Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 287–96. KDD ’06. New York, NY, USA: Association for Computing Machinery. https://doi.org/<a href="https://doi.org/10.1145/1150402.1150436">https://doi.org/10.1145/1150402.1150436</a>.
</div>
<div id="ref-li2007nonlinear" class="csl-entry" role="listitem">
Li, Ping, Trevor J Hastie, and Kenneth W Church. 2007. <span>“Nonlinear Estimators and Tail Bounds for Dimension Reduction in <span>L</span>1 Using <span>C</span>auchy Random Projections.”</span> <em>Journal of Machine Learning Research</em> 8 (Oct): 2497–2532. <a href="https://jmlr.csail.mit.edu/papers/v8/li07b.html">https://jmlr.csail.mit.edu/papers/v8/li07b.html</a>.
</div>
<div id="ref-pkg:VariableScreening" class="csl-entry" role="listitem">
Li, Runze, Liying Huang, and John Dziak. 2022. <em>VariableScreening: High-Dimensional Screening for Semiparametric Longitudinal Regression</em>. <a href="https://CRAN.R-project.org/package=VariableScreening">https://CRAN.R-project.org/package=VariableScreening</a>.
</div>
<div id="ref-li2012feature" class="csl-entry" role="listitem">
Li, Runze, Wei Zhong, and Liping Zhu. 2012. <span>“Feature Screening via Distance Correlation Learning.”</span> <em>Journal of the American Statistical Association</em> 107 (499): 1129–39.
</div>
<div id="ref-ma2016robust" class="csl-entry" role="listitem">
Ma, Xuejun, and Jingxiao Zhang. 2016. <span>“Robust Model-Free Feature Screening via Quantile Correlation.”</span> <em>Journal of Multivariate Analysis</em> 143: 472–80.
</div>
<div id="ref-pkg:QCSIS" class="csl-entry" role="listitem">
Ma, Xuejun, Jingxiao Zhang, and Jingke Zhou. 2015. <em>QCSIS: Sure Independence Screening via Quantile Correlation and Composite Quantile Correlation</em>. <a href="https://CRAN.R-project.org/package=QCSIS">https://CRAN.R-project.org/package=QCSIS</a>.
</div>
<div id="ref-mai2013kolmogorov" class="csl-entry" role="listitem">
Mai, Qing, and Hui Zou. 2013. <span>“The Kolmogorov Filter for Variable Screening in High-Dimensional Binary Classification.”</span> <em>Biometrika</em> 100 (1): 229–34. <a href="https://doi.org/10.1093/biomet/ass062">https://doi.org/10.1093/biomet/ass062</a>.
</div>
<div id="ref-mai2015fusedkolmogorov" class="csl-entry" role="listitem">
———. 2015. <span>“<span class="nocase">The fused Kolmogorov filter: A nonparametric model-free screening method</span>.”</span> <em>The Annals of Statistics</em> 43 (4): 1471–97. <a href="https://doi.org/10.1214/14-AOS1303">https://doi.org/10.1214/14-AOS1303</a>.
</div>
<div id="ref-matouvsek2008variants" class="csl-entry" role="listitem">
Matoušek, Jiřı́. 2008. <span>“On Variants of the Johnson–Lindenstrauss Lemma.”</span> <em>Random Structures &amp; Algorithms</em> 33 (2): 142–56. <a href="https://doi.org/10.1002/rsa.20218">https://doi.org/10.1002/rsa.20218</a>.
</div>
<div id="ref-Dunson2020TargRandProj" class="csl-entry" role="listitem">
Mukhopadhyay, Minerva, and David B. Dunson. 2020. <span>“<span class="nocase">Targeted Random Projection for Prediction From High-Dimensional Features</span>.”</span> <em>Journal of the American Statistical Association</em> 115 (532): 1998–2010. https://doi.org/<a href="https://doi.org/10.1080/01621459.2019.1677240">https://doi.org/10.1080/01621459.2019.1677240</a>.
</div>
<div id="ref-parzer2023sparse" class="csl-entry" role="listitem">
Parzer, Roman, Laura Vana-Gür, and Peter Filzmoser. 2023. <span>“Sparse Projected Averaged Regression for High-Dimensional Data.”</span> <a href="https://arxiv.org/abs/2312.00130">https://arxiv.org/abs/2312.00130</a>.
</div>
<div id="ref-Python" class="csl-entry" role="listitem">
Rossum, Guido van et al. 2011.<em> Programming Language</em>. <a href="http://www.python.org">http://www.python.org</a>.
</div>
<div id="ref-SISR" class="csl-entry" role="listitem">
Saldana, Diego Franco, and Yang Feng. 2018. <span>“: An Package for Sure Independence Screening in Ultrahigh-Dimensional Statistical Models.”</span> <em>Journal of Statistical Software</em> 83 (2): 1–25. https://doi.org/<a href="https://doi.org/10.18637/jss.v083.i02">https://doi.org/10.18637/jss.v083.i02</a>.
</div>
<div id="ref-shin2017scalablebayesianvariableselection" class="csl-entry" role="listitem">
Shin, Minsuk, Anirban Bhattacharya, and Valen E. Johnson. 2017. <span>“Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-Dimensional Settings.”</span> <a href="https://arxiv.org/abs/1507.07106">https://arxiv.org/abs/1507.07106</a>.
</div>
<div id="ref-pkg:BayesS5" class="csl-entry" role="listitem">
Shin, Minsuk, and Ruoxuan Tian. 2020. <em>BayesS5: Bayesian Variable Selection Using Simplified Shotgun Stochastic Search with Screening (S5)</em>. <a href="https://CRAN.R-project.org/package=BayesS5">https://CRAN.R-project.org/package=BayesS5</a>.
</div>
<div id="ref-SIDDHARTH2020100629" class="csl-entry" role="listitem">
Siddharth, R., and G. Aghila. 2020. <span>“RandPro- a Practical Implementation of Random Projection-Based Feature Extraction for High Dimensional Multivariate Data Analysis in r.”</span> <em>SoftwareX</em> 12: 100629. <a href="https://doi.org/10.1016/j.softx.2020.100629">https://doi.org/10.1016/j.softx.2020.100629</a>.
</div>
<div id="ref-Thanei2017RPforHDR" class="csl-entry" role="listitem">
Thanei, Gian-Andrea, Christina Heinze, and Nicolai Meinshausen. 2017. <span>“Random Projections for Large-Scale Regression.”</span> In <em>Big and Complex Data Analysis: Methodologies and Applications</em>, 51–68. Cham: Springer International Publishing. https://doi.org/<a href="https://doi.org/10.1007/978-3-319-41573-4_3">https://doi.org/10.1007/978-3-319-41573-4_3</a>.
</div>
<div id="ref-pkg:RaSEn" class="csl-entry" role="listitem">
Tian, Ye, and Yang Feng. 2021. <em>RaSEn: Random Subspace Ensemble Classification and Variable Screening</em>. <a href="https://CRAN.R-project.org/package=RaSEn">https://CRAN.R-project.org/package=RaSEn</a>.
</div>
<div id="ref-wang2021bayesianiterativescreeningultrahigh" class="csl-entry" role="listitem">
Wang, Run, Somak Dutta, and Vivekananda Roy. 2021. <span>“Bayesian Iterative Screening in Ultra-High Dimensional Settings.”</span> <a href="https://arxiv.org/abs/2107.10175">https://arxiv.org/abs/2107.10175</a>.
</div>
<div id="ref-Wang2015HOLP" class="csl-entry" role="listitem">
Wang, Xiangyu, and Chenlei Leng. 2016. <span>“High-Dimensional Ordinary Least-Squares Projection for Screening Variables.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 78 (June): 589–611. https://doi.org/<a href="https://doi.org/10.1111/rssb.12127">https://doi.org/10.1111/rssb.12127</a>.
</div>
<div id="ref-wang2015conditional" class="csl-entry" role="listitem">
Wang, Xueqin, Wenliang Pan, Wenhao Hu, Yuan Tian, and Heping Zhang. 2015. <span>“Conditional Distance Correlation.”</span> <em>Journal of the American Statistical Association</em> 110 (512): 1726–34.
</div>
<div id="ref-liu2020knockoff" class="csl-entry" role="listitem">
Wanjun Liu, Jingyuan Liu, Yuan Ke, and Runze Li. 2022. <span>“Model-Free Feature Screening and FDR Control with Knockoff Features.”</span> <em>Journal of the American Statistical Association</em> 117 (537): 428–43. <a href="https://doi.org/10.1080/01621459.2020.1783274">https://doi.org/10.1080/01621459.2020.1783274</a>.
</div>
<div id="ref-pkg:LqG" class="csl-entry" role="listitem">
Wu, Mingcong, Yang Li, and Rong Li. 2022. <em>LqG: Robust Group Variable Screening Based on Maximum Lq-Likelihood Estimation</em>. <a href="https://CRAN.R-project.org/package=LqG">https://CRAN.R-project.org/package=LqG</a>.
</div>
<div id="ref-SMLE2014" class="csl-entry" role="listitem">
Xu, Chen, and Jiahua Chen. 2014. <span>“The Sparse MLE for Ultrahigh-Dimensional Feature Screening.”</span> <em>Journal of the American Statistical Association</em> 109 (507): 1257–69.
</div>
<div id="ref-pkg:SMLE" class="csl-entry" role="listitem">
Zang, Qianxiang, Chen Xu, and Kelly Burkett. 2020. <em><span>SMLE</span>:an <span>R</span> Package for Joint Feature Screening in Ultrahigh-Dimensional GLMs</em>.
</div>
<div id="ref-pkg:ball" class="csl-entry" role="listitem">
Zhu, Jin, Wenliang Pan, Wei Zheng, and Xueqin Wang. 2021. <span>“: An Package for Detecting Distribution Difference and Association in Metric Spaces.”</span> <em>Journal of Statistical Software</em> 97 (6): 1–31. <a href="https://doi.org/10.18637/jss.v097.i06">https://doi.org/10.18637/jss.v097.i06</a>.
</div>
<div id="ref-zhu2011model" class="csl-entry" role="listitem">
Zhu, Li-Ping, Lexin Li, Runze Li, and Li-Xing Zhu. 2011. <span>“Model-Free Feature Screening for Ultrahigh-Dimensional Data.”</span> <em>Journal of the American Statistical Association</em> 106 (496): 1464–75.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>