---
title: "[spar]{.pkg}: Sparse Projected Averaged Regression in [R]{.proglang}"
header-includes: #allows you to add in your own Latex packages
- \usepackage{float} #use the 'float' package
- \floatplacement{figure}{H} 
format:
    jss-pdf:
        keep-tex: true
    # jss-html: default
knitr:
  opts_chunk: 
    collapse: true
    comment: "#>" 
    prompt: true
    continue: true
author:
  - name: Roman Parzer
    affiliations:
      - name: TU Wien
        department: Computational Statistics (CSTAT), Institute of Statistics and Mathematical Methods in Economics
        address: Wiedner Hauptstraße 8-10
        city: Vienna
        country: Austria
        postal-code: 1040
    orcid: 0000-0003-0893-3190
    email: romanparzer1@gmail.com
  - name: Laura Vana Gür
    affiliations:
      - name: TU Wien
        department: Computational Statistics (CSTAT), Institute of Statistics and Mathematical Methods in Economics
        address: Wiedner Hauptstraße 8-10
        city: Vienna
        country: Austria
        postal-code: 1040
    orcid: 0000-0002-9613-7604
  - name: Peter Filzmoser
    affiliations:
      - name: TU Wien
        department: Computational Statistics (CSTAT), Institute of Statistics and Mathematical Methods in Economics
        address: Wiedner Hauptstraße 8-10
        city: Vienna
        country: Austria
        postal-code: 1040
    orcid: 0000-0002-8014-4682
    
abstract: |
  [spar]{.pkg} is a newly developed package for [R]{.proglang} that builds ensembles of predictive generalized linear models (GLMs) with high-dimensional (HD) predictors. It employs an algorithm utilizing variable screening and random projection tools to efficiently handle the computational challenges associated with large sets of predictors.
  The package is designed with a strong focus on extensibility. Screening and random projection techniques are implemented as [S]{.proglang}3 classes with user-friendly constructor functions, enabling users to easily integrate and develop new procedures. This design enhances the package's adaptability and makes it a powerful tool for a variety of high-dimensional applications.

keywords: [random projection, variable screening, ensemble learning, R]
keywords-formatted: [random projection, variable screening, ensemble learning, "[R]{.proglang}"]

bibliography: SPAR.bib   
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(fig.pos = "t!",
                      out.extra = "", 
                      eval=TRUE)
```



## Introduction {#sec-intro}

The [spar]{.pkg} package for [R]{.proglang} [@RLanguage] offers functionality for estimating predictive generalized linear models (GLMs) for high-dimensional settings, where the number of predictors $p$ significantly exceeds the number of observations $n$ i.e., $p>n$ or even $p>>n$. The package leverages variable screening 
methods with random projection techniques to effectively reduce 
the dimensionality of the predictor space, which are repeatedly performed in order 
to reduce the variability stemming in the inherent randomness of the procedure, which in turn enables the construction of an ensemble of predictive generalized linear models.

Random projection is a computationally-efficient method which linearly maps a set of points in high dimensions into a much lower-dimensional space. 
Several packages in [R]{.proglang} provide functionality for random
projections. For instance, package [RandPro]{.pkg}
[@RandProR; @SIDDHARTH2020100629] allows a Gaussian random matrix, a sparse matrix introduced in @ACHLIOPTAS2003JL,@LiHastie2006VerySparseRP
or a matrix generated using the equal probability distribution with the 
elements $\{-1,1\}$ to be applied to the predictor matrix before employing one of $k$ 
nearest neighbor, support vector machine or naive Bayes classifier on the 
projected features. Package [SPCAvRP]{.pkg} [@SPCAvRPR] implements sparse 
principal component analysis, based on the aggregation of eigenvector
information from "carefully-selected" axis-aligned random projections of the 
sample covariance matrix. Additionally, 
package [RPEnsembleR]{.pkg} [@RPEnsembleR] implements a similar idea when 
building the ensemble of classifiers: for each classifier in the ensemble,
a collection of 
(Gaussian, axis-aligned projections, or Haar) random projection matrices is generated, and the one that minimizes a risk measure 
for classification on a test set is selected.
For [Python]{.proglang} [@Python] the [sklearn.random\\_projection]{.pkg} module [@pedregosa2011scikit] implements two types of unstructured random matrices, namely Gaussian random matrix and sparse random matrix.

Random projection can suffer from noise accumulation for very large $p$, 
as too many irrelevant predictors are being considered for prediction purposes [@Dunson2020TargRandProj]. Therefore, screening out irrelevant variables before performing the random projection is advisable in order to tackle this issue. The screening can be performed in a probabilistic fashion, by randomly sampling covariates for inclusion in the model based on probabilities proportional to an importance measure (as opposed to random subspace sampling employed in, e.g.,
random forests). The [R]{.proglang} landscape for variable screening techniques is very rich. An overview of some notable packages on the Comprehensive [R]{.proglang} Archive Network (CRAN) includes the following packages. 
Package [SIS]{.pkg} [@SISR], which implements
the (iterative) sure independence screening procedure and its extensions, as detailed in @Fan2007SISforUHD, @Fan2010sisglms, @fan2010high. This package also provides functionality for estimating a penalized generalized linear model or a cox regression model for the variables selected by the screening procedure.
Package [VariableScreening]{.pkg} [@pkg:VariableScreening] offers screening methods for iid data, varying-coefficient models, and longitudinal data and includes techniques such as sure independent ranking and screening (SIRS), which ranks the predictors by their correlation with the rank-ordered response, or distance correlation sure independence screening (DC-SIS), a non-parametric extension of the correlation coefficient.
Package [MFSIS]{.pkg} [@pkg:MFSIS] provides a collection of model-free screening techniques including SIRS, DC-SIS, the fused Kolmogorov filter [@mai2015fusedkolmogorov] the projection correlation method using knock-off features [@liu2020knockoff], among others. Additional packages implement specific procedures but their review is beyond
the scope of the current paper.
<!-- Further packages are available which implement specific procedures: @pkg:tilting, @pkg:cdcsis, @pkg:QCSIS, @pkg:LqGm @pkg:fusionclust.  -->
<!-- Package [SMLE]{.pkg} [@pkg:SMLE] implements joint feature screening via sparse MLE [@SMLE2014] in high-dimensional linear, logistic, and Poisson models. Package [TSGSIS]{.pkg} [@pkg:TSGSIS] provides a high-dimensional grouped variable selection approach for detecting interactions that may not have marginal effects in high dimensional linear and logistic regression [@10.1093/bioinformatics/btx409]. -->

Combining variable screening with random projection is effective in reducing the
predictor set. Moreover, in practice, the information from multiple such screening and projections can be combined by averaging, in order to reduce the variance introduced by the random sampling (of both projections and screening indicators) [@Thanei2017RPforHDR].
To address these points, [spar]{.pkg} builds an ensemble of GLMs in the spirit 
of @Dunson2020TargRandProj and @parzer2024glms where, in each model of the ensemble, i) variables are first screened based on a screening coefficient, ii) the selected variables are then projected to a lower dimensional space, iii) a GLM is estimated using the projected predictors.

The algorithm implemented in [spar]{.pkg} thus provides a computationally efficient procedure
for high-dimensional regression which accommodates for both discrete and continuous responses. A variety of screening coefficients and procedures for generating random
projection matrices (both data-agnostic and data-driven) are provided. However,
the package is designed with flexibility in mind, providing users with a versatile framework to extend the implemented screening and projection techniques with their own custom procedures. This is facilitated by leveraging [R]{.proglang}'s 
[S]{.proglang}3 classes, making the process convenient and user-friendly. 
Therefore, users can seamlessly integrate various techniques by either
writing their own procedures or leveraging the existing [R]{.proglang} packages.

The package provides methods such `plot`, `predict`, `coef`,
`print`, which allow users to more easily interact with the model output and analyze the results. The GLM framework,
especially when combined with random projections which 
preserve information on the original coefficients 
[such as the one in @parzer2024glms], facilitates interpretability of the model output, allowing users to  understand variable effects.

While [spar]{.pkg} offers the first implementation of the 
described algorithm and, to the best of our knowledge, no 
other package offers the same functionality for GLMs, few other [R]{.proglang} packages
focus on building ensembles of *classifiers* where the  dimensionality of the predictors is reduced. 
Most notably, package [RPEnsemble]{.pkg}  [@RPEnsembleR]
implements the procedure in @cannings2017random where
"carefully-selected" random projections are used for projecting the predictors before they are employed in a classifier such as $k$-nearest neighbor, linear or quadratic discriminant analysis. 
On the other hand, package [RaSEn]{.pkg} [@pkg:RaSEn] implements the RaSE algorithm for ensemble classification and regression problems, where random subspaces are generated and the optimal one is chosen to train a weak learner on the basis of some criterion.


<!-- Regarding variable screening, users can seamlessly integrate various techniques by either writing their own procedures or leveraging the  -->

<!-- Therefore, no other package in [R]{.proglang} -->
<!-- provides the functionality of [spar]{.pkg} for GLMs.  -->




<!-- Various choices of base classifiers are implemented, for instance, linear discriminant analysis, quadratic discriminant analysis, $k$-nearest neighbor, logistic or linear regression, decision trees, random forest, support vector machines. The selected percentages of variables can be employed for variable screening. -->

<!-- Package [Ball]{.pkg} [@pkg:ball] provides functionality for variable screening using ball statistics, which is appropriate for shape, directional, compositional and symmetric positive definite matrix data. -->

<!-- Package [BayesS5]{.pkg} [@pkg:BayesS5] implements Bayesian variable selection using simplified shotgun stochastic search algorithm with screening [@shin2017scalablebayesianvariableselection] while package [bravo]{.pkg} [@pkg:bravo] implements the Bayesian iterative screening method proposed in [@wang2021bayesianiterativescreeningultrahigh]. -->

The rest of the paper is organized as follows: @sec-models provides the methodological details of the implemented algorithm. The package is described in @sec-software and @sec-extensibility exemplifies how a new screening coefficient and a new random projection can be integrated in the package. @sec-illustrations contains two examples of employing the package on real data sets. Finally, @sec-conclusion concludes.

## Methods {#sec-models}

The package implements a procedure for building an ensemble 
of GLMs where we employ screening and random projection 
to the predictor matrix pre-model estimation for the purpose of
dimensionality reduction.

Throughout the section we assume to observe 
high-dimensional data $\{(\boldsymbol{x}_i,y_i)\}_{i=1}^n$, 
where $\boldsymbol{x}_i\in\mathbb{R}^p$ is a predictor vector and $y_i\in\mathbb{R}$ is the response, with $p\gg n$. The predictor vectors are collected in the rows of the predictor matrix $X\in \mathbb R^{n\times p}$.

### Variable screening

In this section we provide an overview of possible approaches to performing variable screening in a high-dimensional setting. In high-dimensional modeling, the goal of variable screening is to reduce the predictor set by selecting a small subset of variables with a strong *utility* to the response variable. This initial selection enables more efficient downstream analyses by discarding less relevant predictors early in the modeling process, thus reducing computational costs and potential noise accumulation stemming from irrelevant variables [see e.g., @Dunson2020TargRandProj].

Classic approaches such as sure independence screening (SIS),
proposed by @Fan2007SISforUHD, use the vector of marginal empirical correlations 
$\hat\omega=(\omega_1,\ldots ,\omega_p)^\top\in\mathbb{R}^p,\omega_j=\text{Cor}(X_{j},y)$, 
where $y$ is the $(n\times 1)$ vector of responses and 
$X_{j}$ is the $j$-th column of the matrix of predictors,
to screen predictors in a linear regression setting
by selecting the variable set $\mathcal{A}_\gamma = \{j\in [p]:|w_j|>\gamma\}$ depending on a threshold $\gamma>0$, 
where $[p]=\{1,\dots,p\}$. Under certain technical conditions,  this screening coefficient has the *sure screening property*
$\Prob(\mathcal{A} \subset \mathcal{A}_{\gamma_n})\to 1 \text{ for } n\to \infty$,  where $\mathcal{A}=\{j\in[p]:\beta_j\neq 0\}$ is the set of truly active variables.
Extensions to SIS include modifications for GLMs [@Fan2010sisglms], where screening
is performed based on the log-likelihood or the slope coefficient of the GLM containing only $X_j$ as a predictor: $\hat\omega_j=: \text{argmin}_{\beta\in\mathbb{R}}\text{min}_{{\beta_0}\in\mathbb{R}}\sum_{i=1}^n -\ell(\beta_j,\beta_0;y_i,x_{ij})$, where $x_{ij}$ is the $j$-th entry of the vector $x_i$.

However, both mentioned approaches face limitations related to 
the required technical conditions which can rule out practically possible
scenarios where an important variable is marginally uncorrelated to the response due to their multicollinearity. To tackle these issues,
@fan2009ultrahigh propose to 
use an iterative procedure where SIS is applied subsequently on the residuals of the model estimated in a previous step.
Additionally, in a linear regression setting,  @Wang2015HOLP
propose employing the ridge estimator when the penalty term converges to zero while @cho2012high propose using the tilted correlation, i.e., the correlation of a tilted version of $X_j$ with $y$ where the effect of other variables is reduced. For discrete outcomes, joint feature screening [@SMLE2014] has been proposed. 

In order to tackle potential model misspecification, a rich stream of literature focuses on developing semi- or non-parametric alternatives to SIS. For linear regression, approaches include using the ranked correlation [@zhu2011model], (conditional) distance correlation [@li2012feature;@wang2015conditional] or quantile correlation [@ma2016robust].
For GLMs, @fan2011nonparametric extend @Fan2010sisglms by fitting a generalized additive model with B-splines. Further extensions for discrete (or categorical) outcomes include the fused Kolmogorov filter [@mai2013kolmogorov], the mean conditional variance, i.e., the expectation in $X_j$ of the variance in the response of the conditional cumulative
distribution function $\Prob(X\leq x|Y)$ [@cui2015model].
@ke2023sufficient propose a model free method where the contribution of each individual predictor is quantified marginally and conditionally in the presence of the control variables as well as the other candidates by reproducing-kernel-based $R^2$ and partial 
$R^2$ statistics.


[spar]{.pkg} allows the integration of various advanced screening techniques using a flexible framework, which in turn enables users to apply various screening methods tailored to their data characteristics in the algorithm generating the ensemble. This flexibility allows users to evaluate different strategies, ensuring that the most effective approach is chosen for the specific application at hand. Moreover, it incorporates probabilistic screening strategies, which can be
particularly useful in ensembles, as they enhance the diversity of predictors across ensemble models. Instead of relying on a fixed threshold or number of predictors to be screened, predictors are sampled with probabilities proportional to their screening score [see @Dunson2020TargRandProj;@parzer2024glms].


### Random projection tools {#sec-rps}

[spar]{.pkg} has been designed to allow the incorporation of various random projection techniques, enabling users to tailor the procedure to their specific data needs. Below, we provide background information on random projection techniques and an overview of possible choices for building such random projection matrices.

The random projection method relies on the Johnson-Lindenstrauss (JL) lemma [@JohnsonLindenstrauss1984], which asserts that there exists a linear map $\Phi\in \mathbb{R}^{m \times p}$ that embeds any set of points in $p$-dimensional Euclidean space collected in the rows of $X\in \mathbb{R}^{n\times p}$ into an $m$-dimensional Euclidean space where pairwise distances are approximately preserved within a factor of $1\pm\epsilon)$ for $m\leq m_0=\mathcal O(\epsilon^{-2}\log(n))$.
<!-- and it also gives a lower bound on the goal dimension $m$ in order to preserve the distances between all pairs of points within a factor $1\pm \varepsilon$: -->
<!-- $m>\frac{24\log n}{3\varepsilon^2-2\varepsilon^3}$ for any $0 <\varepsilon< 1$.  -->
Computationally, an attractive feature of the method for high-dimensional settings is that the bound does not depend on $p$.

The random map $\Phi$ that satisfies the JL lemma with high probability should be chosen such that it fulfills certain conditions [see @JohnsonLindenstrauss1984] and the literature focuses on constructing such matrices either by sampling them from some "appropriate" distribution, 
by inducing sparsity in the matrix and/or by employing specific fast constructs which lead to efficient matrix-vector multiplications.
It turns out that the conditions are generally satisfied by
nearly all sub-Gaussian distributions [@matouvsek2008variants]. A common choice is  the standard normal
distribution $\Phi_{ij} \overset{iid}{\sim} N(0,1)$ [@FRANKL1988JLSphere] or a sparser version where
$\Phi_{ij}\overset{iid}{\sim} N(0,1/\sqrt{\psi})$ 
with probability $\psi$
and $0$ otherwise [@matouvsek2008variants].
Another computationally simpler option  is the Rademacher distribution where
$\Phi_{ij} =  \pm 1/\sqrt{\psi}$ with probability $\psi/2$ and zero otherwise for  $\quad 0<\psi\leq 1$,
where @ACHLIOPTAS2003JL shows results for $\psi=1$ and $\psi=1/3$ while @LiHastie2006VerySparseRP recommend using $\psi=1/\sqrt{p}$ to obtain very sparse matrices.

Further approaches include using the Haar measure to generate random orthogonal matrices [@cannings2017random] or 
a non-sub-Gaussian distribution like the standard Cauchy, 
proposed by @LiHastie2006VerySparseRP for preserving approximate $\ell_1$ distances in settings where the data is high-dimensional, non-sparse, and heavy-tailed.
<!-- TODO: citation -->
Structured matrices, which allow for more efficient multiplication, have also been proposed
[see e.g., @ailon2009fast;@Clarkson2013LowRankApprox].
<!-- propose the fast Johnson- Lindenstrauss transform (FJLT), where the random projection matrix is given by $\Phi=PHD$ with $P$ random and sparse, $P_{ij} \sim N (0, 1/q)$ with probability $1/q$ and $0$ otherwise, $H$ the normalized Hadamard (orthogonal) matrix $H_{ij} = p^{-1/2}(-1)^{\langle i-1,j-1\rangle}$, where $\langle i, j\rangle$ is the dot-product of the $m$-bit vectors $i$, $j$ expressed in binary, and $D = \text{diag}(\pm 1)$ is a diagonal matrix with random elements $D_{ii}$. -->
<!-- TODO: find a refernce for this, check if it holds -->
<!-- An orthonormalization is usually applied  $(\Phi\Phi^\top)^{-1/2}\Phi$. -->
<!-- Orthonormalization can constitute a computational bottleneck -->
<!-- for the random projection method, however, in high-dimensions it can be omitted.  -->

The conventional random projections mentioned above are 
data-agnostic. However, recent work has proposed incorporating information  from the data either to select the "best" random projection or to directly inform the random projection procedure.
For example, @cannings2017random
build an ensemble classifier where 
the random projection matrix is chosen by 
selecting the one that minimizes the test error of the 
classification problem among a set of data-agnostic random projections. 

On the other hand,  @parzer2024glms propose
to use a random projection matrix for GLMs which directly incorporates 
information about the relationship between the predictors and 
the response in the projection matrix, rather than a projection matrix which satisfies the JL lemma. @parzer2024sparse also provide in the linear regression 
a theoretical bound on the expected gain in prediction error in using a
projection which incorporates information
about the true $\beta$ coefficients compared to a conventional random projection.
Motivated by this result, they propose to construct a projection matrix using 
the sparse embedding matrix of @Clarkson2013LowRankApprox, where the random 
diagonal elements are replaced in practice by a ridge coefficient with a minimal 
$\lambda$ penalty. This method has the advantage of allowing the regression coefficients to be recovered after the projection,
which is in general not possible with conventional random projections 
[@Thanei2017RPforHDR]. 

Moreover, another data-driven approach to random projection for  regression has been proposed by  @ryder2019asymmetric,
who propose a data-informed random projection using an asymmetric transformation of the predictor matrix without using information of the response.


<!-- @Clarkson2013LowRankApproxShort propose a sparse embedding matrix ${\Phi=BD}$, where $B\in\{0,1\}^{m \times p}$ is random binary matrix and $D$ is a $p\times p$ diagonal matrix with $(D_{ii}+1)/2\sim \text{Unif}\{0,1\}$, and prove that the dimension $m$ is bounded by -->
<!-- a polynomial in $r\varepsilon^{-1}$ for $0 <\varepsilon< 1$ and $r$ being the rank of $X$. While this is generally larger than that of FJLT, the sparse embedding matrix requires less time to compute $\Phi X$ compared to other subspace embeddings. -->




<!-- see more info at https://www.cs.waikato.ac.nz/~bobd/ECML_Tutorial/ECML_handouts.pdf. -->

<!-- See also https://web.math.princeton.edu/~amits/publications/LeanWalsh_published.pdf. -->

<!-- https://arxiv.org/pdf/1710.03163.pdf -->

<!-- https://cseweb.ucsd.edu/~dasgupta/papers/randomf.pdf -->

<!-- https://people.math.ethz.ch/~nicolai/mv/notes6.pdf -->

<!-- https://web.math.princeton.edu/~amits/publications/LeanWalsh_published.pdf -->

<!-- https://pastel.hal.science/tel-01481912/document -->


### Generalized linear models 

After we perform in each marginal model an
initial screening step followed
by a projection step, 
we assume that the reduced and projected set of 
predictors $\boldsymbol{z}_i$ together with the response arises from a GLM with the response having conditional density from a (reproductive) exponential dispersion family of the form 
\begin{align}\label{eqn:y_density}
  f(y_i|\theta_i,\phi) = \exp\Bigl\{\frac{y_i\theta_i- b(\theta_i)}{a(\phi)} + c(y_i,\phi)\Bigr\},
  \quad
    g(\E[y_i|\boldsymbol{z}_i]) = \gamma_0 + \boldsymbol{z}_i^\top\boldsymbol{\gamma}=:\eta_i,
\end{align}
where $\theta_i$ is the natural parameter,  $a(.)>0$ and $c(.)$ are specific real-valued functions determining different families, $\phi$ is a dispersion parameter, and 
$b(.)$ is the log-partition function normalizing the density to integrate to one. If $\phi$ is known, we obtain densities in the natural exponential family for our responses. 
The responses are related to the $m$-dimensional reduced and projected predictors through the conditional mean, i.e., 
the conditional mean of $y_i$ given ${\boldsymbol{z}}_i$
depends on a linear combination of the predictors through a (invertible) link function $g(.)$, where $\gamma_0\in\mathbb{R}$ is the intercept and $\boldsymbol{\gamma}\in\mathbb{R}^m$ is a vector of regression coefficients for the $m$ projected predictors.

Given that $m$, the goal dimension of the projection need not necessarily be small in comparison to $n$ 
(we recommend using a dimension of around $n/2$),
we observe that adding a small $L_2$ penalty in the marginal models, especially for the binomial family, can make estimation more stable as it alleviates problems
related to separation. The marginal models we estimate therefore
involve minimizing the following function:
$$
 \text{argmin}_{{\gamma}\in\mathbb{R}^m}\min_{\gamma_0\in\mathbb{R}}  \sum_{i=1}^n -\ell(\gamma_0, \gamma;y_i,\boldsymbol{z}_i) + \frac{\varepsilon}{2}\sum_{j=1}^m{\gamma}_j^2, \, \varepsilon > 0.
$$
We will employ for this purpose the 
function `glmnet()` of package [glmnet]{.pkg} 
[@glmnet2023] in the estimation of the marginal models.

### Algorithm {#sec-algo}

We present the general algorithm implemented in package 
[spar]{.pkg}. 

1.   Choose family with corresponding log-likelihood $\ell(.)$ and link.

2.   Standardize the $(n\times p)$ matrix of predictors $X$ by subtracting the sample column mean and dividing by the sample standard deviation. Additionally, for Gaussian response standardize $y$ to by subtracting the sample mean and dividing by the sample standard deviation.

3.   Calculate screening coefficients $\hat\omega$.

4.   For $k=1,\dots,M$ models:

  4.1.  If $p>2n$, screen $2n$ predictors based on the screening coefficient $\hat\omega$, which yields for model $k$ the screening index set $I_k=\{j_1^k,\dots,j_{2n}^k\}\subset[p]$; if probabilistic screening should be employed draw the predictors sequentially without replacement using an initial vector of probabilities $p_j\propto |\hat\omega_j|$. Otherwise, select the $2n$ variables with the highest $|\hat\omega_j|$. If $p < 2n$, perform no screening and $I_k=\{1,\dots,p\}$.
  
  4.2.   project screened variables to a random dimension $m_k\sim \text{Unif}\{\log(p),\dots,n/2\}$ using \textbf{projection matrix} $\Phi_k$ to obtain $Z_k=X_{\cdot I_k}\Phi_k^\top \in \mathbb{R}^{n\times m_k}$, 
  where $X_{\cdot I_k}$ contains the columns in $X$ having a column index in 
  $I_k$.
  
  4.3.  fit a ($L_2$ penalized) \textbf{GLM} of $y$ on $Z_k$ to obtain estimated coefficients $\widehat\gamma^k\in\mathbb{R}^{m_k}$ and $\hat \beta_{I_k}^k=\Phi_k^\top\widehat\gamma^k$, $\hat \beta_{\bar I_k}^k=0$.

5.   For a given threshold $\nu>0$, set all $\hat\beta_j^k$ with $|\hat\beta_j^k|<\nu$ to $0$ for all $j,k$.

6.   Choose $M$ and $\nu$ via a validation set or cross-validation by repeating steps 1 to 4 and employing a
loss function on the test set $K(M, \nu)$ 
\begin{align}
         (M_{\text{best}},\nu_{\text{best}}) = \text{argmin}_{M,\nu}K(M,\nu)
       \end{align}

7.   Combine models of the ensembles via the coefficients using **simple average** 
$\hat \beta = \sum_{k=1}^M\hat \beta^k / M$.

8. Output the estimated coefficients and predictions for the chosen $M$ and $\nu$.

## Software {#sec-software}

The package can be installed from [GitHub]{.proglang} 


```{r eval=FALSE}
devtools::install_github("RomanParzer/SPAR")
```


and loaded by


```{r}
library("spar")
```


In this section we rely for illustration purposes on the example data set from the package which contains $n=200$ observations of a continuous response `y` and $p=2000$ predictors `x` which can be used as a training data set and $n=100$ observations to be used as a test set.


```{r}
data("example_data", package = "spar")
str(example_data)
```


This data set has been simulated from a linear regression model
with $\sigma^2=83$, an intercept $\mu=1$ and $\beta$ coefficients
with 100 non-zero entries, where the non-zero entries are uniformly sampled from $\{-3,-2,-1,1,2,3\}$.


### Main functions and their arguments

The two main functions for fitting the SPAR algorithm are:
```
spar(x, y, family = gaussian("identity"), rp = NULL, screencoef = NULL,
  xval = NULL, yval = NULL, nnu = 20, nus = NULL, nummods = c(20),
  measure = c("deviance", "mse", "mae", "class", "1-auc"),
  inds = NULL, RPMs = NULL, ...)
```
which implements the algorithm in @sec-algo without cross-validation and returns an object of class [spar]{.class}, 
and
```
spar.cv(x, y, family = gaussian("identity"), rp = NULL, screencoef = NULL,
  nfolds = 10, nnu = 20, nus = NULL, nummods = c(20),
  measure = c("deviance", "mse", "mae", "class", "1-auc"), ...)
```
which implements the cross-validated procedure and returns an object of class [spar.cv]{.class}.

The common arguments of these functions are:

-   `x`  an $n \times p$ numeric matrix of predictor variables,

-   `y` numeric response vector of length $n$,

-   `family` object from [stats::family]{.fct}; defaults to `gaussian()`; 

-   `rp` an object of class [randomprojection]{.class}, defaults to `rp_cw(data=TRUE)`;

-   `screencoef` an object of class [screencoef]{.class},
defaults to `screen_glmnet()`;

-    `nnu` is the number of threshold values $\nu$ which should be considered for thresholding; defaults to 20;

-   `nus` is an optional vector of $\nu$ values to be considered for thresholding. If it is not provided, is defaults to a grid of `nnu` values. This grid is generated by including zero and `nnu`$-1$ quantiles of the absolute values of the estimated coefficients from the marginal models,
chosen to be equally spaced  on the probability scale .


-   `nummods` is the number of models to be considered in the ensemble;
    defaults to 20. If a vector is provided, all combinations of `nus` and 
     `nummods` are considered when choosing the optimal $\nu_\text{best}$ and $M_\text{best}$.

-   `measure` gives the measure based on which the thresholding value $\nu_\text{opt}$ and the number of models `M` should be chosen on the validation set (for `spar()`) or in each of the folds (in `spar.cv()`). Defaults to `"deviance"`, which is available for all families. Other options are `"mse"` or `"mae"` (between responses and predicted means, for all families), 
`"class"` (misclassification error) and `"1-auc"` (one minus area under the ROC curve) both just for binomial family.

Furthermore, `spar()` has the specific arguments:

-  `xval` and  `yval` which are used as validation sets for choosing $\nu_\text{best}$ and $M_\text{best}$. If not provided, `x` and  `y` will be employed.

- `inds` is an optional list of length `max(nummods)` containing column index-vectors corresponding to variables that should be kept after screening for each marginal model; dimensions need to fit those of the dimensions of the provided matrices in `RPM`.

-   `RPMs` is an optional list of length `max(nummods)` which contains  projection matrices to be used in each marginal model. 

Function `spar.cv()` has the specific argument `nfolds` which is the number of folds to be used for cross-validation. It relies on `spar()`
as a workhorse, which is called for each fold. The random projections
for each model are held fixed throughout the cross-validation
to reduce the computational burden. This is possible by calling `spar()`
in each fold with a predefined `inds` and `RPMs` argument,
which are generated by first calling `spar()` on the whole 
dataset, before starting the cross-validation procedure. 
However, it is possible to specify whether the data associated with the random projection (relevant for data-driven random projections) should be updated in each fold iteration with the corresponding training data. This is achieved by modifying elements of the [randomprojection]{.class} object, which we will exemplify in @sec-extensibility.



### Screening coefficients

The objects for creating screening coefficients are implemented as \proglang{S}3 classes [screencoef]{.class}. These objects are created by
several implemented `screen_*` functions, which take `...` and a list of controls `control`
as arguments. 

Consider as an example function `screen_marglik` which
implements a screening procedure based on  the coefficients of univariate GLMs:


```{r}
screen_marglik
```



Arguments related to the screening procedure can be passed through `...`, and will be saved as attributes of the [screencoef]{.class} object.
More specifically, the following attributes are relevant for function `spar()`:

  * `nscreen` integer giving the number of variables to be retained after screening; defaults to $2n$.
  
  * `split_data_prop`, double between 0 and 1 which indicates the proportion of the data that should be used for computing the screening coefficient. The remaining data will be used for estimating the marginal models in the SPAR algorithm; defaults to `NULL`. In this case the whole data will be used for estimating the 
  screening coefficient and the maginal models.
  
  * `type` character -- either `"prob"` (indicating that probabilistic screening
  should be employed)  or `"fixed"` (indicating that a fixed set of `nscreen`
  variables should be employed across the ensemble; defaults to `type = "prob"`.
  
The `control` argument, on the other hand, is a list
containing extra parameters to be passed to the 
main function computing the screening coefficients. 


The following screening coefficients are implemented in [spar]{.pkg}:

* `screen_marglik()` - computes the screening coefficients by the coefficient of $x_j$ for $j =1,\dots,p$  in a univariate GLM using the `stats::glm()` function. 
 $$
 \hat\omega_j=:\text{argmin}_{\beta_j\in \mathbb{R}}\text{min}_{{\beta_0}\in\mathbb{R}}\sum_{i=1}^n -\ell(\beta_0,\beta_j;y_i,x_{ij})
 $$
 It allows to pass a list of controls through the `control` argument to `stats::glm` such as weights, family, offsets.

* `screen_cor()` -- computes the screening coefficients by the correlation between $y$ and $x_j$ using the function `stats::cor()`. It allows to pass a list of controls through the `control` argument to `stats::cor`.

* `screen_glmnet()` -- computes by default the ridge coefficient 
where the penalty $\lambda$ is very small [see @parzer2024glms for clarification]. 
$$
\hat\omega=: \text{argmin}_{{\beta}\in\mathbb{R}^p}\sum_{i=1}^n -\ell(\beta;y_i,x_i) + \frac{\varepsilon}{2}\sum_{j=1}^p{\beta}_j^2, \, \varepsilon > 0
$$
The function 
relies on `glmnet::glmnet()` and, while it assumes by 
default $\alpha = 0$ and a small penalty, it allows to pass a list of 
controls through the `control` argument to `glmnet::glmnet()` such 
as `alpha = 1`. Used as default if `screencoef = NULL` in function call of
`spar()` or `spar.cv()`.



All implemented `screen_*` functions return an object of class [screencoef]{.class}
which in turn is a list with three elements: 
  
  * a character `name`, 
  
  * `generate_fun` -- an [R]{.proglang} function for generating the screening coefficient. This
  function should have as following arguments the [screencoef]{.class} object itself and `data`, which should be a list of `x` -- the matrix of standardized predictors --  and `y` -- the vector of (standardized in the Gaussian case) responses. It returns a
  vector of screening coefficients of length $p$.
  
  * `control`, which is the control list passed by the user in `screen_*`. These controls are 
  arguments which are needed in `generate_fun()` in order to generate the desired 
  screening coefficients.
  


For illustration purposes, consider the object created by calling `screen_marglik()`:


```{r}
obj <- screen_marglik()
```


A user-friendly `print` of the [screencoef]{.class} is provided:


```{r}
obj
```


The structure of the object is the following:


```{r}
unclass(obj)
```


Function `generate_fun()` defines the generation of the screening coefficient. Note that it considers the controls in `object$control` when calling
the `stats::glm()` function (unless it is provided, the `family` argument in `stats::glm()` will be set to the "global" family of the SPAR algorithm which is assigned
inside the `spar()` function an attribute for the `screencoef` object). 


For convenience, a constructor function `constructor_screencoef()`
is provided, which can be used to create new `screen_*` functions.  An example is presented in @sec-extensscrcoef.


### Random projections

Similar to the screening procedure, 
the objects for creating random projections are
implemented as \proglang{S}3
classes [randomprojection]{.class} and are created by functions
`rp_*(..., control = list())`, 
which take `...` and a list of controls `control` as arguments. 

Arguments related to the random projection
can be passed through `...`, which will
then be saved as attributes of the [randomprojection]{.class} object.
More specifically, the following attributes are relevant in
the SPAR algorithm:

  * `mslow`: integer giving the minimum dimension to which the predictors should
  be projected; defaults to $\log(p)$.
  
  * `msup`: integer giving the maximum dimension to which the predictors should
  be projected; defaults to $n/2$.
  
  * `data`: boolean indicating whether the random projection is data-driven.
  
Note that for random projection matrices which satisfy the JL lemma, `mslow`
can be determined by employing existing results which give 
a lower bound on the goal dimension in order to preserve the distances between 
all pairs of points within a factor $(1 \pm \epsilon)$.
For example, @ACHLIOPTAS2003JL show $m_0 = \log n(4 + 2\tau)/(\epsilon^2/2 − \epsilon^3/3)$ for probability
$1 − n^{-\tau}$.


The following random projections are implemented in [spar]{.pkg}:

* `rp_gaussian()` -- random projection object where the generated matrix will have iid entries from a normal distribution (defaults to standard normal entries)

* `rp_sparse()` -- random projection object where the generated matrix will be the one in [@ACHLIOPTAS2003JL] with `psi=1` by default.

* `rp_cw()` -- sparse embedding random projection in [@Clarkson2013LowRankApprox]  for `rp_cw(data = FALSE)`. Defaults to `rp_cw(data=TRUE)`, which replaces the random elements on the diagonal by the ridge coefficients with a small penalty, as introduced in @parzer2024glms.


The `rp_*` functions return an object of class [randomprojection]{.class}
which  is a list with three elements: 
  
  * `name`, 
  
  * `generate_fun` function for generating the random projection matrix. This
    function should have arguments \code{rp}, which is itself a [randomprojection]{.class}
    object, \code{m}, the target dimension and a vector of indices
    \code{included_vector} which indicates the column index of the original variables
    in the \code{x} matrix to be projected using the random projection. 
    This is needed due to the fact that screening can be employed pre-projection. It can return a matrix or a sparse matrix of class 
    "`dgCMatrix`" of the [Matrix]{.pkg} with \code{m} rows and 
    \code{length(included_vector)} columns.

  * `update_data_rp` optional function used for data-driven random projections, which updates the [randomprojection]{.class} object with data information which is relevant for the random projection. The updating happens only once, before the start of the SPAR algorithm, where appropriate attributes are added to the [randomprojection]{.class} object. All relevant quantities
  are to be used in the `generate_fun()` function.
  This
function should have arguments \code{rp}, which is a [randomprojection]{.class} object to be updated and `data`,  which should be a list of `x` -- the matrix of standardized predictors --  and `y` -- the vector of (standardized in the Gaussian case) responses. Returns a `randomprojection` object.
  
  * `update_rpm_w_data` optional function for updating the random projection matrices provided in the argument `RPMs` of functions `spar` and `spar.cv` with data-dependent parameters. While `update_data_rp` is employed only once at the start of the algorithm, `update_rpm_w_data` specifies how to modify each random projection provided in `RPMs`. 
This is particularly relevant for the cross-validation procedure, which employs the random projection matrices 
generated by calling the `spar()` function on the whole dataset before starting the cross-validation exercise.For example, in our implementation of the data-driven `rp_cw()`, we only update `RPMs` by adjusting with the vector of 
screening coefficients computed on the current training data in each fold, but do not modify the random elements in each fold, to reduce the computational burden.
Defaults  to `NULL`. If not provided, the values of the provided `RPMs` do not change. 
 
  * `control`, which is the control list in `rp_*`. These controls are arguments needed in `generate_fun()` in order to generate the desired random projection.
  

For illustration purposes, consider the implemented function
`rp_gaussian`, which is a random projection with entries drawn from the normal distribution.
The `print` method returns key information about the random projection procedure.


```{r}
obj <- rp_gaussian()
obj
```


We turn to looking at the structure of the object:


```{r}
unclass(obj)
```


The `generate_fun()` function returns a matrix with  `m` rows and `length(included_vector)` columns.
Note that `included_vector` gives the indices of the 
variables which have been selected by the screening 
procedure. In this case, where the random projection does 
not use any data information, we are only interested in the 
length of this vector. 

The element `update_data_fun` and `update_rpm_w_data` are `NULL` as this conventional 
random projection is data-agnostic. 


### Methods 


Methods `print`, `plot`, `coef`, `predict` are available for both [spar]{.class}
and [spar.cv]{.class} classes.

#### print
The `print` method returns information on $\nu_\text{best}$
$M_\text{best}$, the number of active predictors
(i.e., predictors which have at least a nonzero coefficient across the marginal models)
and a five-point summary of the non-zero coefficients.


```{r}
spar_res <- spar(example_data$x, example_data$y,
                 xval = example_data$xtest,
                 yval = example_data$ytest,
                 nummods = c(5,10,15,20,25,30))
spar_cv <- spar.cv(example_data$x, example_data$y,
                   nummods = c(5,10,15,20,25,30))
```

```{r}
spar_res
```

```{r}
spar_cv
```




#### coef
Method `coef` takes as inputs a `spar` or `spar.cv` object, together with further arguments: 

* `nummod` --	number of models used to compute the averaged coefficients; value of `nummod` with minimal  `measure` is used if not provided.

* `nu` -- threshold level used to compute the averaged coefficients; value with minimal  `measure` is used if not provided.



```{r}
str(coef(spar_res))
```



It returns a list with the intercept, vector of `beta` coefficients and the `nummod` and `nu` employed in the calculation.

Additionally for [spar.cv]{.class}, the `coef` method 
also has argument `opt_par` which is one of 
`c("1se","best")` and chooses whether to select the best pair of `nus` and `nummods` according to cross-validation 
`measure`, or the solution yielding sparsest vector of coefficients
within one standard deviation
of that optimal cross-validation 
`measure`. This argument is ignored when `nummod` and `nu` are given.

#### predict
Functionality for computing predictions is provided through the method `predict` which takes a `spar` or `spar.cv` object, 
together with

* `xnew` --	matrix of new predictor variables; must have same number of columns as `x`.

* `type` --	the type of required predictions; either on `"response"` level (default) or on `"link"` level

* `avg_type` --	type of averaging used across the marginal models; either on `"link"` (default) or on `"response"` level

* `nummod` --	number of models used to compute the averaged coefficients; 
value of `nummod` with minimal  `measure` is used if not provided.

* `nu` -- threshold level used to compute the averaged coefficients; value with minimal  `measure` is used if not provided.

* `coef` --	optional vector of coefficients to be used directly in the prediction.

Additionally, for class [spar.cv]{.class}, argument `opt_par` is available and used in the computation of the 
coefficients to be used for prediction (see above description of  method `coef`). 


#### plot

Plotting functionality is provided through the `plot` method, 
which takes a `spar` or `spar.cv` object, 
together with further arguments:

* `plot_type` -- one of:

  * `"Val_Measure"` plots the (cross-)validation `measure` for either a grid of `nu` values for a fixed number of models `nummod` or viceversa.

  * `"Val_numAct"` plots the number of active variables for either a grid of `nu` values for a fixed number of models `nummod` or viceversa.
  
  * `"res-vs-fitted"` produces a residuals-vs-fitted plot. The residuals are computed as $y- \widehat y$, where $\widehat y$ is the prediction computed on response level.
   
  * `"coefs"`  produces a plot of the value of the standardized coefficients for each predictor in each marginal model (before thresholding).  For each predictor, the values of the coefficients are sorted from largest to smallest 
  across the marginal models and then represented in the plot.

* `plot_along` -- one of `c("nu","nummod")`; for 
`plot_type = "Val_Measure"` or `plot_type = "Val_numAct"` it indicates whether the values of the cross-validation measure or number of active variables, respectively, should be shown for a grid of $\nu$ values while keeping the number of models `nummod` fixed or viceversa. This argument is ignored when `plot_type = "res-vs-fitted"` or `plot_type = "coefs"`.

* `nummod` -- fixed value for number of models when `plot_along = "nu"` for `plot_type = "Val_Measure"` or `"Val_numAct"`;  if `plot_type = "res-vs-fitted"`, it is used in the `predict` method, as described above.

* `nu` -- fixed value for  $\nu$ when `plot_along = "nummod"` 
for `plot_type = "Val_Measure"` or `"Val_numAct"`; if `plot_type = "res-vs-fitted"`, it is used in the `predict` method, as described above.


* `xfit` -- if `plot_type = "res-vs-fitted"`, it is the matrix of
predictors used in computing the fitted values. Must provided for this plot, as the `spar` or `spar.cv` objects do not store the original data.

* `yfit` -- if `plot_type = "res-vs-fitted"`, vector of
responses used in computing the residuals. Must provided for this plot, as the `spar` or `spar.cv` objects do not store the original data.

* `prange` -- optional vector of length 2 in case `plot_type = "coefs"` which gives the limits of the predictors' plot range; defaults to `c(1, p)`.

*  `coef_order` --  optional index vector of length $p$ in case
`plot_type = "coefs"` to give the order of the predictors; defaults to `1 : p`.

For class [spar.cv]{.class} there is the extra argument
`opt_par = c("best", "1se")` which, for `plot_type = "res-vs-fitted"` indicates whether the predictions 
should be based on coefficients using the best $(\nu, M)$ combination or on the one which delivers the sparsest 
$\beta$ having validation measure within one standard deviation from the minimum

The `plot` methods return objects of class "`ggplot`" [@ggplotR].

## Extensibility {#sec-extensibility}
 
###  Screening coefficients {#sec-extensscrcoef}

We exemplify how new screening coefficients implemented in package 
[VariableScreening]{.pkg} can easily be used in the framework of
[spar]{.pkg}. 

We start by defining the function for generating the screening coefficients
using the `screenIID()` function in [VariableScreening]{.pkg}.


```{r}
generate_scr_sirs <- function(object, data) {
  res_screen <- do.call(function(...) 
    VariableScreening::screenIID(data$x, data$y, ...), 
    object$control)
  coefs <- res_screen$measurement
  coefs
}
```


Note that `screenIID()` also takes method as an argument. 
To allow for flexibility, we do not
fix the method in `generate_scr_sirs` but rather allow the user to 
pass a method through the `control` argument in the `screen_*` function.
This function is created using `constructor_screencoef`:


```{r}
screen_sirs <- constructor_screencoef(
  "screen_sirs", 
  generate_fun = generate_scr_sirs)
```



We now call the `spar()` function with the newly created screening procedure.
We consider the method SIRS of @zhu2011model, which ranks the predictors by their correlation with the rank-ordered response and we do not perform probabilistic variable
screening but employ the top $2n$ variables in each marginal model.


```{r}
set.seed(123)      
spar_example <- spar(example_data$x, example_data$y,
    screencoef = screen_sirs(type = "fixed",
      control=list(method = "SIRS")),
                   measure = "mse")
spar_example
```



### Random projections

We exemplify how new random projections can be implemented
in the framework of [spar]{.pkg}. 

We implement the random projection of @cannings2017random, 
who propose using the Haar measure for generating the random projections. 
They simulate matrices from the Haar measure by 
independently drawing each entry of a matrix $Q$ from a
standard normal distribution, and then to take the projection matrix to 
be the transpose of the matrix of left singular vectors in
the singular value decomposition of $Q$.
Moreover, they suggest using "good" random projections, in the sense that they deliver the best out-of-sample prediction. The
proposed approach employs $B_1$ models in an ensemble of classifiers and for each model $k$, $B_2$ data independent
random projections are generated and the one with the lowest 
error on a test set is the one chosen to project the variables
in model $k$.

We can implement such a random projection in [spar]{.pkg}
by the following building block: 


```{r }
update_data_cannings <- function(rp, data) {
  attr(rp, "data") <- data
  rp
}
```


This is the function which adds data information to the random projection object. Here, the whole data can be added as information for the $M$ random projection (alternatively,
one could only pass sufficient statistics for computing the 
desired measures).

While the $B_2$ random projections are data agnostic, 
the `generate_fun` element of the random projection will 
need the data information in order to evaluate which method performs best in terms of an error measure.
We will, in the following, define the function for the generation of the 
random projection matrix to be used in a  model $k$.

This helper simulates $m\times p$ matrices from the Haar measure:


```{r}
simulate_haar <- function(m, p) {
    R0 <- matrix(1/sqrt(p) * rnorm(p * m), nrow = p, ncol = m)
    RM <- qr.Q(qr(R0))[, seq_len(m)]
    RM <- Matrix(t(RM), sparse = TRUE)  
}
```



The function that generates the random projection matrix for 
model $k$ uses 25% of the data as a test set for choosing the best among 
$B_2$ random projections in terms of minimizing misclassification error for 
the binomial family and MSE for all other families:


```{r }
generate_cannings <- function(rp, m, included_vector) {
    p <- length(included_vector)
    if (is.null(rp$control$B2)) rp$control$B2 <- 50
    x <- attr(rp, "data")$x[, included_vector]
    y <- attr(rp, "data")$y
  
    B2 <- rp$control$B2
    n <- nrow(x)
    id_test <- sample(n, size = n %/% 4)
    xtrain <- x[-id_test, ];  xtest <- x[id_test,]
    ytrain <- y[-id_test];  ytest <- y[id_test]
  
    if (is.null(rp$control$family)) rp$control$family <- attr(rp, "family")
  
    family <- rp$control$family
    control_glm <-
      rp$control[names(rp$control) %in% names(formals(glm.fit))]

    error_all <- lapply(seq_len(B2), FUN = function(s){
      RM <- simulate_haar(m, p)
      xrp <- tcrossprod(xtrain, RM)
      mod <- do.call(function(...) 
       glm.fit(x =  cbind(1, xrp), y = ytrain, ...), control_glm)
      eta_test <- drop(cbind(1, tcrossprod(xtest, RM)) %*% mod$coefficients)
      pred <- family$linkinv(eta_test)
      out <-  ifelse(family$family == "binomial",
                     mean(((pred > 0.5) + 0) != ytest), 
                     mean((pred - ytest)^2))
      list(RM, out)
   })
   id_best <- which.min(sapply(error_all, "[[", 2))
   RM <- error_all[[id_best]][[1]]
   return(RM)
}
```


In the cross-validation procedure, we do not generate new matrices for each step to keep computational costs low, so we do not specify a function `update_rpm_w_data`.

Putting it all together, we get:


```{r }
rp_cannings <- constructor_randomprojection(
  "rp_cannings",
  generate_fun = generate_cannings,
  update_data_fun = update_data_cannings
)
```



We can now estimate SPAR for a binomial model, where we transform the response to a binary variable. 


```{r }
ystar <- (example_data$y > 0) + 0
ystarval <- (example_data$ytest > 0) + 0
```



We use $100$ models (which is in line to recommendations for $B_1$ in @cannings2017random), and no screening procedure. 
If no screening is desired, `nscreen` can be set to `p`
in the `screen_*` function:


```{r echo=FALSE}
file <- "cannings_example.rda"
if (file.exists(file)) {
  load(file)
} else {
  set.seed(123)   
  spar_example_1 <- spar(
    x = example_data$x, y = ystar,
    xval = example_data$xtest, yval = ystarval,
    family = binomial(),
    nummods = 100, 
    screencoef = screen_marglik(nscreen = ncol(example_data$x)),
    rp = rp_cannings(control = list(B2 = 50)),
    measure = "class"
  )
  
  spar_example_2 <- spar(
    x = example_data$x, y = ystar,
    family = binomial(),
    screencoef = screen_marglik(nscreen = ncol(example_data$x)),
    rp = rp_cw(data = TRUE),
    nummods = 100, 
    xval = example_data$xtest, yval = ystarval,
        measure = "class"
  )
  save(spar_example_1, spar_example_2, 
       file = file)
}

```

```{r eval=FALSE}
set.seed(123)   
spar_example_1 <- spar(
  x = example_data$x, y=ystar,
  xval = example_data$xtest, yval = ystarval,
  family = binomial(),
  nummods = 100, 
  screencoef = screen_marglik(nscreen = ncol(example_data$x)),
  rp = rp_cannings(control = list(B2 = 50)),
  measure = "class"
)
```


Using the data-driven `rp_cw()`:


```{r eval=FALSE}
spar_example_2 <- spar(x = example_data$x, y = ystar,
  family = binomial(),
  screencoef = screen_marglik(nscreen = ncol(example_data$x)),
  rp = rp_cw(data = TRUE),
  nummods = 100, 
  xval = example_data$xtest, yval = ystarval,
  measure = "class"
)
```


We can extract the measures on the validation set by:


```{r}
head(spar_example_1$val_res)
```


We can compare the two approaches by looking at the minimum `Meas` 
achieved:



```{r}
min_val_1 <- min(spar_example_1$val_res$Meas)
id_best_1 <- max(which(spar_example_1$val_res$Meas == min_val_1))
min_val_2 <- min(spar_example_2$val_res$Meas)
id_best_2 <- max(which(spar_example_2$val_res$Meas == min_val_2))
spar_example_1$val_res[id_best_1, ]
spar_example_2$val_res[id_best_2, ]
```


            
## Illustrations {#sec-illustrations}



### Face image data

We illustrate the functionality of [spar]{.pkg} on the Isomap data set 
containing $n = 698$
black and white face images of size $p = 64 \times 64 = 4096$ together
with the 
faces' horizontal looking direction angle as the response 
variable.\footnote{
The Isomap face data can be found online on https://web.archive.org/web/20160913051505/http://isomap.
stanford.edu/datasets.html.}


```{r eval=FALSE}
url1 <- "https://web.archive.org/web/20150922051706/"
url2 <- "http://isomap.stanford.edu/face_data.mat.Z"
download.file(paste0(url1, url2),
              file.path("face_data.mat.Z"))
system('uncompress face_data.mat.Z')
```

```{r ,echo=FALSE}
if (!file.exists("face_data.mat")) {
  url <- "https://web.archive.org/web/20150922051706/http://isomap.stanford.edu/face_data.mat.Z"
  library("R.matlab")
  download.file(url, file.path("face_data.mat.Z"))
  system(sprintf('uncompress %s', paste0("face_data.mat.Z")))
}
```



The `.mat` file format can be read using [R.matlab]{.pkg} [@pkg:rmatlab]


```{r message=FALSE}
library("R.matlab")
facedata <- readMat(file.path("face_data.mat"))
x <- t(facedata$images)
y <- facedata$poses[1,]
```


We can visualize e.g., the first observation in this data set in
Figure \ref{fig:facesplot_i1}.


```{r, echo=FALSE,fig.width=8, fig.height=8,out.width="50%", fig.pos="center",fig.fullwidth=FALSE, fig.cap="Image corresponding to the first observation in the \\emph{Isomap faces} data set. \\label{fig:facesplot_i1}"}
library(ggplot2)
i <- 415
ggplot(data.frame(X = rep(1:64,each=64),Y = rep(64:1,64),
                  Z = facedata$images[,i]),
       aes(X, Y, fill = Z)) +
  geom_tile() +
  theme_void() +
  ggtitle(paste0("y = ", round(facedata$poses[1, i],1))) +
  theme(legend.position = "none",
        plot.title = element_text(hjust = 0.5))
```


This data set has the issue of many columns being almost constant, which can make estimation unstable. Given that `spar()`
and `spar.cv()` ignore constant columns, we can alleviate this issue by setting all columns which have a low standard deviation to zero.


```{r}
x[, apply(x, 2, sd) < 1e-2] <- 0
```



We split the data into training vs test sample


```{r }
set.seed(123)
ntot <- length(y); ntest <- ntot * 0.25
testind <- sample(ntot, ntest, replace = FALSE)
xtrain <- as.matrix(x[-testind, ]); ytrain <- y[-testind]
xtest <- as.matrix(x[testind, ]); ytest <- y[testind]
```


and estimate on the training data the SPAR algorithm with cross-validation. We employ the data-driven random projection with screening based on the ridge coefficients:


```{r}
library("spar")
p <- ncol(xtrain)
spar_faces <- suppressWarnings(spar.cv(
  xtrain, ytrain,
  rp = rp_cw(data = TRUE, 
             control = list(lambda.min.ratio = 0.007)),
  nummods = c(5, 10, 20, 50),
  measure = "mse"))
spar_faces
```


The `plot` method for [spar.cv]{.class} objects displays by default the 
measure employed in the cross-validation (in this case MSE) 
for a grid of $\nu$
values, where the number of models is fixed to the value found to perform 
best in cross-validation exercise (see Figure \ref{fig:facesplot_valmeasure}). We observe that no thresholding delivers the lowest MSE. 


```{r , fig.width=10, fig.height=6,out.width="80%", fig.cap="Plot of mean squared error over a grid of threshold values $\\nu$ for fixed number of optimal models $M=50$ for the \\emph{Isomap faces} data set. Confidence bands represent one standard deviation in the measures across the number of folds. \\label{fig:facesplot_valmeasure}"}
plot(spar_faces)
```


The coefficients of the different variables (in this example pixels) obtained by averaging over the coefficients the marginal models (for optimal $\lambda$ and number of models) are given by:


```{r}
face_coef <- coef(spar_faces, opt_par = "best")
str(face_coef)
```


For a sparser solution we can compute the coefficients using 
`opt_par = "1se"` which leads to more sparsity and a lower number of models.


```{r}
face_coef_1se <- coef(spar_faces, opt_par = "1se")
str(face_coef_1se)
```


The standardized coefficients from each of `max(nummods)` models  (before averaging and before thresholding) can be plotted by setting `plot_type = "coefs"` (see Figure \ref{fig:faces_coefs}). We observe that pixels close to each other have more correlation than pixels further apart.


```{r , fig.width=15, fig.height=8,out.width="80%", fig.pos="center", fig.cap="Coefficient plot for all variables and all $M=50$ models in the SPAR ensemble for the \\emph{Isomap faces} data set. The coefficients are standardized, before thresholding. \\label{fig:faces_coefs}"}
plot(spar_faces, plot_type = "coefs")
```


The `predict()` function can be applied to the [spar.cv]{.class} object. We will employ the sparser solution chosen by the `opt_par = "1se"` rule:


```{r }
ynew <- predict(spar_faces, xnew = xtest, coef = face_coef_1se)
```


In the high-dimensional setting it is interesting to look at the relative mean square prediction error which compares the MSE to the MSE of a model containing only an intercept:


```{r }
rMSPEconst <- mean((ytest - mean(y))^2) 
mean((ynew-ytest)^2)/rMSPEconst
```


Additionally, for this data set, one can visualize the effect of each pixel 
$\hat\beta_j x^\text{new}_{i,j}$ in predicting the face orientation in a given image e.g., the 11th one in the test set. The contribution of each pixel can be visualized in Figure \ref{fig:faces_predictions}.


```{r echo=FALSE, fig.width=8, fig.height=8,out.width="50%", fig.pos="center", fig.cap="The effect of each pixel $\\hat\\beta_j x^\\text{new}_{i,j}$ in predicting the face orientation in Figure \ref{fig:facesplot_i1}. \\label{fig:faces_predictions}"}
i <- 1
plot4 <- ggplot(data.frame(X = rep(1:64, each = 64),
                           Y = rep(64:1, 64),
                           effect = xtest[i,] * face_coef_1se$beta), 
                aes(X, Y, fill = effect)) +
  geom_tile() +
  theme_void() +
  scale_fill_gradient2() +
  ggtitle(bquote(hat(y) == .(round(ynew[i])))) +
  theme(plot.title = element_text(hjust = 0.5)) 
plot4
```



### Darwin data set

The Darwin dataset [@CILIA2022darwin] contains a binary response for Alzheimer's disease (AD) together with extracted features from 25 handwriting tests (18 features per task) for 89 AD patients and 85 healthy people ($n=174$).\footnote{The data set can be downloaded from  https://archive.ics.uci.edu/dataset/732/darwin}


```{r eval=FALSE}
download.file("https://archive.ics.uci.edu/static/public/732/darwin.zip",
              "darwin.zip")
```

```{r ,echo=FALSE}
if (!file.exists("data.csv")) {
  download.file("https://archive.ics.uci.edu/static/public/732/darwin.zip",
                "darwin.zip")
}
```

```{r}
darwin_tmp <- read.csv(unzip("darwin.zip",  "data.csv"), 
                       stringsAsFactors = TRUE)
```


Before proceeding with the analysis, the data is screened for 
multivariate outliers using the DDC algorithm in package [cellWise]{.pkg}
[@rcellwise].


```{r }
darwin_orig <- list(
  x = darwin_tmp[, !(colnames(darwin_tmp) %in% c("ID", "class"))],
  y = as.numeric(darwin_tmp$class) - 1)
tmp <- cellWise::DDC(
  as.matrix(darwin_orig$x),
  list(returnBigXimp = TRUE, 
       tolProb = 0.999,
       silent = TRUE))
darwin <- list(x = tmp$Ximp, y = darwin_orig$y)
```


The structure of the data is:


```{r}
str(darwin)
```



We estimate the SPAR algorithm with the screening and random projection
introduced in @parzer2024glms for binomial family and logit link, 
using $1-$area under the ROC curve as the cross-validation measure,


```{r }
spar_darwin <- spar.cv(darwin$x, darwin$y,
                       family = binomial(logit),
                       nummods = c(5, 10, 20, 50),
                       measure = "1-auc")
```



We can look at the average number of active variables for
a grid of $\nu$  where the number of models is fixed to the value found to perform  best in cross-validation exercise by using the 
`plot` method for [spar.cv]{.class} (see Figure \ref{fig:darwin_activevars}). We observe again that no thresholding achieves the best measure and translates 
to almost all variables being active (some variables can be inactive at $\nu=0$ as they may never be screened).
The 1-standard-error rule would however indicate that more sparsity can be introduced without too much increase in the cross-validation measure.


```{r , fig.width=10, fig.height=6, fig.pos="center", out.width="80%", fig.cap="Average number of active variables for the grid of thresholding values $\\nu$ and $M=20$ models for the \\emph{Darwin} data set. The red points correspond to the average number of active variables for the model with the lowest cross-validation measures and to the one chosen by the 1-standard-error rule. \\label{fig:darwin_activevars}"}
plot(spar_darwin, plot_type = "Val_numAct")
```


The predictors are ordered by task, where the first 18 covariates represent different features measured for the first task. 
Given that there is clear grouping in the variables in this example, we can reorder the coefficients for plotting by grouping them by feature, rather than task.
We can achieve this by using reordering argument `coef_order` in method `plot` with `plot_type = "coefs"` (see Figure \ref{fig:darwin_coefs}).


```{r echo=FALSE, fig.width=10, fig.height=6, fig.pos="center",out.width="50%", fig.cap="Coefficient plot for all variables and all $M=50$ models in the SPAR ensemble \\emph{Darwin} data set. The coefficients are standardized, before thresholding. \\label{fig:darwin_coefs}"}
ntasks <- 25
nfeat <- 18
reorder_ind <- c(outer(
  (seq_len(ntasks) - 1) * nfeat,
  seq_len(nfeat), "+"))
feat_names <- sapply(colnames(darwin$x)[seq_len(nfeat)],
                     function(name) substr(name, 1, nchar(name) - 1))

plot(spar_darwin,"coefs",coef_order = reorder_ind) + 
  geom_vline(xintercept = 0.5 + seq_len(ntasks - 1) * ntasks, 
             alpha = 0.2, linetype = 2) +
  annotate("text",x = (seq_len(nfeat) - 1) * ntasks + 12,
           y = 45,label=feat_names, angle = 90,
           size = 3)
```


In general we observe that the different features measures across different tasks have the same impact on the probability of AD (observable by the blocks of blue or red lines).

## Conclusion {#sec-conclusion}

Package [spar]{.pkg} can be employed for modeling data in a high-dimensional setting, where the number of predictors is much higher than the number of 
observations. The package provides an implementation of the SPAR algorithm,
which combines variable screening and random projection in an ensemble of
GLMs. The package provides flexible classes for i) specifying the 
screening coefficient based on which screening should be performed (both in a classical or probabilistic fashion), ii) generating the random projection 
to be employed in each marginal model.

## Computational details {.unnumbered .unlisted}

The results in this paper were obtained using [R]{.proglang} `r paste(R.Version()[6:7], collapse = ".")`.

[R]{.proglang} itself and all packages used are available from the Comprehensive [R]{.proglang} Archive Network (CRAN) at \url{https://CRAN.R-project.org/}.

## Acknowledgments {.unnumbered .unlisted}

Roman Parzer and Laura Vana-Gür acknowledge funding from the Austrian Science Fund (FWF) for the project "High-dimensional statistical learning: New methods to advance economic and sustainability policies" (ZK 35), jointly carried out by WU Vienna University of Economics and Business, Paris Lodron University Salzburg, TU Wien, and the Austrian Institute of Economic Research (WIFO).

## References {.unnumbered .unlisted}


