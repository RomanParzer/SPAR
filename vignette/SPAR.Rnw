\documentclass[nojss]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{orcidlink,thumbpdf,lmodern}

%% another package (only for this demo article)
\usepackage{framed}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}
\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE)
library("MASS")
@


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation (and optionally ORCID link)
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Roman Parzer~\orcidlink{}\\TU Wien
   \And
   Peter Filzmoser~\orcidlink{}\\TU Wien
   \And
   Laura Vana G{\"u}r~\orcidlink{}\\TU Wien
}
\Plainauthor{Roman Parzer,  Peter Filzmoser, Laura Vana G{\"u}r}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{\pkg{SPAR}: Sparse Projected Averaged Regression in \proglang{R}}
\Plaintitle{SPAR: Sparse Projected Averaged Regression in R}
\Shorttitle{Sparse Projected Averaged Regression}

%% - \Abstract{} almost as usual
\Abstract{

}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{random projection, variable screening, ensemble learning, \proglang{R}}
\Plainkeywords{random projection, variable screening, ensemble learning,  R}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Roman Parzer\\
  Computational Statistics (CSTAT) \\
  Institute of Statistics and Mathematical Methods in Economics\\
  TU Wien\\
  1040 Vienna, Austria\\
  E-mail: \email{Roman.Parzers@tuwien.ac.at}
}

\begin{document}

%% -- Introduction -------------------------------------------------------------

%% - In principle "as usual".
%% - But should typically have some discussion of both _software_ and _methods_.
%% - Use \proglang{}, \pkg{}, and \code{} markup throughout the manuscript.
%% - If such markup is in (sub)section titles, a plain text version has to be
%%   added as well.
%% - All software mentioned should be properly \cite-d.
%% - All abbreviations should be introduced.
%% - Unless the expansions of abbreviations are proper names (like "Journal
%%   of Statistical Software" above) they should be in sentence case (like
%%   "generalized linear models" below).

\section[Introduction]{Introduction} \label{sec:intro}

\pkg{SPAR} is a package for building predictive generalized linear models
(GLMs) with high-dimensional (HD) predictors in \proglang{R}. In package
\pkg{SPAR}, probabilistic variable screening and random projection of the predictors are performed to obtain an ensemble of GLMs, which are then averaged to obtain predictions in an high-dimensional regression setting.

Random projection is a computationally-efficient method which linearly maps a set of points in high dimensions into a much
lower-dimensional space while approximately preserving pairwise distances.
For very large $p$, random projection can suffer from overfitting, as too many irrelevant predictors are being considered for prediction purposes \citep{Dunson2020TargRandProj}. Therefore, screening out irrelevant variables before performing the random projection is advisable in order to tackle this issue. The screening can be performed in a probabilistic fashion, by randomly
sampling covariates for inclusion in the model based on probabilities proportional to an importance measure (as opposed to random subspace sampling employed in e.g.,~random forests).
Finally, in practice the information from multiple such screening and projections can be combined by averaging, in order to reduce the variance introduced by the random sampling (of both projections and screening indicators) \cite{Thanei2017RPforHDR}.
%%

Several packages which provide functionality for random projections are available for \proglang{R}. Package \pkg{RandPro} \citep{RandProR, SIDDHARTH2020100629} allows for four different random projection matrices to be applied to the predictor matrix before employing one of $k$~nearest neighbor, support vector machine or naive Bayes classifier. Package \pkg{SPCAvRP} \citep{SPCAvRPR} implements sparse principal component analysis, based on the aggregation of eigenvector information from ``carefully-selected'' axis-aligned random projections of the sample covariance matrix.
Package \pkg{RPEnsembleR} \citep{RPEnsembleR} implements the same idea of ``carefully-selected'' random projections when building an ensemble of classifiers.

On the other hand, there are a multitude of packages dealing with
variable screening on the Comprehensive \proglang{R} Archive Network (CRAN).

The (iterative) sure independence screening procedure and extensions in
\cite{Fan2007SISforUHD,Fan2010sisglms,fan2010high} is implemented in package \pkg{SIS} \citep{SISR}, which also
provides functionality for estimating a penalized generalized linear model or a cox regression model for the variables picked by the screening procedure.

Package \pkg{VariableScreening} \citep{pkg:VariableScreening} implements screening for iid data,
varying-coefficient models, and longitudinal data using different screening methods: Sure Independent Ranking and Screening -- which ranks the predictors by their correlation with the rank-ordered response (SIRS),
Distance Correlation Sure Independence Screening -- a non-parametric extension of the correlation coefficient (DC-SIS), MV Sure Independence Screening -- using the mean conditional variance measure (MV-SIS).

A collection of model-free screening techniques such as SIRS, DC-SIS, MV-SIS, the fused Kolmogorov filter \citep{mai2015fusedkolmogorov}, the projection correlation method using knock-off features \citep{liu2020knockoff}, are provided in package \pkg{MFSIS} \citep{pkg:MFSIS}.
Package \pkg{tilting} \citep{pkg:tilting} implements an algorithm for variable selection in high-dimensional linear regression using tilted correlation, which takes into account high correlations among the variables in a data-driven way.
Feature screening based on conditional distance correlation \citep{wang2015conditional} can be performed with the \pkg{cdcsis} package \citep{pkg:cdcsis} while package \pkg{QCSIS} \citep{pkg:QCSIS} implements screening based on
(composite) quantile correlation.

Package \pkg{LqG} \citep{pkg:LqG} provides a group screening procedure that is based on maximum Lq-likelihood estimation, to simultaneously account for the group structure and data contamination in variable screening.

Feature screening using an $L1$ fusion penalty can be performed with package \pkg{fusionclust} \citep{pkg:fusionclust}.
Package \pkg{SMLE} \citep{pkg:SMLE} implements joint feature screening via sparse MLE \citep{SMLE2014} in high-dimensional linear, logistic, and Poisson models.
Package \pkg{TSGSIS} \citep{pkg:TSGSIS} provides a high-dimensional grouped variable selection approach for detecting interactions that may not have marginal effects in high dimensional linear and logistic regression  \citep{10.1093/bioinformatics/btx409}.

Package \pkg{RaSEn} implements the RaSE algorithm for ensemble classification and classification problems, where random subspaces are generated and the optimal one is chosen to train a weak learner on the basis of some criterion. Various choices of base classifiers are implemented, for instance, linear discriminant analysis, quadratic discriminant analysis, k-nearest neighbour, logistic or linear regression, decision trees, random forest, support vector machines. The selected percentages of variables can be employed for variable screening.
%% Special
Package \pkg{Ball} \citep{pkg:ball} provides functionality for variable screening using ball statistics, which is appropriate for shape, directional, compositional and symmetric positive definite matrix data.
%% Bayesian
Package \pkg{BayesS5} \citep{pkg:BayesS5} implements Bayesian variable selection using simplified shotgun stochastic search  algorithm with screening
\citep{shin2017scalablebayesianvariableselection}
while package \pkg{bravo} \citep{pkg:bravo} implements the Bayesian iterative screening method proposed in \cite{wang2021bayesianiterativescreeningultrahigh}




The rest of the paper is organized as follows:
Section~\ref{sec:models} provides the methodological details of the implemented algorithm. The package is described in Section~\ref{sec:software}.
Section~\ref{sec:illustrations} contains two examples of employing the package on real data sets. Finally, Section~\ref{sec:conclusion} concludes.

%% -- Manuscript ---------------------------------------------------------------

%% - In principle "as usual" again.
%% - When using equations (e.g., {equation}, {eqnarray}, {align}, etc.
%%   avoid empty lines before and after the equation (which would signal a new
%%   paragraph.
%% - When describing longer chunks of code that are _not_ meant for execution
%%   (e.g., a function synopsis or list of arguments), the environment {Code}
%%   is recommended. Alternatively, a plain {verbatim} can also be used.
%%   (For executed code see the next section.)

\section{Methods} \label{sec:models}

Random projection matrices can be generated by nearly all sub-Gaussian distributions:
see more info at \url{https://www.cs.waikato.ac.nz/~bobd/ECML_Tutorial/ECML_handouts.pdf}.

See also \url{https://web.math.princeton.edu/~amits/publications/LeanWalsh_published.pdf}.

\url{https://arxiv.org/pdf/1710.03163.pdf}

\url{https://cseweb.ucsd.edu/~dasgupta/papers/randomf.pdf}

\url{https://people.math.ethz.ch/~nicolai/mv/notes6.pdf}

\url{https://web.math.princeton.edu/~amits/publications/LeanWalsh\_published.pdf}

Implemented random projection matrices $\Phi\in \mathbb{R}^{m \times p}$:
\begin{itemize}
    \item Gaussian \citep{FRANKL1988JLSphere}: $\Phi_{ij} \overset{iid}{\sim} N(0,1)$. An orthonormalization is normally applied $(\Phi\Phi^\top)^{-1/2}\Phi$ but this can be skipped if $p$ is large.
    \item \citep{matouvsek2008variants}:
\begin{align}\label{eq:mat08rp}
  \Phi_{ij} = \begin{cases}
      {\sim} N(0,1/\sqrt{\psi}) & \text{with prob. } \psi \\
      0 & \text{with prob. } 1 - \psi
    \end{cases},
  \end{align}
  \item Uniform:
  \begin{align}
  \Phi_{ij} = \begin{cases}
      1 & \text{with prob. } 1/2 \\
      -1 & \text{with prob. } 1/2
    \end{cases},
  \end{align}
  \begin{align}
  \Phi_{ij} = \begin{cases}
      1 & \text{with prob. } 1/6 \\
      0 & \text{with prob. } 1/6 \\
      -1 & \text{with prob. } 2/3
    \end{cases},
  \end{align}
    \item Achlioptas \citep{ACHLIOPTAS2003JL} shows results for $\psi=1$ or $\psi=1/3$
\begin{align}\label{eq:sparseRMgen}
  \Phi _{ij} = \begin{cases}
      \pm 1/\sqrt{\psi} & \text{with prob. } \psi/2 \\
      0 & \text{with prob. } 1 - \psi
    \end{cases},
  \end{align}
  for $0<\psi\leq 1$ can satisfy the property after appropriate scaling.
    \item Li et all uses \label{eq:sparseRMgen} with $\psi=$
    \item sparse embedding matrix CW
    \item CWHolp
    \item Ailon-Chazelle 2006: $\Phi=PHD$ with $P$ random and sparse,
$P_{ij} \sim N (0, 1/q)$ w.p $1/q$ and 0 otherwise, $H$ the normalized Hadamard (orthogonal) matrix $H_{ij} = p^{-1/2}(-1)^{\langle i-1,j-1\rangle}$, where $\langle i, j\rangle$ is the dot-product of the $m$-bit vectors $i$, $j$ expressed in binary, $D = diag(\pm 1)$ random. Extended in
Ailon-Liberty 2009 and Ailon-Liberty 2011.
   \item Dasgupta, Kumar, Sarlos 2011: locality-sensitive hashing for nearest neighbor classification.
\end{itemize}
For the RH examples, taking $\psi$
too small gives high distortion of sparse
vectors \citep{matouvsek2008variants}. Ailon-Chazelle 2006 get around this by using a randomized
orthogonal (normalized Hadamard) matrix to ensure w.h.p all data
vectors are dense.

\subsection{Algorithm}
\begin{enumerate}
 %  \item choose family with corresponding log-likelihood $\ell(.)$ and link
    \vspace{-0.1cm} \item standardize predictors $X:n\times p$
    \vspace{-0.1cm} \item calculate \textbf{\color{bordercolor}screening coefficients} $\hat\alpha$ e.g.,
   \begin{itemize}
      \vspace{-0.2cm}\item ridge: $\hat\alpha=: \text{argmin}_{{\beta}\in\mathbb{R}^p}\sum_{i=1}^n -\ell(\beta;y_i,x_i) + \frac{\varepsilon}{2}\sum_{j=1}^p{\beta}_j^2, \, \varepsilon > 0$
     \vspace{-0.2cm}\item marginal likelihood: $\hat\alpha_j=: \text{min}_{{\beta_j}\in\mathbb{R}}\sum_{i=1}^n -\ell(\beta;y_i,x_{ij})$
   \end{itemize}
      %  \item calculate $\hat\beta_{L_2}:= \text{argmin}_{{\beta}\in\mathbb{R}^p}\sum_{i=1}^n -\ell(\beta;y_i) + \frac{\varepsilon}{2}\sum_{j=1}^p{\beta}_j^2, \, \varepsilon > 0$
    \vspace{-0.1cm} \item For $k=1,\dots,M$ models:
    \begin{enumerate}
    \vspace{-0.1cm}\item[3.1.] draw $2n$ predictors with probabilities $p_j\propto |\hat\alpha_j|$ yielding screening index set $I_k=\{j_1^k,\dots,j_{2n}^k\}\subset[p]$
     %  \vspace{-0.2cm}\item[3.1.] draw $2n$ predictors with probabilities $p_j\propto |\hat\beta_{L_2,j}|$ yielding screening index set $I_k=\{j_1^k,\dots,j_{2n}^k\}\subset[p]$
    \vspace{-0.1cm}\item[3.2.] project remaining variables to dim. $m_k\sim \text{Unif}\{\log(p),\dots,n/2\}$ using  \textbf{\color{bordercolor}projection matrix} $\Phi_k$ to   obtain $Z_k=X_{\cdot I_k}\Phi_k^\top \in \mathbb{R}^{n\times m_k}$:
    \begin{itemize}
        \item   $(\Phi_k)_{ij} \sim N(0,1)$
        \item $(\Phi_k)_{ij} = \pm 1/\sqrt{\psi}$ with prob. $\psi/2$ and zero with prob. $1 - \psi$,
        \item  ${\Phi_k=BD}$, $B:m_k\times 2n$ binary matrix and $D$ diagonal with $(D_{ii}+1)/2\sim \text{Unif}\{0,1\}$ \cite{Clarkson2013LowRankApproxShort} or  ${D_{ii}=\hat \alpha_{j_i^k}}$ [see Paper]
    \end{itemize}

       \vspace{-0.1cm}\item[3.3.] fit a \textbf{GLM} of $y$ against $Z_k$ (with small $L_2$-penalty \cite{glmnet2023}) to obtain estimated coefficients $\gamma^k\in\mathbb{R}^{m_k}$ and $\hat \beta_{I_k}^k=\Phi_k'\gamma^k$, $\hat \beta_{\bar I_k}^k=0$.
    \end{enumerate}
   \vspace{-0.1cm}  \item for a given threshold $\lambda>0$, set all $\hat\beta_j^k$ with $|\hat\beta_j^k|<\lambda$ to $0$ for all $j,k$
    \vspace{-0.1cm} \item \textit{Optional:} choose $M$ and $\lambda$ via cross-validation by repeating steps 1 to 4
    for each fold and
    evaluating a prediction performance measure on the withheld fold;
    % and choose
    % \begin{align}
    %   (M_{\text{best}},\lambda_{\text{best}}) = \text{argmin}_{M,\lambda}\text{Dev}(M,\lambda)
    % \end{align}
    \vspace{-0.1cm} \item combine via \textbf{\color{bordercolor}simple average} $\hat \beta = \sum_{k=1}^M\hat \beta^k / M$

  %  \item output the estimated coefficients and predictions for the chosen $M$ and $\lambda$
  \end{enumerate}

\section{Software}\label{sec:software}
The two main functions are:

\begin{Code}
spar(x, y, family = gaussian(), ...)
\end{Code}
and
\begin{Code}
spar.cv(x, y, family = gaussian(), nfolds = 10L, ...)\end{Code}
Most important arguments:
\begin{boenumerate}
 \item \code{x} $n \times p$ numeric matrix of predictor variables.
\item \code{y} numeric response vector of length $n$.
\item \code{family}	object from \code{stats::family()}.

 \item \code{type.rpm}
 type of random projection matrix to be employed; one of \code{"cwdatadriven"} [see Paper], \code{"cw"} \cite{Clarkson2013LowRankApproxShort}, \code{"gaussian"}, \code{"sparse"}.
  %    \item \code{nscreen}	max number of variables kept after screening in each marginal model; defaults to $2n$.

    \item \code{type.screening} measure by which the  coefficients are screened; \code{"ridge"} performs screening based on ridge regression, \code{"marglik"} marginal likelihood of fitting a GLM for each predictor, \code{"corr"} correlation of the response with each predictor.

   \item \code{type.measure}
 loss to use for choosing $\lambda$ and $M$; defaults to \code{"deviance"} (available for all families). Other options are \code{"mse"} or \code{"mae"} (for all families), \code{"class"} and \code{"1-auc"} for \code{"binomial"}.

 % \item \code{nlambda} number of $\lambda$ values to be considered for thresholding and optionally  \code{lambdas}, a vector of values.

  \item \code{nummods} vector containing the size of the different ensembles $M$ to consider for the prediction.

\end{boenumerate}
Methods \code{print},  \code{plot},  \code{coef},  \code{predict} are available.

\begin{table}[t!]
\centering
\begin{tabular}{lllp{7.4cm}}
\hline
Type           &  &    & Description \\ \hline
\end{tabular}
\caption{\label{tab:overview} Overview of implemented random projections and screening techniques in \pkg{SPAR}}
\end{table}


%% -- Illustrations ------------------------------------------------------------

%% - Virtually all JSS manuscripts list source code along with the generated
%%   output. The style files provide dedicated environments for this.
%% - In R, the environments {Sinput} and {Soutput} - as produced by Sweave() or
%%   or knitr using the render_sweave() hook - are used (without the need to
%%   load Sweave.sty).
%% - Equivalently, {CodeInput} and {CodeOutput} can be used.
%% - The code input should use "the usual" command prompt in the respective
%%   software system.
%% - For R code, the prompt "R> " should be used with "+  " as the
%%   continuation prompt.
%% - Comments within the code chunks should be avoided - these should be made
%%   within the regular LaTeX text.

\section{Illustrations} \label{sec:illustrations}

Show example with Darwin and Image Data?

%% -- Summary/conclusions/discussion -------------------------------------------
\section{Conclusion}\label{sec:conclusion}

%% -- Optional special unnumbered sections -------------------------------------

\section*{Computational details}

\begin{leftbar}
If necessary or useful, information about certain computational details
such as version numbers, operating systems, or compilers could be included
in an unnumbered section. Also, auxiliary packages (say, for visualizations,
maps, tables, \dots) that are not cited in the main text can be credited here.
\end{leftbar}

The results in this paper were obtained using
\proglang{R}~\Sexpr{paste(R.Version()[6:7], collapse = ".")} with the
\pkg{MASS}~\Sexpr{packageVersion("MASS")} package.

\proglang{R} itself
and all packages used are available from the Comprehensive
\proglang{R} Archive Network (CRAN) at
\url{https://CRAN.R-project.org/}.


\section*{Acknowledgments}

\begin{leftbar}
All acknowledgments (note the AE spelling) should be collected in this
unnumbered section before the references. It may contain the usual information
about funding and feedback from colleagues/reviewers/etc. Furthermore,
information such as relative contributions of the authors may be added here
(if any).
\end{leftbar}


%% -- Bibliography -------------------------------------------------------------
%% - References need to be provided in a .bib BibTeX database.
%% - All references should be made with \cite, \citet, \citep, \citealp etc.
%%   (and never hard-coded). See the FAQ for details.
%% - JSS-specific markup (\proglang, \pkg, \code) should be used in the .bib.
%% - Titles in the .bib should be in title case.
%% - DOIs should be included where available.

\bibliography{SPAR}


%% -- Appendix (if any) --------------------------------------------------------
%% - After the bibliography with page break.
%% - With proper section titles and _not_ just "Appendix".

\newpage

\begin{appendix}

\section{More technical details} \label{app:technical}

\begin{leftbar}
Appendices can be included after the bibliography (with a page break). Each
section within the appendix should have a proper section title (rather than
just \emph{Appendix}).

For more technical style details, please check out JSS's style FAQ at
\url{https://www.jstatsoft.org/pages/view/style#frequently-asked-questions}
which includes the following topics:
\begin{itemize}
  \item Title vs.\ sentence case.
  \item Graphics formatting.
  \item Naming conventions.
  \item Turning JSS manuscripts into \proglang{R} package vignettes.
  \item Trouble shooting.
  \item Many other potentially helpful details\dots
\end{itemize}
\end{leftbar}


\section[Using BibTeX]{Using \textsc{Bib}{\TeX}} \label{app:bibtex}

\begin{leftbar}
References need to be provided in a \textsc{Bib}{\TeX} file (\code{.bib}). All
references should be made with \verb|\cite|, \verb|\citet|, \verb|\citep|,
\verb|\citealp| etc.\ (and never hard-coded). This commands yield different
formats of author-year citations and allow to include additional details (e.g.,
pages, chapters, \dots) in brackets. In case you are not familiar with these
commands see the JSS style FAQ for details.

Cleaning up \textsc{Bib}{\TeX} files is a somewhat tedious task -- especially
when acquiring the entries automatically from mixed online sources. However,
it is important that informations are complete and presented in a consistent
style to avoid confusions. JSS requires the following format.
\begin{itemize}
  \item JSS-specific markup (\verb|\proglang|, \verb|\pkg|, \verb|\code|) should
    be used in the references.
  \item Titles should be in title case.
  \item Journal titles should not be abbreviated and in title case.
  \item DOIs should be included where available.
  \item Software should be properly cited as well. For \proglang{R} packages
    \code{citation("pkgname")} typically provides a good starting point.
\end{itemize}
\end{leftbar}

\end{appendix}

%% -----------------------------------------------------------------------------


\end{document}
