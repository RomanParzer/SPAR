% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  article]{jss}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{orcidlink,thumbpdf,lmodern}

\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}{}
\makeatother
\makeatletter
\makeatother
\makeatletter
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[enhanced, sharp corners, frame hidden, interior hidden, borderline west={3pt}{0pt}{shadecolor}, breakable, boxrule=0pt]}{\end{tcolorbox}}\fi
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={: Sparse Projected Averaged Regression in },
  pdfauthor={Roman Parzer; Laura Vana G端r},
  pdfkeywords={random projection, variable screening, ensemble
learning, R},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


%% -- Article metainformation (author, title, ...) -----------------------------

%% Author information
\author{Roman Parzer~\orcidlink{0000-0003-0893-3190}\\TU Wien \And Laura
Vana G端r~\orcidlink{0000-0002-9613-7604}\\TU Wien}
\Plainauthor{Roman Parzer, Laura Vana G端r} %% comma-separated

\title{\pkg{SPAR}: Sparse Projected Averaged Regression in \proglang{R}}
\Plaintitle{: Sparse Projected Averaged Regression
in} %% without formatting

%% an abstract and keywords
\Abstract{\pkg{SPAR} is a package for building ensembles of predictive
generalized linear models (GLMs) with high-dimensional (HD) predictors
in \proglang{R} by making use of probabilistic variable screening and
random projection tools. The package design is focused on extensibility,
where the screening and random projections are implemented as classes
with convenient constructor functions, allowing users to easily
implement new procedures.}

%% at least one keyword must be supplied
\Keywords{random projection, variable screening, ensemble
learning, \proglang{R}}
\Plainkeywords{random projection, variable screening, ensemble
learning, R}

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}
%% \setcounter{page}{1}
%% \Pages{1--xx}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
Roman Parzer\\
Computational Statistics (CSTAT) ~Institute of Statistics and
Mathematical Methods in Economics\\
Karlsplatz 4\\
Vienna Austria\\
E-mail: \email{romanparzer1@gmail.com}\\
\\~
Laura Vana G端r\\
\\~

}

\begin{document}
\maketitle


\section{Introduction}\label{sec-intro}

\pkg{SPAR} is a new package designed for building predictive generalized
linear models (GLMs) with high-dimensional (HD) predictors in
\proglang{R} integrating probabilistic variable screening and random
projection techniques of large predictors sets. The algorithm provides a
computationally efficient procedure for high-dimensional regression for
both discrete and continuous responses while the package design offers a
flexible framework, allowing users to extend the implemented screening
and projection techniques with their own procedures in a convenient way
by using \proglang{R}'s \proglang{S}3 classes. The adaptability of the
algorithm and the extensibility of the package to combine different
techniques enhances {[}SPAR\{.pkg\}'s suitability across a range of
high-dimensional applications.

The algorithm builds an ensemble of GLMs in the spirit of
\citet{Dunson2020TargRandProj} and \citet{parzer2024glms} where in each
model of the ensemble variable screening and random projections are used
for dimensionality reduction of the predictors. Random projection is a
computationally-efficient method which linearly maps a set of points in
high dimensions into a much lower-dimensional space, but for very large
\(p\), it can suffer from noise accumulation, as too many irrelevant
predictors are being considered for prediction purposes
\citep{Dunson2020TargRandProj}. Therefore, screening out irrelevant
variables before performing the random projection is advisable in order
to tackle this issue. The screening can be performed in a probabilistic
fashion, by randomly sampling covariates for inclusion in the model
based on probabilities proportional to an importance measure (as opposed
to random subspace sampling employed in e.g., random forests). Finally,
in practice, the information from multiple such screening and
projections can be combined by averaging, in order to reduce the
variance introduced by the random sampling (of both projections and
screening indicators) \citep{Thanei2017RPforHDR}.

While the package provides a first implementation of the described
algorithm, there are several packages which provide functionality for
random projections availble \proglang{R}. Package \pkg{RandPro}
\citep{RandProR, SIDDHARTH2020100629} allows for four different random
projection matrices to be applied to the predictor matrix before
employing one of \(k\)\textasciitilde nearest neighbor, support vector
machine or naive Bayes classifier. Package \pkg{SPCAvRP}
\citep{SPCAvRPR} implements sparse principal component analysis, based
on the aggregation of eigenvector information from
``carefully-selected'' axis-aligned random projections of the sample
covariance matrix. Package \pkg{RPEnsembleR} \citep{RPEnsembleR}
implements the same idea of ``carefully-selected'' random projections
when building an ensemble of classifiers. For \proglang{Python}
\citet{Python} the \pkg{sklearn.random\_projection} module implements
two types of unstructured random matrix, namely Gaussian random matrix
and sparse random matrix.

Related to screening, the users could seamlessly integrate different
techniques for variable screening by writing they own procedure or
leveraging the extensive landscape of packages implementing screening
procedures in \proglang{R}. An incomplete overview of the packages
available on the Comprehensive \proglang{R} Archive Network (CRAN)
includes: package \pkg{SIS} \citep{SISR}, which implements the
(iterative) sure independence screening procedure and extensions in
\citet{Fan2007SISforUHD}, \citet{Fan2010sisglms}, \citet{fan2010high},
which also provides functionality for estimating a penalized generalized
linear model or a cox regression model for the variables picked by the
screening procedure; package \pkg{VariableScreening}
\citep{pkg:VariableScreening} implements screening for iid data,
varying-coefficient models, and longitudinal data using different
screening methods such as sure independent ranking and screening --
which ranks the predictors by their correlation with the rank-ordered
response, or distance correlation sure independence screening -- a
non-parametric extension of the correlation coefficient. A collection of
model-free screening techniques such as SIRS, DC-SIS, MV-SIS, the fused
Kolmogorov filter \citep{mai2015fusedkolmogorov}, the projection
correlation method using knock-off features \citep{liu2020knockoff}, are
provided in package \pkg{MFSIS} \citep{pkg:MFSIS}. Further packages are
available which implement specific procedures:
\citep[\citet{pkg:fusionclust}]{pkg:tilting, pkg:cdcsis, pkg:QCSIS, pkg:LqG}.

Packages which are similar in spirit to \pkg{SPAR}, which also build
ensembles, but only for classification purposes, include package
\pkg{RaSEn} \citep{pkg:RaSEn}, which implements the RaSE algorithm for
ensemble classification and classification problems, where random
subspaces are generated and the optimal one is chosen to train a weak
learner on the basis of some criterion, and package \pkg{RPEnsemble}
\citep{pkg:RPEnsembleR}, which implements the procedure in
\citet{cannings2017random}. \pkg{RaSEn} provides various choices of base
classifiers but do not perform random projections while \pkg{RPEnsemble}
implements only the random projection approach in the package and
provides limited functionality for using different classifiers.
Therefore, no other package in \proglang{R} provides the functionality
of \pkg{SPAR} for GLMs.

The package provides methods such \texttt{plot}, \texttt{predict},
\texttt{coef}, \texttt{print}, which allow user to more easily interact
with the model output and analyze the results. The GLM framework,
especially when combined with random projections which preserve
information on the original coefficients \citep[such as the one
in][]{parzer2024glms}, facilitates interpretability of the model out,
allowing users to understanding variable effects.

The rest of the paper is organized as follows: Section~\ref{sec-models}
provides the methodological details of the implemented algorithm. The
package is described in Section~\ref{sec-software}.
Section~\ref{sec-illustrations} contains two examples of employing the
package on real data sets. Finally, Section~\ref{sec-conclusion}
concludes.

\section{Methods}\label{sec-models}

The package implements a procedure for building an ensemble of GLMs
where we employ screening and random projection to the predictor matrix
pre-model estimation for the purpose of dimensionality reduction.

Throughout the section we assume to observe high-dimensional data
\(\{(\boldsymbol{x}_i,y_i)\}_{i=1}^n\), where
\(\boldsymbol{x}_i\in\mathbb{R}^p\) is a predictor vector and
\(y_i\in\mathbb{R}\) is the response, with \(p\gg n\).

\subsection{Variable screening}\label{variable-screening}

In this section we provide an overview of possible approaches to
performing variable screening in a high-dimensional setting. In
high-dimensional modeling, the goal of variable screening is to reduce
the predictor set by selecting a small subset of variables with a strong
\emph{utility} to the response variable. This initial selection enables
more efficient downstream analyses by discarding less relevant
predictors early in the modeling process, thus reducing computational
costs and potential noise accumulation stemming from irrelevant
variables \citep[see e.g.,][]{Dunson2020TargRandProj}.

Classic approaches sure independence screening (SIS), proposed by
\citet{Fan2007SISforUHD}, use the vector of marginal empirical
correlations
\(\hat\alpha=(\alpha_1,\ldots ,\alpha_p)'\in\mathbb{R}^p,\alpha_j=\text{Cor}(X_{.j},y)\)
to screen predictors in a linear regression setting by selecting the
variable set \(\mathcal{A}_\gamma = \{j\in [p]:|w_j|>\gamma\}\)
depending on a threshold \(\gamma>0\), where \([p]=\{1,\dots,p\}\).
Under certain technical conditions, that this screening coefficient has
the \emph{sure screening property}
\(\mathbb{P}(\mathcal{A} \subset \mathcal{A}_{\gamma_n})\to 1 \text{ for } n\to \infty\).
Extensions to SIS include modifications for GLMs \citep{Fan2010sisglms},
where screening is performed based on the log-likelihood or the slope
coefficient of the GLM containing only \(X_j\) as a predictor:
\(\hat\alpha_j=: \text{argmin}_{\beta\in\mathbb{R}}\text{min}_{{\beta_0}\in\mathbb{R}}\sum_{i=1}^n -\ell(\beta_j,\beta_0;y_i,x_{ij})\).
However, both approaches face limitations related to the required
technical conditions which can rules out practically possible scenarios
where an important variable is marginally uncorrelated to the response
due to their multicollinearity. To tackle these issues,
\citet{fan2009ultrahigh} propose to use an iterative procedure where SIS
is applied subsequently on the residuals of the model estimated in a
previous step.

To account for multicollinearity among the predictors, which can cause
relevant predictors to be marginally uncorrelated with the response,
various extensions have been proposed. In a linear regression setting,
\citet{Wang2015HOLP} propose employing the ridge estimator when the
penalty term converges to zero while \citet{cho2012high} propose using
the tilted correlation, i.e., the correlation of a tilted version of
\(X_j\) with \(y\). For discrete outcomes, joint feature screening
\citet{SMLE2014} has been proposed.

In order to tackle potential model misspecification, a rich stream of
literature focuses on developing semi- or non-parametric alternatives to
SIS. For linear regression, approaches include using the ranked
correlation \citep{zhu2011model}, (conditional) distance correlation
\citep{li2012feature, wang2015conditional}. or quantile correlation
\citep{ma2016robust}. For GLMs, \citet{fan2011nonparametric} extend
\citet{Fan2010sisglms} by fitting a generalized additive model with
B-splines. Further extensions for discrete (or categorical) outcomes
include the fused Kolmogorov filter \citep{mai2013kolmogorov}, using the
mean conditional variance, i.e., the expectation in \(X_j\) of the
variance in the response of the conditional cumulative distribution
function \(\Prob(X\leq x|Y)\) \citep{cui2015model}.
\citet{ke2023sufficient} propose a model free method where the
contribution of each individual predictor is quantified marginally and
conditionally in the presence of the control variables as well as the
other candidates by reproducing-kernel-based \(R^2\) and partial \(R^2\)
statistics.

\pkg{SPAR} allows the integration of various advanced screening
techniques using a flexible framework, which in turn enables users to
apply various screening methods tailored to their data characteristics
in the algorithm generating the ensemble. This flexibility allows users
to evaluate different strategies, ensuring that the most effective
approach is chosen for the specific application at hand. Moreover, it
incorporates probabilistic screening strategies, which can be
particularly useful in ensembles, as they to enhance the diversity of
predictors across ensemble models. Instead of relying on a fixed
threshold or number of predictors to be screened, predictors are sampled
with probabilities proportional to their screening score
\citep[see][]{Dunson2020TargRandProj, parzer2024glms}.

\subsection{Random projection tools}\label{random-projection-tools}

The random projection method relies on the Johnson-Lindenstrauss (JL)
lemma \citep{JohnsonLindenstrauss1984}, which asserts that there exists
a random map \(\Phi\in \mathbb{R}^{m \times p}\) that embeds any set of
points in \(p\)-dimensional Euclidean space collected in the rows of
\(X\in \mathbb{R}^{n\times p}\) into a \(m\)-dimensional Euclidean space
where pairwise distances are approximately preserved and it also gives a
lower bound on the goal dimension \(m\) in order to preserve the
distances between all pairs of points within a factor
\(1\pm \varepsilon\):
\(m>\frac{24\log n}{3\varepsilon^2-2\varepsilon^3}\) for any
\(0 <\varepsilon< 1\). Computationally, an attractive feature of the
method for high-dimensional settings is that the bound does not depend
on \(p\).

The random map \(\Phi\) that satisfies the JL lemma should be chosen
such that it fulfills certain conditions
\citep[see][]{JohnsonLindenstrauss1984} and the literature focuses on
constructing such matrices either by sampling them from some
``appropriate'' distribution, by inducing sparsity in the matrix and/or
by employing specific fast constructs which lead to efficient
matrix-vector multiplications.

It turns out that the conditions are generally satisfied by nearly all
sub-Gaussian distributions \citep{matouvsek2008variants}. A common
choice is the standard normal distribution
\(\Phi_{ij} \overset{iid}{\sim} N(0,1)\) \citep{FRANKL1988JLSphere} or a
sparser version \(\Phi_{ij}{\sim} N(0,1/\sqrt{\psi})\) with probability
\(\psi\) and \(0\) otherwise \citep{matouvsek2008variants}. Another
common choice which is computationally simpler to generate from is the
Rademacher distribution where \(\Phi_{ij} =  \pm 1/\sqrt{\psi}\) with
probability \(\psi/2\) and zero otherwise for \(\quad 0<\psi\leq 1\).
where \citet{ACHLIOPTAS2003JL} shows results for \(\psi=1\) and
\(\psi=1/3\) while \citet{LiHastie2006VerySparseRP} recommend using
\(\psi=1/\sqrt{p}\) to obtain very sparse matrices. Further approaches
include using Haar measure to generate random orthogonal matrices
\citep{cannings2017random} or distributions which are not sub-Gaussian,
such as standard Cauchy for settings where the data is high-dimensional,
non-sparse, and heavy-tailed by preserving approximate \(\ell_1\)
distances \citep[see e.g.,][]{li2007nonlinear}. Instead of using sparse
matrices, structured matrices, for which multiplication can more easily
be performed have also been proposed \citep[see
e.g.,][]{ailon2009fast, Clarkson2013LowRankApproxShort}

The conventional random projections mentioned above are data-agnostic.
However, recent work has proposed incorporating information from the
data either in choosing an appropriate random projection or directly in
the random projection. For example, \citet{cannings2017random} build an
ensemble classifier where they propose to choose the random projection
matrix to be used in a model by selecting the one that minimizes the
test error of the classification problem among a set of data-agnostic
random projections. On the other hand, \citet{parzer2024glms} propose to
use a random projection matrix for GLMs which directly incorporates
information about the relationship between the predictors and the
response in the projection matrix, rather than a projection matrix which
satisfies the JL lemma. More specifically, it constructs a matrix that
where the vector of coefficients can be recovered after the projection,
which is not the case for conventional random projections
\citep{Thanei2017RPforHDR} by using the sparse embedding matrix of
\citet{Clarkson2013LowRankApproxShort}, where the random diagonal
elements are replaced by a ridge coefficient with a minimal \(\lambda\)
penalty. Another data-driven approach to random projection for
regression has been proposed by \citet{ryder2019asymmetric}, who propose
a data-informed random projection using an asymmetric transformation of
the predictor matrix without using information of the response.

\pkg{SPAR} has been designed in a way that allows the incorporation of
different random projection techniques which allows users to tailor the
employed procedure to their specific data needs.

\subsection{Generalized linear models}\label{generalized-linear-models}

After we perform in each marginal model an initial screening step
followed by a projection step, we assume that the reduced and projected
set of predictors \(\boldsymbol{z}_i\) together with the response arises
from a GLM with the responses having conditional densities from a
(reproductive) exponential dispersion family of the form
\begin{align}\label{eqn:y_density}
  f(y_i|\theta_i,\phi) = \exp\Bigl\{\frac{y_i\theta_i- b(\theta_i)}{a(\phi)} + c(y_i,\phi)\Bigr\},
  \quad
    g(\E[y_i|\boldsymbol{z}_i]) = \gamma_0 + \boldsymbol{z}_i'\boldsymbol{\gamma}=:\eta_i,
\end{align} where \(\theta_i\) is the natural parameter, \(a(.)>0\) and
\(c(.)\) are specific real-valued functions determining different
families, \(\phi\) is a dispersion parameter, and \(b(.)\) is the
log-partition function normalizing the density to integrate to one. If
\(\phi\) is known, we obtain densities in the natural exponential family
for our responses. The responses are related to the \(m\)-dimensional
reduced and projected predictors through the conditional mean, i.e., the
conditional mean of \(y_i\) given \({\boldsymbol{z}}_i\) depends on a
linear combination of the predictors through a (invertible) link
function \(g(.)\), where \(\gamma_0\in\mathbb{R}\) is the intercept and
\(\boldsymbol{\gamma}\in\mathbb{R}^m\) is a vector of regression
coefficients for the \(m\) projected predictors.

Given that \(m\), the goal dimension of the projection need not
necessarily be small in comparison to \(n\) (we recommend using a
dimension of around \(n/2\)), we observe that adding a small \(L_2\)
penalty in the marginal models, especially for the binomial family, can
make estimation more stable as it alleviates problems related to
separation. The marginal models we estimate therefore involve minimizing
the following function: \[
 \text{argmin}_{{\gamma}\in\mathbb{R}^m}\min_{\gamma_0\in\mathbb{R}}  \sum_{i=1}^n -\ell(\beta;y_i,\boldsymbol{z}_i) + \frac{\varepsilon}{2}\sum_{j=1}^m{\gamma}_j^2, \, \varepsilon > 0
\] We will employ function \texttt{glmnet()} of package {glmnet}
\citep{glmnet2023} in the estimation of the marginal models.

\subsection{Algorithm}\label{sec-algo}

We present the general algorithm implemented in package \pkg{SPAR}.

\begin{itemize}
\item
  Choose family with corresponding log-likelihood \(\ell(.)\) and link
\item
  Standardize predictors \(X:n\times p\) by subtracting the sample
  column mean and dividing the sample standard deviation.
\item
  Calculate screening coefficients \(\hat\alpha\) e.g.,
\item
  For \(k=1,\dots,M\) models:

  \begin{itemize}
  \item
    screen \(2n\) predictors based on the screening coefficient
    \(\hat\alpha_j\) yielding screening index set
    \(I_k=\{j_1^k,\dots,j_{2n}^k\}\subset[p]\); if probabilistic
    screening should be employed draw the predictors without
    replacements using an initial vector of probabilities
    \(p_j\propto |\hat\alpha_j|\). Otherwise, select the \(2n\)
    variables with the highest \(|\hat\alpha_j|\).
  \item
    project screened variables to a random dimension
    \(m_k\sim \text{Unif}\{\log(p),\dots,n/2\}\) using
    \textbf{projection matrix} \(\Phi_k\) to obtain
    \(Z_k=X_{\cdot I_k}\Phi_k^\top \in \mathbb{R}^{n\times m_k}\):
  \item
    fit a \textbf{GLM} of \(y\) against \(Z_k\) (with small
    \(L_2\)-penalty \cite{glmnet2023}) to obtain estimated coefficients
    \(\gamma^k\in\mathbb{R}^{m_k}\) and
    \(\hat \beta_{I_k}^k=\Phi_k'\gamma^k\),
    \(\hat \beta_{\bar I_k}^k=0\).
  \end{itemize}
\item
  for a given threshold \(\nu>0\), set all \(\hat\beta_j^k\) with
  \(|\hat\beta_j^k|<\nu\) to \(0\) for all \(j,k\)
\item
  choose \(M\) and \(\lambda\) via a validation set or cross-validation
  by repeating steps 1 to 4 and evaluating a prediction performance
  measure on the withheld fold; and choose \begin{align}
       (M_{\text{best}},\lambda_{\text{best}}) = \text{argmin}_{M,\lambda}\text{Dev}(M,\lambda)
     \end{align}
\item
  combine via the coefficients \textbf{simple average}
  \(\hat \beta = \sum_{k=1}^M\hat \beta^k / M\)
\item
  output the estimated coefficients and predictions for the chosen \(M\)
  and \(\lambda\).
\end{itemize}

\section{Software}\label{sec-software}

The package be installed from \proglang{GitHub}

\begin{verbatim}
R> devtools::install_github("RomanParzer/SPAR")
\end{verbatim}

and loaded by:

\begin{verbatim}
R> library("SPAR")
\end{verbatim}

In this section we rely for illustration purposes on an example data set
from the package:

\begin{verbatim}
R> data("example_data", package = "SPAR")
R> str(example_data)
\end{verbatim}

\subsection{Main functions and their
arguments}\label{main-functions-and-their-arguments}

The two main functions for fitting the SPAR algorithm are:

\begin{verbatim}
R> spar(x, y, family = gaussian("identity"), rp = NULL, screencoef = NULL,
+  xval = NULL, yval = NULL, nnu = 20, nus = NULL, nummods = c(20),
+  measure = c("deviance", "mse", "mae", "class", "1-auc"),
+  inds = NULL, RPMs = NULL, ...)
\end{verbatim}

which implements the algorithm in Section~\ref{sec-algo} without
cross-validation and returns an object of class ``\texttt{spar}'', and

\begin{verbatim}
R> spar.cv(x, y, family = gaussian("identity"), rp = NULL, screencoef = NULL,
+  nfolds = 10, nnu = 20, nus = NULL, nummods = c(20),
+  measure = c("deviance", "mse", "mae", "class", "1-auc"), ...)
\end{verbatim}

which implements the cross-validated procedure and returns an object of
class ``\texttt{spar.cv}''.

The common arguments of these functions are:

\begin{itemize}
\item
  \texttt{x} is an \(n \times p\) numeric matrix of predictor variables.
\item
  \texttt{y} numeric response vector of length \(n\).
\item
  \texttt{family} object from \fct{stats::family}.
\item
  \texttt{rp} an object of class \class{randomprojection}
\item
  \texttt{screencoef} an object of class \class{screencoef}
\item
  \texttt{nnu} is the number of threshold values \(\nu\) which should be
  considered for thresholding; defaults to 20
\item
  \texttt{nus} is an optional vector of \(\nu\) values to be considered
  for thresholding. If it is not provided, is defaults to a grid of
  \texttt{nnu} values. This grid is generated by including zero and
  \texttt{nnu}\(-1\) equally spaced quantiles of the absolute values of
  the estimated coefficients from the marginal models.
\item
  \texttt{nummods} is the number of models to be considered in the
  ensemble; defaults to 20. If a vector is provided, all combinations of
  \texttt{nus} and \texttt{nummods} are considered when choosing the
  optimal \(\nu_\text{best}\) and \(M_\text{best}\).
\item
  \texttt{measure} gives the measure based on which the thresholding
  value \(\nu_\text{opt}\) and the number of models \texttt{M} should be
  chosen on the validation set (for \texttt{spar()}) or in each of the
  folds (in \texttt{spar.cv()}). Defaults to \texttt{"deviance"}, which
  is available for all families. Other options are \texttt{"mse"} or
  \texttt{"mae"} (between responses and predicted means, for all
  families), \texttt{"class"} (misclassification error) and
  \texttt{"1-auc"} (one minus area under the ROC curve) both just for
  binomial family.
\end{itemize}

Furthermore, \texttt{spar()} has the specific arguments:

\begin{itemize}
\item
  \texttt{xval} and \texttt{yval} which are used as validation sets for
  choosing \(\nu_\text{best}\) and \(M_\text{best}\). If not provided,
  \texttt{x} and \texttt{y} will be employed.
\item
  \texttt{inds} is an optional list of length \texttt{max(nummods)}
  containing column index-vectors corresponding to variables that should
  be kept after screening for each marginal model; dimensions need to
  fit those of the dimensions of the provided matrices in \texttt{RPM}.
\item
  \texttt{RPMs} is an optional list of length \texttt{max(nummods)}
  which contains projection matrices to be used in each marginal model.
\end{itemize}

Function \texttt{spar.cv()} has the specific argument \texttt{nfolds}
which is the number of folds to be used for cross-validation. It relies
on \texttt{spar()} as a workhorse, which is called for each fold. The
random projections for each model are held fixed throughout the
cross-validation to reduce the computational burden. This is possible by
calling \texttt{spar()} in each fold with a predefined \texttt{inds} and
\texttt{RPMs} argument.

\subsection{Screening coefficients}\label{screening-coefficients}

The objects for creating screening coefficients are implemented as
\proglang{S}3 classes \class{screencoef}. These objects are created by
several \texttt{screen\_*} functions, which take \texttt{...} and a list
of controls \texttt{control} as arguments. These functions return an
object of class \class{screencoef} which in turn is a list with three
elements:

\begin{itemize}
\item
  \texttt{name},
\item
  \texttt{generate\_fun} -- an \proglang{R} function for generating the
  screening coefficient. This function should have the following
  arguments:

  \begin{itemize}
  \tightlist
  \item
    a \class{screencoef} object which has as attributes all the
    information passed through \texttt{...} ,\\
  \item
    \texttt{data}, which should be a list of \texttt{x} -- the matrix of
    standardized predictors -- and \texttt{y} -- the vector of
    (standardized in the Gaussian case) responses. It returns a vector
    of screening coefficients of length \(p\).
  \end{itemize}
\item
  \texttt{control}, which is the control list in \texttt{screen\_*}.
  These controls are arguments which are needed in
  \texttt{generate\_fun} in order to generate the desired screening
  coefficients.
\end{itemize}

The following screening coefficients are implemented in \pkg{SPAR}:

\begin{itemize}
\item
  \texttt{screen\_marglik()} - computes the screening coefficients by
  the coefficient of \(x_j\) in a univariate GLM using the
  \texttt{stats::glm()} function. \[
   \hat\alpha_j=:\text{argmin}_{\beta_j\in \mathbb{R}}\text{min}_{{\beta_0}\in\mathbb{R}}\sum_{i=1}^n -\ell(\beta_0,\beta;y_i,x_{ij})
   \] It allows to pass a list of controls through the \texttt{control}
  argument to \texttt{stats::glm} such as weights, family, offsets.
\item
  \texttt{screen\_cor()} -- computes the screening coefficients by the
  correlation between \(y\) and \(x_j\) using the function
  \texttt{stats::cor()}. It allows to pass a list of controls through
  the \texttt{control} argument to \texttt{stats::cor}.
\item
  \texttt{screen\_glmnet()} -- computes by default the ridge coefficient
  where the penalty \(\lambda\) is very small \citep[see][ for
  motivation]{parzer2024glms}. \[
  \hat\alpha=: \text{argmin}_{{\beta}\in\mathbb{R}^p}\sum_{i=1}^n -\ell(\beta;y_i,x_i) + \frac{\varepsilon}{2}\sum_{j=1}^p{\beta}_j^2, \, \varepsilon > 0
  \] The function relies on \texttt{glmnet::glmnet()} and, while it
  assumes by default \(\alpha = 0\) and a small penalty, it allows to
  pass a list of controls through the \texttt{control} argument to
  \texttt{glmnet::glmnet()} such as \texttt{alpha\ =\ 1}. Used as
  default if \texttt{screencoef\ =\ NULL} in function call of
  \texttt{spar()} or \texttt{spar.cv()}.
\end{itemize}

Further arguments related to screening can be passed through
\texttt{...}, which will be saved as attributes of the
\class{screencoef} object. More specifically, the following are employed
in function \texttt{spar()}:

\begin{itemize}
\item
  \texttt{nscreen} integer giving the number of variables to be retained
  after screening; defaults to \(2n\)
\item
  \texttt{split\_data\_prop}, double between 0 and 1 which indicates the
  proportion of the data that should be used for computing the screening
  coefficient. The remaining data will be used for estimating the SPAR
  algorithm; defaults to \texttt{NULL}. In this case the whole data will
  be used for estimating the screening coefficient and the SPAR
  algorithm.
\item
  \texttt{type} character -- either \texttt{"prob"} (indicating that
  probabilistic screening should be employed) or \texttt{"fixed"}
  (indicating that a fixed set of \texttt{nscreen} variables should be
  employed across the ensemble; defaults to \texttt{type\ =\ "prob"}.
\end{itemize}

For illustration purposes, consider the implemented function
\texttt{screen\_marglik()}, which is used to define a screening
procedure based on the coefficients of univariate marginal GLMs between
each predictor and the response.

\begin{verbatim}
R> obj <- screen_marglik()
\end{verbatim}

A user-friendly \texttt{print} of the \class{screencoef} is provided:

\begin{verbatim}
R> obj
\end{verbatim}

The structure of the object is the following:

\begin{verbatim}
R> unclass(obj)
\end{verbatim}

Function \texttt{generate\_scrcoef\_marglik} defines the generation of
the screening coefficient. It considers the controls in
\texttt{object\$control} when calling the \texttt{stats::glm()}
function. Given that the proposed framework estimates GLMs for the
marginal models, the \texttt{spar()} function assigns by default its
\texttt{family} argument as an attribute for the \texttt{screencoef}
object. In \texttt{generate\_scrcoef\_marglik}, we employ
\texttt{family} attribute of the \class{screencoef} object if the
control list argument does not contain any particular family.

For convenience, a constructor function
\texttt{constructor\_screencoef()} is provided, which can be used to
create new \texttt{screen\_*} functions. An example is presented in
Section~\ref{sec-extensscrcoef}

\subsection{Random projections}\label{random-projections}

Similar to the screening procedure, the objects for creating random
projections are implemented as \proglang{S}3 classes
\class{randomprojection} and are created by functions
\texttt{rp\_*(...,\ control\ =\ list())}, which take \texttt{...} and a
list of controls \texttt{control} as arguments.

These functions return an object of class \class{randomprojection} which
is a list with three elements:

\begin{itemize}
\item
  \texttt{name},
\item
  \texttt{generate\_fun} function for generating the random projection
  matrix. This function should have with arguments \code{rp}, which is a
  \class{randomprojection} object, \code{m}, the target dimension and a
  vector of indexes \code{included_vector} which shows the column index
  of the original variables in the \code{x} matrix to be projected using
  the random projection. This is needed due to the fact that screening
  can be employed pre-projection. It returns a sparse random projection
  matrix of class ``\texttt{dgCMatrix}'' of the \pkg{Matrix} with
  \code{m} rows and \code{length(included_vector)} columns.
\item
  \texttt{update\_data\_rp} optional function with attributes relying
  with information from the data which should only be computed once,
  before the start of the SPAR algorithm. This function should have with
  arguments \code{rp}, which is a \class{randomprojection} object and
  \texttt{data}, which should be a list of \texttt{x} -- the matrix of
  standardized predictors -- and \texttt{y} -- the vector of
  (standardized in the Gaussian case) responses. This function is
  relevant for \textbf{data driven} random projections, because to keep
  computational costs low, all data-dependent computations which do not
  need to be computed in each random projection will be computed once
  before the start of the algorithm. All computed quantities which will
  be used in the \texttt{generate\_fun} function should be saved as
  attributes to the \texttt{randomprojection} object, which is what this
  function returns.
\item
  \texttt{update\_rpm\_w\_data} optional function for updating the
  random projection matrices provided in the argument \texttt{RPMs} of
  functions \texttt{spar} and \texttt{spar.cv} with data-dependent
  parameters. If argument \texttt{RPMs} is provided, the random
  structure is kept fixed, but the data-dependent part gets updated with
  the provided data set. Defaults to \texttt{NULL}. If not provided, the
  values of the provided \texttt{RPMs} do not change. This is
  particularly relevant in the cross-validation procedure.
\item
  \texttt{control}, which is the control list in
  \texttt{rp}\_*\texttt{.\ These\ controls\ are\ \ \ arguments\ which\ are\ needed\ in}generate\_fun`
  in order to generate the desired random projection.
\end{itemize}

Further arguments related to the random projection can be passed through
\texttt{...}, which will then be saved as attributes of the
\class{randomprojection} object.

More specifically, the following attributes are relevant in the SPAR
algorithm:

\begin{itemize}
\item
  \texttt{mslow}: integer giving the minimum dimension to which the
  predictors should be projected; defaults to \(\log(p)\)
\item
  \texttt{msup}: integer giving the maximum dimension to which the
  predictors should be projected; defaults to \(n/2\)
\item
  \texttt{use\_data}: boolean indicating whether the random projection
  is data driven.
\end{itemize}

Note that for random projection matrices which satisfy the JL lemma,
\texttt{mslow} can be determined by employing the result of the JL
lemma, which gives a lower bound on the goal dimension in order to
preserve the distances between all pairs of points within a factor
\((1 \pm \epsilon)\): \(m>\frac{24}{3\epsilon^2-2\epsilon^3}\log n\).

The following random projections are implemented in \pkg{SPAR}:

\begin{itemize}
\item
  \texttt{rp\_gaussian()} -- random projection object where the
  generated matrix will have entries from a normal distribution
  (defaults to standard normal entries)
\item
  \texttt{rp\_sparse()} -- random projection object where the generated
  matrix will be the one in \citep{ACHLIOPTAS2003JL} with \texttt{psi=1}
  by default.
\item
  \texttt{rp\_cw()} -- sparse embedding random projection in
  \citep{Clarkson2013LowRankApprox} for \texttt{rp\_cw(data\ =\ FALSE)}.
  Defaults to \texttt{rp\_cw(data=TRUE)}, which replaces the random
  elements on the diagonal by the ridge coefficients with a small
  penalty, as introduced in \citet{parzer2024glms}.
\end{itemize}

For illustration purposes, consider the implemented function
\texttt{rp\_gaussian}, which is a random projection with entries drawn
from the normal distribution.

\begin{verbatim}
R> rp_gaussian()
\end{verbatim}

The \texttt{generate\_fun} returns a sparse matrix object of class
``\texttt{dgCMatrix}'' by using the \texttt{Matrix::Matrix} function.
Note that \texttt{included\_vector} gives the indexes of the variables
which have been selected by the screening procedure. In this case, where
the random projection does not use any data information, we are only
interested in the length of this vector.

The elements \texttt{update\_data\_fun} and
\texttt{update\_rpm\_w\_data} of the object are \texttt{NULL}, as this
conventional random projection is data agnostic.

\subsection{Methods}\label{methods}

Methods \texttt{print}, \texttt{plot}, \texttt{coef}, \texttt{predict}
are available for both ``\texttt{spar}'' and ``\texttt{spar.cv}''
classes.

\subsubsection{print}\label{print}

The \texttt{print} method returns information on \(\nu_\text{best}\)
\(M_\text{best}\), the number of active predictors (i.e., predictors
which have at least a nonzero coefficient across the marginal models)
and a five-point summary of the non-zero coefficients.

\begin{verbatim}
R> spar_res <- spar(example_data$x, example_data$y,
+                 xval = example_data$xtest,
+                 yval = example_data$ytest,
+                 nummods = c(5,10,15,20,25,30))
R> spar_cv <- spar.cv(example_data$x, example_data$y,
+                   nummods = c(5,10,15,20,25,30))
\end{verbatim}

\begin{verbatim}
R> spar_res
\end{verbatim}

\begin{verbatim}
R> spar_cv
\end{verbatim}

\subsubsection{coef}\label{coef}

Method \texttt{coef} takes as inputs a \texttt{spar} or \texttt{spar.cv}
object, together with further arguments:

\begin{itemize}
\item
  \texttt{nummod} -- number of models used to compute the averaged
  coefficients; value of \texttt{nummod} with minimal\\
  \texttt{measure} is used if not provided.
\item
  \texttt{nu} -- threshold level used to compute the averaged
  coefficients; value with minimal \texttt{measure} is used if not
  provided.
\end{itemize}

\begin{verbatim}
R> str(coef(spar_res))
\end{verbatim}

It returns a list with the intercept, vector of \texttt{beta}
coefficients and the \texttt{nummod} and \texttt{nu} employed in the
calculation.

Additionally for ``\texttt{spar.cv}'', the \texttt{coef} method also has
argument \texttt{opt\_par} which is one of \texttt{c("1se","best")} and
chooses whether to select the best pair of \texttt{nus} and
\texttt{nummods} according to cross-validation \texttt{measure}, or the
solution within one standard deviation of that optimal cross-validation
\texttt{measure}. This argument is ignored when \texttt{nummod} and
\texttt{nu} are given.

\subsubsection{predict}\label{predict}

Functionality for computing predictions is provided through the method
\texttt{predict} which takes a \texttt{spar} or \texttt{spar.cv} object,
together with

\begin{itemize}
\item
  \texttt{xnew} matrix of new predictor variables; must have same number
  of columns as \texttt{x}.
\item
  \texttt{type} the type of required predictions; either on
  \texttt{"response"} level (default) or on \texttt{"link"} level
\item
  \texttt{avg\_type} type of averaging used across the marginal models;
  either on \texttt{"link"} (default) or on \texttt{"response"} level
\item
  \texttt{nummod} -- number of models used to compute the averaged
  coefficients; value of \texttt{nummod} with minimal\\
  \texttt{measure} is used if not provided.
\item
  \texttt{nu} -- threshold level used to compute the averaged
  coefficients; value with minimal \texttt{measure} is used if not
  provided.
\item
  \texttt{coef} optional vector of coefficients to be used directly in
  the prediction.
\end{itemize}

Additionally, for class ``\texttt{spar.cv}'', argument \texttt{opt\_par}
is available and used in the computation of the coefficients to be used
for prediction (see above description of method \texttt{coef}).

\subsubsection{plot}\label{plot}

Plotting functionality is provided through method \texttt{plot} which
allows for the following arguments:

\begin{itemize}
\item
  \texttt{x} an object of class ``\texttt{spar}'' or
  ``\texttt{spar.cv}''
\item
  \texttt{plot\_type} one of:

  \begin{itemize}
  \item
    \texttt{"Val\_Measure"} plots the (cross-)validation
    \texttt{measure} for either a grid of \texttt{nu} values for a fixed
    number of models \texttt{nummod} or viceversa.
  \item
    \texttt{"Val\_numAct"} plots the number of active variables for
    either a grid of \texttt{nu} values for a fixed number of models
    \texttt{nummod} or viceversa.
  \item
    \texttt{"res-vs-fitted"} produces a residuals-vs-fitted plot. The
    residuals are computed as \(y- \widehat y\), where \(\widehat y\) is
    the prediction computed on response level.
  \item
    \texttt{"coefs"} produces a plot of the value of the standardized
    coefficients for each predictor in each marginal model (before
    thresholding). For each predictor, the values of the coefficients
    are sorted from largest to smallest across the marginal models and
    then represented in the plot.
  \end{itemize}
\item
  \texttt{plot\_along}, one of \texttt{c("nu","nummod")}; indicates
  whether the (cross-)validation ignored when
  \texttt{plot\_type="res-vs-fitted"} or \texttt{plot\_type="coefs"}.
\item
  \texttt{nummod} -- fixed value for number of models when
  \texttt{plot\_along\ =\ "nu"} for
  \texttt{plot\_type\ =\ "Val\_Measure"} or \texttt{"Val\_numAct"}; if
  \texttt{plot\_type="res-vs-fitted"}, it is used in the
  \texttt{predict} method, as described above.
\item
  \texttt{nu}-- fixed value for \(\nu\) when
  \texttt{plot\_along\ =\ "nummod"} for
  \texttt{plot\_type\ =\ "Val\_Measure"} or \texttt{"Val\_numAct"}; if
  \texttt{plot\_type="res-vs-fitted"}, it is used in the
  \texttt{predict} method, as described above.
\item
  \texttt{xfit} -- if \texttt{plot\_type\ =\ "res-vs-fitted"}, it is the
  matrix of predictors used in computing the fitted values. Must
  provided for this plot, as the \texttt{spar} or \texttt{spar.cv}
  objects do not store the original data.
\item
  \texttt{yfit} -- if \texttt{plot\_type\ =\ "res-vs-fitted"}, vector of
  responses used in computing the residuals. Must provided for this
  plot, as the \texttt{spar} or \texttt{spar.cv} objects do not store
  the original data.
\item
  \texttt{prange}, optional vector of length 2 in case
  \texttt{plot\_type\ =\ "coefs"} which gives the limits of the
  predictors' plot range; defaults to \texttt{c(1,\ p)}.
\item
  \texttt{coef\_order} optional index vector of length \(p\) in case
  \texttt{plot\_type\ =\ "coefs"} to give the order of the predictors;
  defaults to \texttt{1\ :\ p}.
\end{itemize}

For class ``\texttt{spar.cv}'' there is the extra argument
\texttt{opt\_par\ =\ c("best",\ "1-se")} which, for
\texttt{plot\_type\ =\ "res-vs-fitted"} indicates whether the
predictions should be based on coefficients using the best \((\nu, M)\)
combination or on the one which delivers the largest error within
1-standard-deviation from the minimum.

The \texttt{plot} methods return objects of class ``\texttt{ggplot}''
\citet{ggplotR}.

\section{Extensibility}\label{extensibility}

The user can implement their own screening and random projections.

\subsection{Screening coefficients}\label{sec-extensscrcoef}

We exemplify how new screening coefficients implemented in package
\pkg{VariableScreening} can easily be used in the framework of
\pkg{SPAR}.

We start by defining the function for generating the screening
coefficients using the \texttt{screenIID()} function in
\pkg{VariableScreening}.

\begin{verbatim}
R> generate_scr_sirs <- function(object, data) {
+  res_screen <- do.call(function(...) 
+    VariableScreening::screenIID(data$x, data$y, ...), 
+    object$control)
+  coefs <- res_screen$measurement
+  coefs
+}
\end{verbatim}

Note that \texttt{screenIID()} also takes method as an argument. To
allow for flexibility, we do not fix the method in
\texttt{generate\_scr\_sirs} but rather allow the user to pass a method
through the \texttt{control} argument in the \texttt{screen\_*}
function. This function is created using
\texttt{constructor\_screencoef}:

\begin{verbatim}
R> screen_sirs <- constructor_screencoef(
+  "screen_sirs", 
+  generate_fun = generate_scr_sirs)
\end{verbatim}

We now call the \texttt{spar()} function with the newly created
screening procedure. We consider method SIRS of \citet{zhu2011model},
which ranks the predictors by their correlation with the rank-ordered
response and we do not perform probabilistic variable screening but
employ the top \(2n\) variables in each marginal model.

\begin{verbatim}
R> set.seed(123)      
R> spar_example <- spar(example_data$x, example_data$y,
+                     screencoef = screen_sirs(type = "fixed",
+                                           control=list(method = "SIRS")),
+                     measure = "mse")
R> print(spar_example)
\end{verbatim}

\subsection{Random projections}\label{random-projections-1}

We exemplify how new random projections can be implemented in the
framework of \pkg{SPAR}.

We implement the random projection of \citet{cannings2017random}, who
propose using the Haar measure for generating the random projections.
They simulate matrices from Haar measure by independently drawing each
entry of a matrix \(Q\) from a standard normal distribution, and then to
take the projection matrix to be the transpose of the matrix of left
singular vectors in the singular value decomposition of \(Q\). Moreover,
they suggest using ``good'' random projections, in the sense that they
deliver the best out-of-sample prediction. The proposed approach employs
\(B_1\) models in an ensemble of classifiers and for each model \(k\),
\(B_2\) data independent random projections are generated and the one
with the lowest error on a test set is the one chosen to project the
variables in model \(k\).

We can implement such a random projection in \pkg{SPAR} by the following
building block:

\begin{verbatim}
R> update_data_cannings <- function(rp, data) {
+  attr(rp, "data") <- data
+  rp
+}
\end{verbatim}

This the function which adds data information to the random projection
object. Here, the whole data can be added as information for the \(M\)
random projection (alternatively, one could only pass sufficient
statistics for computing the desired measures).

While the \(B_2\) random projections are data agnostic, the
\texttt{generate\_fun} element of the random projection will need the
data information in order to evaluate which method performs best in
terms of an error measure. We will in the following define the function
for the generation of the random projection matrix to be used in a model
\(k\).

This helper simulates \(m\times p\) matrices from the Haar measure:

\begin{verbatim}
R> simulate_haar <- function(m, p) {
+    R0 <- matrix(1/sqrt(p) * rnorm(p * m), nrow = p, ncol = m)
+    RM <- qr.Q(qr(R0))[, seq_len(m)]
+    RM <- Matrix(t(RM), sparse = TRUE)  
+}
\end{verbatim}

The function that generates the random projection matrix for model \(k\)
used 25\% of the data as a test set for choosing the best among \(B_2\)
random projections in terms of minimizing misclassification error for
the binomial family and MSE for all other families:

\begin{verbatim}
R> generate_cannings <- function(rp, m, included_vector) {
+  p <- length(included_vector)
+  if (is.null(rp$control$B2)) rp$control$B2 <- 50
+  x <- attr(rp, "data")$x[, included_vector]
+  y <- attr(rp, "data")$y
+  
+  B2 <- rp$control$B2
+  n <- nrow(x)
+  id_test <- sample(n, size = n %/% 4)
+  xtrain <- x[-id_test, ];  xtest <- x[id_test,]
+  ytrain <- y[-id_test];  ytest <- y[id_test]
+  
+  if (is.null(rp$control$family)) rp$control$family <- attr(rp, "family")
+  
+  family <- rp$control$family
+  control_glm <-
+    rp$control[names(rp$control)  %in% names(formals(glm.fit))]
+
+  error_all <- lapply(seq_len(B2), FUN = function(s){
+    RM <- simulate_haar(m, p)
+    xrp <- tcrossprod(xtrain, RM)
+    mod <- do.call(function(...) 
+      glm.fit(x =  cbind(1, xrp), y = ytrain, ...), control_glm)
+    eta_test <- drop(cbind(1, tcrossprod(xtest, RM)) %*% mod$coefficients)
+    pred <- family$linkinv(eta_test)
+    out <-  ifelse(family$family == "binomial",
+                   mean(((pred > 0.5) + 0) != ytest), 
+                   mean((pred - ytest)^2))
+    list(RM, out)
+  })
+  id_best <- which.min(sapply(error_all, "[[", 2))
+  RM <- error_all[[id_best]][[1]]
+  return(RM)
+}
\end{verbatim}

In the cross-validation procedure, sample new random projection matrices
and pick the best one for the current iteration. Given that we do not
wish to keep the random structure of the matrices fixed, we do not
specify a function \texttt{update\_rpm\_w\_data}.

Putting it all together, we get:

\begin{verbatim}
R> rp_cannings <- constructor_randomprojection(
+  "rp_cannings",
+  generate_fun = generate_cannings,
+  update_data_fun = update_data_cannings
+)
\end{verbatim}

We can now estimate SPAR for a binomial model, where we transform the
response to a binary variable.

\begin{verbatim}
R> ystar <- (example_data$y > 0) + 0
R> ystarval <- (example_data$ytest > 0) + 0
\end{verbatim}

We use \(100\) models (which is in line to recommendations for \(B_1\)
in \citet{cannings2017random}), and no screening procedure. If no
screening is desired, \texttt{nscreen} can be set to \texttt{p} in the
\texttt{screen\_*} function:

\begin{verbatim}
R> set.seed(123)   
R> spar_example_1 <- spar(
+  x = example_data$x, y=ystar,
+  xval = example_data$xtest, yval = ystarval,
+  family = binomial(),
+  nummods = 100, 
+  screencoef = screen_marglik(nscreen = ncol(example_data$x)),
+  rp = rp_cannings(control = list(B2 = 50)),
+  measure = "class"
+)
R> 
R> spar_example_2 <- spar(x = example_data$x, y = ystar,
+  family = binomial(),
+  screencoef = screen_marglik(nscreen = ncol(example_data$x)),
+  rp = rp_cw(data = TRUE),
+  nummods = 100, 
+  xval = example_data$xtest, yval = ystarval,
+  measure = "class"
+)
\end{verbatim}

We can extract the measures on the validation set by:

\begin{verbatim}
R> head(spar_example_1$val_res)
\end{verbatim}

We can compare the two approaches by looking at the minimum
\texttt{Meas} achieved:

\begin{verbatim}
R> min_val_1 <- min(spar_example_1$val_res$Meas)
R> id_best_1 <- max(which(spar_example_1$val_res$Meas == min_val_1))
R> min_val_2 <- min(spar_example_2$val_res$Meas)
R> id_best_2 <- max(which(spar_example_2$val_res$Meas == min_val_2))
R> 
R> spar_example_1$val_res[id_best_1, ]
R> spar_example_2$val_res[id_best_2, ]
\end{verbatim}

\section{Illustrations}\label{sec-illustrations}

\subsection{Face image data}\label{face-image-data}

We illustrate the functionality of \pkg{SPAR} on the Isomap data set
containing \(n = 698\) black and white face images of size
\(p = 64 \times 64 = 4096\) together with the faces' horizontal looking
direction angle as the response variable.\footnote{
The Isomap face data can be found online on https://web.archive.org/web/20160913051505/http://isomap.
stanford.edu/datasets.html.}

\begin{verbatim}
R> url <- "https://web.archive.org/web/20150922051706/http://isomap.stanford.edu/face_data.mat.Z"
R> download.file(url, file.path("face_data.mat.Z"))
R> system('uncompress face_data.mat.Z')
\end{verbatim}

The \texttt{.mat} file format can be read using \pkg{R.matlab}
\citep{pkg:rmatlab}

\begin{verbatim}
R> library("R.matlab")
R> facedata <- readMat(file.path("face_data.mat"))
R> x <- t(facedata$images)
R> y <- facedata$poses[1,]
\end{verbatim}

We can visualize e.g., the first observation in this data set in Figure
\ref{fig:facesplot_i1}:

\begin{verbatim}
R> library(ggplot2)
R> i <- 1
R> ggplot(data.frame(X = rep(1:64,each=64),Y = rep(64:1,64),
+                  Z = facedata$images[,i]),
+       aes(X, Y, fill = Z)) +
+  geom_tile() +
+  theme_void() +
+  ggtitle(paste0("y = ", round(facedata$poses[1, i],1))) +
+  theme(legend.position = "none",
+        plot.title = element_text(hjust = 0.5))
\end{verbatim}

We split the data into training vs test sample:

\begin{verbatim}
R> set.seed(1234)
R> ntot <- length(y); ntest <- ntot * 0.25
R> testind <- sample(ntot, ntest, replace=FALSE)
R> xtrain <- as.matrix(x[-testind, ]); ytrain <- y[-testind]
R> xtest <- as.matrix(x[testind, ]); ytest <- y[testind]
\end{verbatim}

and estimate on the training data the SPAR algorithm with
cross-validation. We employ a Gaussian random projection where entries
are generated from a normal distribution with mean zero and standard
deviation \(1/p\) and with screening based on the ridge coefficients:

\begin{verbatim}
R> library(SPAR)
R> p <- ncol(xtrain)
R> spar_faces <- spar.cv(
+  xtrain, ytrain,
+  rp = rp_gaussian(control = list(c(mean = 0, sd = 1/p))),
+  screencoef = screen_cor(method = "kendall"),
+  nummods = c(5, 10, 20, 50),
+  measure = "mse")
R> spar_faces
\end{verbatim}

The \texttt{plot} method for `\texttt{spar.cv}' objects displays by
default the measure employed in the cross-validation (in this case MSE)
for a grid of \(\nu\) values, where the number of models is fixed to the
value found to perform best in cross-validation exercise:

\begin{verbatim}
R> plot(spar_faces)
\end{verbatim}

The coefficients of the different variables (in this example pixels)
obtained by averaging over the coefficients the marginal models (for
optimal \(\lambda\) and number of models) are given by:

\begin{verbatim}
R> face_coef <- coef(spar_faces, opt_par = "best")
R> str(face_coef)
\end{verbatim}

For a sparser solution we can compute the coefficients using
\texttt{opt\_par\ =\ "1se"} which leads to more sparsity and a lowe
number of models.

\begin{verbatim}
R> face_coef_1se <- coef(spar_faces, opt_par = "1se")
R> str(face_coef_1se)
\end{verbatim}

The standardized coefficients from each of \texttt{max(nummods)} models
(before averaging and before thresholding) can be plotted by:

\begin{verbatim}
R> plot(spar_faces, "coefs")
\end{verbatim}

The \texttt{predict()} function can be applied to the `\texttt{spar.cv}'
object. We will employ the sparser solution chosen by the
\texttt{opt\_par\ =\ "1se"} rule:

\begin{verbatim}
R> ynew <- predict(spar_faces, xnew = xtest, coef = face_coef_1se)
\end{verbatim}

In the high-dimensional setting it is interesting to look at the
relative mean square prediction error which compares the MSE to the MSE
of a model containing only an intercept:

\begin{verbatim}
R> rMSPEconst <- mean((ytest - mean(y))^2) 
R> mean((ynew-ytest)^2)/rMSPEconst
\end{verbatim}

Additionally, for this data set, one can visualize the effect of each
pixel \(\hat\beta_j x^\text{new}_{i,j}\) in predicting the face
orientation in a given image e.g., the 10th one in the test set:

\begin{verbatim}
R> i <- 11
R> plot4 <- ggplot(data.frame(X = rep(1:64, each = 64),
+                           Y = rep(64:1, 64),
+                           effect = xtest[i,] * face_coef_1se$beta), 
+                aes(X, Y, fill = effect)) +
+  geom_tile() +
+  theme_void() +
+  scale_fill_gradient2() +
+  ggtitle(bquote(hat(y) == .(round(ynew[i])))) +
+  theme(plot.title = element_text(hjust = 0.5)) 
R> plot4
\end{verbatim}

\subsection{Darwin data set}\label{darwin-data-set}

The Darwin dataset \citep{CILIA2022darwin} contains a binary response
for Alzheimer's disease (AD) together with extracted features from 25
handwriting tests (18 features per task) for 89 AD patients and 85
healthy people
(\(n=174\)).\footnote{The data set can be downloaded from  https://archive.ics.uci.edu/dataset/732/darwin}

\begin{verbatim}
R> download.file("https://archive.ics.uci.edu/static/public/732/darwin.zip",
+              "darwin.zip")
\end{verbatim}

\begin{verbatim}
R> darwin_tmp <- read.csv(unzip("darwin.zip",  "data.csv"), 
+                       stringsAsFactors = TRUE)
\end{verbatim}

Before proceeding with the analysis, the data is screened for
multivariate outliers using the DDC algorithm in package \pkg{cellWise}
\citep{rcellwise}.

\begin{verbatim}
R> darwin_orig <- list(
+  x = darwin_tmp[, !(colnames(darwin_tmp) %in% c("ID", "class"))],
+  y = as.numeric(darwin_tmp$class) - 1)
R> tmp <- cellWise::DDC(
+  as.matrix(darwin_orig$x),
+  list(returnBigXimp = TRUE, 
+       tolProb = 0.999,
+       silent = TRUE))
R> darwin <- list(x = tmp$Ximp, y = darwin_orig$y)
\end{verbatim}

The structure of the data is:

\begin{verbatim}
R> str(darwin)
\end{verbatim}

We estimate the SPAR algorithm with the screening and random projection
introduced in \citet{parzer2024glms} for binomial family and logit link,
using \(1-\)area under the ROC curve as the cross-validation measure,

\begin{verbatim}
R> spar_darwin <- spar.cv(darwin$x, darwin$y,
+                       family = binomial(logit),
+                       nummods = c(5, 10, 20, 50),
+                       measure = "1-auc")
\end{verbatim}

We can look at the average number of active variables for a grid of
\(\nu\) where the number of models is fixed to the value found to
perform best in cross-validation exercise by using the \texttt{plot}
method for ``\texttt{spar.cv}'' (see Figure
\ref{fig:darwin_activevars}).

\begin{verbatim}
R> plot(spar_darwin, plot_type = "Val_numAct")
\end{verbatim}

The predictors are ordered by task, where the first 18 covariates
represent different features measured for the first task. Given that
there is clear grouping in the variables in this example, we can reorder
the coefficients plot by grouping them by feature, rather than task. We
can achieve this by using reordering argument \texttt{coef\_order} in
method \texttt{plot} with \texttt{plot\_type\ =\ "coefs"} (see Figure
\ref{fig:darwin_coefs}).

\begin{verbatim}
R> ntasks <- 25
R> nfeat <- 18
R> reorder_ind <- c(outer(
+  (seq_len(ntasks) - 1) * nfeat,
+  seq_len(nfeat), "+"))
R> feat_names <- sapply(colnames(darwin$x)[seq_len(nfeat)],
+                     function(name) substr(name, 1, nchar(name) - 1))
R> 
R> plot(spar_darwin,"coefs",coef_order = reorder_ind) + 
+  geom_vline(xintercept = 0.5 + seq_len(ntasks - 1) * ntasks, 
+             alpha = 0.2, linetype = 2) +
+  annotate("text",x = (seq_len(nfeat) - 1) * ntasks + 12,
+           y = 45,label=feat_names, angle = 90,
+           size = 3)
\end{verbatim}

In general we observe that the different features measures across
different tasks have the same impact on the probability of AD
(observable by the blocks of blue or red lines).

\section{Conclusion}\label{sec-conclusion}

Package \pkg{SPAR} can be employed for modeling data in a
high-dimensional setting, where the number of predictors is much higher
than the number of observations. The package provides an implementation
of the SPAR algorithm, which combines variable screening and random
projection in an ensemble of GLMs. The package provides flexible classes
for i) specifying the screening coefficient based on which screening
should be performed (both in a classical or probabilistic fashion), ii)
generating the random projection to be employed in each marginal model.

\section*{Computational details}\label{computational-details}

The results in this paper were obtained using \proglang{R} 4.4.0.

\proglang{R} itself and all packages used are available from the
Comprehensive \proglang{R} Archive Network (CRAN) at
\url{https://CRAN.R-project.org/}.

\section*{Acknowledgments}\label{acknowledgments}

Roman Parzer and Laura Vana-G端r acknowledge funding from the Austrian
Science Fund (FWF) for the project ``High-dimensional statistical
learning: New methods to advance economic and sustainability policies''
(ZK 35), jointly carried out by WU Vienna University of Economics and
Business, Paris Lodron University Salzburg, TU Wien, and the Austrian
Institute of Economic Research (WIFO).


\renewcommand\refname{References}
  \bibliography{SPAR.bib}



\end{document}
